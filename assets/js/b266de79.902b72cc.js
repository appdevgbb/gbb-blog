"use strict";(globalThis.webpackChunkdocusaurus=globalThis.webpackChunkdocusaurus||[]).push([[3518],{4369:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2025/07/02/when-infrastructure-scales-but-understanding-doesnt","metadata":{"permalink":"/2025/07/02/when-infrastructure-scales-but-understanding-doesnt","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2025-07-02/when-infrastructure-scales-but-understanding-doesnt/index.md","source":"@site/blog/2025-07-02/when-infrastructure-scales-but-understanding-doesnt/index.md","title":"When Infrastructure Scales But Understanding Doesn\'t","description":"Modern infrastructure can scale infinitely, but human understanding doesn\'t. Explore the cognitive overload crisis and productivity paradox affecting development teams, plus how AI-powered platforms can bridge the gap and make modern tooling more humane.","date":"2025-07-02T00:00:00.000Z","tags":[],"readingTime":7,"hasTruncateMarker":true,"authors":[{"name":"Ray Kao","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/raykao","socials":{"x":"https://x.com/raykao","github":"https://github.com/raykao"},"imageURL":"https://github.com/raykao.png","key":"ray_kao","page":null},{"name":"Diego Casati","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/dcasati","socials":{"x":"https://x.com/ve6dpc","github":"https://github.com/dcasati"},"imageURL":"https://github.com/dcasati.png","key":"diego_casati","page":null}],"frontMatter":{"authors":["ray_kao","diego_casati"],"date":"2025-07-02","description":"Modern infrastructure can scale infinitely, but human understanding doesn\'t. Explore the cognitive overload crisis and productivity paradox affecting development teams, plus how AI-powered platforms can bridge the gap and make modern tooling more humane.","tags":[],"title":"When Infrastructure Scales But Understanding Doesn\'t"},"unlisted":false,"nextItem":{"title":"The Human Scale Problem in Platform Engineering","permalink":"/2025/06/24/human-scale-problem-in-platform-engineering"}},"content":"We all know this, even if we don\'t like to admit it: modern infrastructure can scale infinitely, but human understanding doesn\'t.\\n\\nWe\'ve all seen it happen\u2014organizations going from managing dozens of servers to thousands of containers, from deploying weekly to deploying hundreds of times per day, from serving thousands of users to millions. The technology handled the scale beautifully. The humans? Not so much.\\n\\nThis is the first industry issue that platform engineering should be addressing: **how do we manage infrastructure complexity that has outgrown not just individual cognitive capacity, but our collective ability to communicate and transfer knowledge as teams?**\\n\\n\x3c!-- truncate --\x3e\\n\\n## The Cognitive Overload Crisis\\n\\nTraditional approaches: runbooks, tribal knowledge, heroic individual efforts, break down when you\'re operating distributed systems across multiple clouds, regions, and a diverse org with multiple teams. The solution isn\'t making people smarter; **platform engineering should be about making systems more humane**.\\n\\nPicture this scenario: A deployment fails at 2 AM. In the old world, you\'d SSH into a server, check some logs, restart a service, and move on. Today? You\'re correlating logs across seventeen microservices each using a different log collector, checking metrics in three different monitoring systems, verifying configuration drift across multiple environments, and somehow trying to understand if the failure is related to the service mesh configuration change from last week, the database migration from yesterday, or the new security policy that rolled out an hour ago (it\'s probably synergistacally all of the above).\\n\\nWe haven\'t simply added \\"just\\" complexity, we\'ve added *interdependent* complexity. Every piece affects every other piece in ways that aren\'t always obvious and the failure modes are exponentially more varied than anything we\'ve dealt with in simpler architectures.\\n\\n## The Productivity Paradox\\n\\nHere\'s something I\'ve felt more often lately, and we\'ve all been there: developers spending more time fighting with tools than building features. We\'ve given them incredible capabilities with container orchestration, service meshes, observability platforms, security scanning tools, but we\'ve also given them incredible complexity and, let\'s be honest, still poorly written error messages for new systems and tools.\\n\\nThe second industry issue is **developer productivity fragmentation**...when the cognitive overhead of using our tools exceeds the value they provide.\\n\\nPicture this: you work for a large financial organization where developers need to interact with fourteen different systems just to deploy a simple API change. Each system has its own authentication, its own interface, its own mental model. The developers are technically empowered to do anything, but practically paralyzed by choice and complexity.\\n\\n## The Tool Sprawl Reality\\n\\nHere\'s what actually happens in most organizations:\\n\\n**Monday morning**: Developer needs to deploy a simple API update. They open their laptop and immediately face decision paralysis. Which CI/CD pipeline should they use? The company has three different \\"approved\\" options, each with different capabilities and limitations. They spend 30 minutes just figuring out which one handles their specific use case.\\n\\n**Tuesday**: The deployment worked, but now they need to set up monitoring. There\'s Prometheus for infrastructure metrics, Datadog for application performance, Azure Monitor for cloud resources, and some custom logging solution the security team requires. Each tool requires different configuration, different query languages, different mental models.\\n\\n**Wednesday**: Something breaks in production. They have access to all the observability data in the world, but they don\'t know how to correlate it. They spend two hours hunting through different dashboards, Slack channels, and documentation trying to piece together what happened.\\n\\n**Thursday**: They try to implement the same deployment pattern for another service, but they can\'t remember the exact sequence they used on Monday. The documentation is out of date, and the person who originally set it up is on vacation or blissfully retired.\\n\\n**Friday**: They\'re burned out from fighting with tools instead of solving business problems.\\n\\n## When AI Actually Helps\\n\\nThink about what happens today when a deployment fails. Someone stares at logs, checks metrics, searches through documentation, asks around on Slack, and eventually pieces together what went wrong. What if instead, they could just ask: \\"Why did my deployment fail?\\" (or maybe a bit more complicated of an ask than that...) and get a conversation with an assistant that already knows about your logs, your metrics, and your team\'s troubleshooting patterns?\\n\\nWe\'re not trying to replace the human expertise here...we\'re trying to make that expertise accessible when people need it. Instead of requiring everyone to become Kubernetes troubleshooting experts, the AI can guide them through the investigation, explaining what each step reveals about what\'s actually happening.\\n\\nThis isn\'t about replacing human expertise...it\'s about making that expertise accessible to more people at the right moment that they need it. We\'re moving from \\"you need to know how to use this specific tool right now, in order to get answers\\" to \\"you need know where things could breakdown well enough to ask the questions, to make the right decisions\\" The expertise shifts from knowing exactly which buttons to click to having the experience and wisdom to articulate what evidence you actually need to diagnose a solution.\\n\\n## The Platform Solution\\n\\nWhat we\'re learning is that platforms work best when they don\'t try to replace the tools people already use and love. Instead of building another portal, we should be building small, focused products that fill the gaps between the tools people already know; making GitHub, Slack, your cloud provider, and your monitoring tools work together more seamlessly.\\n\\nThere\'s a clearer stack emerging here. You have multiple choices at each level...some glue together well...some not so much. But our job as platform engineers isn\'t to rebuild everything from scratch. It\'s to take what\'s already there and solve for the odd grey areas between these broadly adopted tools. The platform becomes the smart glue, not the central command center.\\n\\nWhen we talk about \\"platform as a product,\\" what we really mean is an opinionated starting point...golden/paved paths that handle the common cases...but offer the proper escape hatches when teams need to solve their specific human scale and tool sprawl problems.\\n\\n## The Golden/Paved Path Approach\\n\\nThe key is building **opinionated starter paths**\u2014what some may refer to as paved or golden paths. Instead of making people remember how to set up monitoring, security scanning, and deployment pipelines every time, these paths just do the right thing by default.\\n\\nWhy do they work? Because we\'ve all been through this before. We\'ve seen the patterns emerge, tried and failed a few times, and collectively figured out what the rational package should look like. With AI assistance, they\'re getting even smarter\u2014understanding your specific context and helping orchestrate complex operations through simple conversation.\\n\\nThink about how we work with platforms today. You want to deploy a service, so you open a dozen browser tabs, remember which CLI commands go with which environment, hunt for the right Slack channel to ask about permissions, and somehow piece together a deployment that mostly works.\\n\\nWhat if instead, you could just say \\"I need to deploy this API with the standard security setup\\" and the system figured out the rest? The AI becomes the bridge between what you\'re trying to accomplish and all the tools that need to coordinate to make it happen.\\n\\n## The Conversation Layer\\n\\nInstead of training everyone on every tool, we create AI-powered interaction layers that make platform expertise accessible through natural conversation, whether that\'s in their IDE, in Slack, or through a CLI that understands context and can perform complex workflows.\\n\\nBetter yet, the AI can document what we learn as we learn it...writing up new knowledge and evolving best practices automatically. This eliminates the toil of properly documenting discoveries and keeps our institutional/tribal knowledge current without requiring someone to stop their actual work to write it all down.\\n\\nAgain the end goal isn\'t to eliminate human expertise...it\'s to amplify it and make it accessible at the point and time of need, in the context where people are already working.\\n\\n---\\n\\n**Next in this series**: We\'ll explore how environmental inconsistency and governance gaps create reliability nightmares at enterprise scale, and why \\"it works on my machine\\" becomes a critical business problem when multiplied across hundreds of teams.\\n\\n*Have you experienced this productivity paradox in your organization? How much time do your developers spend fighting with tools versus building features?*\\n\\nCross Posted to: https://www.linkedin.com/pulse/when-infrastructure-scales-understanding-doesnt-ray-kao-2rpic\\n\\n#PlatformEngineering #DevOps #DeveloperProductivity #AI #AgenticDevOps"},{"id":"/2025/06/24/human-scale-problem-in-platform-engineering","metadata":{"permalink":"/2025/06/24/human-scale-problem-in-platform-engineering","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2025-06-24/human-scale-problem-in-platform-engineering/index.md","source":"@site/blog/2025-06-24/human-scale-problem-in-platform-engineering/index.md","title":"The Human Scale Problem in Platform Engineering","description":"Platform engineering insights focused on scaling human capability, not just technical systems. Practical approaches to making complex infrastructure more humane and accessible.","date":"2025-06-24T00:00:00.000Z","tags":[],"readingTime":6.03,"hasTruncateMarker":true,"authors":[{"name":"Ray Kao","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/raykao","socials":{"x":"https://x.com/raykao","github":"https://github.com/raykao"},"imageURL":"https://github.com/raykao.png","key":"ray_kao","page":null},{"name":"Diego Casati","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/dcasati","socials":{"x":"https://x.com/ve6dpc","github":"https://github.com/dcasati"},"imageURL":"https://github.com/dcasati.png","key":"diego_casati","page":null}],"frontMatter":{"authors":["ray_kao","diego_casati"],"date":"2025-06-24","description":"Platform engineering insights focused on scaling human capability, not just technical systems. Practical approaches to making complex infrastructure more humane and accessible.","tags":[],"title":"The Human Scale Problem in Platform Engineering"},"unlisted":false,"prevItem":{"title":"When Infrastructure Scales But Understanding Doesn\'t","permalink":"/2025/07/02/when-infrastructure-scales-but-understanding-doesnt"},"nextItem":{"title":"Updating AKS Network Plugin from Kubenet to Azure CNI","permalink":"/2025/01/31/updating-aks-network-plugin-from-kubenet-to-azure-cni"}},"content":"We keep doing this thing where we solve a problem, celebrate the victory, then realize we\'ve created three new problems we didn\'t even know existed.\\n\\nRemember when manually configuring servers was the bottleneck? So we built containers. Great! Now we\'re orchestrating thousands of them. Remember when monolithic deployments were too slow? So we built microservices. Fantastic! Now we\'re drowning in distributed system complexity. We solved manual infrastructure provisioning with infrastructure as code. Perfect! Now we\'re coordinating dozens of Terraform modules across environments and wondering how we got here.\\n\\nEach step forward has been genuinely valuable. But we keep hitting the same pattern: **our solutions outpace our ability to operate them at human scale**.\\n\\n\x3c!-- truncate --\x3e\\n\\n## The Knowledge Explosion\\n\\nThink about what we\'re asking people to know now. Container orchestration, service mesh configuration, observability platforms, security policy engines, AI/ML operations\u2014roles that didn\'t even exist a decade ago. No single person can be an expert in all of this, and that\'s actually okay.\\n\\nWhat we\'ve created is **necessary specialization**. We need people who can go deep on specific problem domains because the breadth of knowledge has expanded beyond what any individual can reasonably master. Silos aren\'t inherently bad\u2014they\'re a natural response to complexity that exceeds what one person can hold in their head.\\n\\nThe real problem isn\'t that we have silos. Silos are actually fine.  Creating clear roles/responsibilities is a good thing.  It\'s that **we\'re terrible at helping these specialists talk to each other**.\\n\\n## The Coordination Crisis\\n\\nPlatform engineering is our attempt to build bridges between these specialists\u2014creating **interfaces and abstractions** that let them work together without everyone needing to become an expert in everything. When it works, specialists can collaborate without understanding every detail of each other\'s domains, while **guardrails** ensure their work still fits together properly.\\n\\nBut let\'s be honest\u2014we\'re still figuring this out. We\'re just now learning what the **proper handoffs and demarcation lines** should be between security engineers, platform engineers, SREs, and application developers. Even when we think we\'ve drawn the boundaries correctly, there are still **knowledge gaps and coordination challenges** wherever these roles overlap each other.\\n\\nAnd then there\'s the tool problem. We have so many overlapping options that nobody knows who should own what. Should observability be owned by the platform team using Prometheus, or the SRE team using Datadog, or the application teams using Azure Monitor? Who owns the deployment pipeline\u2014platform engineering with their GitOps approach, or the dev teams who want to use GitHub Actions and push directly?\\n\\nMany organizations find themselves **paralyzed by choice**, afraid to commit to specific tools because of vendor lock-in concerns or the fear of having to rebuild their entire process when better alternatives emerge. So they end up with tool sprawl across teams, each solving similar problems in slightly different ways, creating even more coordination complexity.\\n\\n## The Monument Problem\\n\\nHere\'s where we keep making the same mistake: we treat software and engineering problems as one-and-done challenges. We chase perfect code and perfect execution, hoping that if we just build it right the first time, we\'ll never need to touch it again. This has been the root of our collective error.\\n\\nLook at something like mainframe systems (...this is not a punch down at mainframe momment by the way...) \u2014 brilliant solutions, built with the assumption they\'d last forever. But the world evolved beyond the paradigm it represented, and now organizations are struggling to rebuild decades of accumulated logic and debt because the original systems had no way to introspect, understand, or extend beyond their initial design and/or we felt like the problem was solved so we didn\'t bother to adjust and evolve it.\\n\\n**We built monuments instead of living systems**. The solutions we build today must be designed differently.\\n\\n## The Current Moment\\n\\nWhat I\'m seeing across organizations\u2014and I think we\'re all experiencing this \u2014 is that these aren\'t really new problems. They\'re the same fundamental challenges we\'ve always had, just amplified by cloud computing, microservices, and the reality of building software at \\"enterprise scale\\". Platform engineering is **the current term our industry is using to rally around addressing** these amplified **human** problems.\\n\\nBut here\'s what\'s really exciting about this moment in our industry: we\'re not just trying to encode operational knowledge into platforms/scripts/docs anymore. **We\'re discovering new ways to make systems more humane through AI assistance** \u2014 creating interfaces that finally match how humans naturally want to interact with complex technology.\\n\\nThe difference isn\'t that AI is magic; it\'s that **we\'ve built sufficently complex systems that conversational interfaces are required and we now have tools available to us that can truly start to make sense of them WITH US**.\\n\\n## The Fundamental Shift\\n\\nWe already have the fundamental tools for this job. Source control, automation runners, infrastructure as code, API calls...endless API calls. These aren\'t going anywhere. They\'re battle-tested, timeless, and they\'ll remain at the core of what we do in platform engineering. Even AI will ultimately read from and output to this.  The challenge isn\'t finding new tools, it\'s orchestrating the ones we have into coherent, user-friendly experiences.\\n\\nWhat\'s interesting about AI assistants is that they can become like that experienced colleague who remembers not just what we did, but why we did it. They can hold onto the collective knowledge we\'ve built up over time, understanding which tools to use, grasping the reasoning behind our decisions, and remembering the rules and governance that shaped those choices. This institutional/tribal memory becomes accessible through simple conversation instead of forcing people to dig through documentation or doing intensive knowledge transfers that still don\'t fully cover the breadth and depth of what we need to build and support.\\n\\n## What This Means for You\\n\\nThe common thread across all engineering challenges (or any task really) is that they\'re fundamentally about scaling human capability, not just technical capability. The limiting factor in most organizations isn\'t compute capacity or network bandwidth...it\'s human cognitive capacity and the work items that are bound by human scale, such as time and speed.\\n\\nPlatform engineering recognizes this reality and focuses on **amplifying human expertise** rather than expecting people to become superhuman. It\'s about building systems that make complex operations incrementally more understandable, moving towards enabling teams to focus on solving business problems instead of fighting with infrastructure, and create sustainable pathways for specialists to collaborate effectively.\\n\\nThis isn\'t a problem we solve once and move on from. The challenges evolve as our organizations grow, as new technologies emerge, and as our understanding deepens. When we see a new pattern emerge that we\'ve had a discussion about 5, 10, 20 times??? then we need as platform engineers to create a pattern for it.  The solutions we build today must be designed to adapt and evolve with us.\\n\\n---\\n\\n**Next in this series**: We\'ll dive into the specific challenge of scaling infrastructure complexity beyond human cognitive capacity, and explore how the productivity paradox is affecting development teams across the industry.\\n\\n*What\'s your experience with the human scale problem in your organization? Are you seeing similar patterns where solutions create new coordination challenges?*\\n\\nCross Posted to: https://www.linkedin.com/pulse/human-scale-problem-platform-engineering-ray-kao-cnjhc/\\n\\n#PlatformEngineering #DevOps #DeveloperExperience #AgenticDevOps #AzureGlobalBlackBelts #FieldNotes"},{"id":"/2025/01/31/updating-aks-network-plugin-from-kubenet-to-azure-cni","metadata":{"permalink":"/2025/01/31/updating-aks-network-plugin-from-kubenet-to-azure-cni","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2025-01-31/updating-aks-network-plugin-from-kubenet-to-azure-cni/index.md","source":"@site/blog/2025-01-31/updating-aks-network-plugin-from-kubenet-to-azure-cni/index.md","title":"Updating AKS Network Plugin from Kubenet to Azure CNI","description":"A procedure on how to update an AKS cluster network plugin, from Kubenet to Azure CNI Overlay Mode.","date":"2025-01-31T00:00:00.000Z","tags":[],"readingTime":1.54,"hasTruncateMarker":true,"authors":[{"name":"Diego Casati","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/dcasati","socials":{"x":"https://x.com/ve6dpc","github":"https://github.com/dcasati"},"imageURL":"https://github.com/dcasati.png","key":"diego_casati","page":null}],"frontMatter":{"authors":["diego_casati"],"date":"2025-01-31","description":"A procedure on how to update an AKS cluster network plugin, from Kubenet to Azure CNI Overlay Mode.","tags":[],"title":"Updating AKS Network Plugin from Kubenet to Azure CNI"},"unlisted":false,"prevItem":{"title":"The Human Scale Problem in Platform Engineering","permalink":"/2025/06/24/human-scale-problem-in-platform-engineering"},"nextItem":{"title":"End to End TLS Encryption with AKS and AFD","permalink":"/2024/11/05/afd-aks-ingress-tls"}},"content":"## Problem Statement\\n\\nWhen updating an Azure Kubernetes Service (AKS) network plugin from **kubenet** to **Azure CNI**, performing the update directly using **Terraform** may result in the cluster being deleted and recreated. The Terraform plan typically indicates that the cluster will be replaced.\\n\\nHowever, using **Azure CLI (az cli)**, the update can be successfully applied without deleting the cluster. Once that\'s done, you can import the state of the cluster back to terraform.\\n\\nA validated approach to avoid cluster recreation involves the following steps:\\n\\n- Perform the **CNI update** out-of-band using **Azure CLI**.\\n- **Import the state** of the cluster back to Terraform.\\n- Perform a **Terraform refresh** and a **Terraform plan** to validate the new state.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Procedure\\n\\n### **Step 1: Perform the upgrade using Azure CLI**\\n\\n```bash\\nexport clusterName=aks-kubenet-cluster\\nexport resourceGroup=aks-kubenet-rg\\n\\naz aks update --name $clusterName \\\\\\n  --resource-group $resourceGroup \\\\\\n  --network-plugin azure \\\\\\n  --network-plugin-mode overlay\\n```\\nWhile is happening, AKS will create a new node, updated with the new network options. Once that is done, the older nodes will be removed, and eventually the entire cluster will be updated.\\n\\n![AKS nodes](/img/2025-01-31-updating-aks-network-plugin-from-kubenet-to-azure-cni/portal.jpg)\\n\\n### **Step 2: Modify Terraform Configuration**\\n\\nUpdate the Terraform configuration to reflect the new **Azure CNI** network plugin settings.\\n\\n```\\n  network_profile {\\n    network_plugin = \\"azure\\"\\n    network_plugin_mode = \\"overlay\\"\\n  }\\n```\\n\\n### **Step 3: Import the new cluster state into Terraform**\\n\\n```bash\\nterraform import azurerm_kubernetes_cluster.aks \\\\\\n  \\"/subscriptions/6edaa0d4-86e4-431f-a3e2-d027a34f03c9/resourceGroups/aks-kubenet-rg/providers/Microsoft.ContainerService/managedClusters/aks-kubenet-cluster\\"\\n```\\n\\n### **Step 4: Run Terraform Refresh and Plan**\\n\\n```bash\\nterraform refresh\\nterraform plan\\n```\\n\\nThis ensures that Terraform recognizes the new state of the AKS cluster without attempting to recreate it.\\n\\n## Conclusion\\n\\nBy following this approach, it is possible to transition an **AKS network plugin from kubenet to Azure CNI** while avoiding unnecessary cluster deletion and recreation."},{"id":"/2024/11/05/afd-aks-ingress-tls","metadata":{"permalink":"/2024/11/05/afd-aks-ingress-tls","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2024-11-05/afd-aks-ingress-tls/index.md","source":"@site/blog/2024-11-05/afd-aks-ingress-tls/index.md","title":"End to End TLS Encryption with AKS and AFD","description":"Using Azure Front Door in-front of an in-cluster nginx ingress controller to provide end-to-end TLS encryption of application traffic.","date":"2024-11-05T00:00:00.000Z","tags":[],"readingTime":17.05,"hasTruncateMarker":true,"authors":[{"name":"Steve Griffith","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/swgriffith","socials":{"x":"https://x.com/SteveGriffith","github":"https://github.com/swgriffith"},"imageURL":"https://github.com/swgriffith.png","key":"steve_griffith","page":null}],"frontMatter":{"authors":["steve_griffith"],"date":"2024-11-05","description":"Using Azure Front Door in-front of an in-cluster nginx ingress controller to provide end-to-end TLS encryption of application traffic.","tags":[],"title":"End to End TLS Encryption with AKS and AFD"},"unlisted":false,"prevItem":{"title":"Updating AKS Network Plugin from Kubenet to Azure CNI","permalink":"/2025/01/31/updating-aks-network-plugin-from-kubenet-to-azure-cni"},"nextItem":{"title":"Multi-Cluster Layer 4 Load Balancing with Fleet Manager","permalink":"/2024/09/06/multi-cluster-layer-4-load-balancing-with-fleet-manager"}},"content":"## Introduction\\n\\nIn this walkthrough we\'ll create deploy an app with end to end TLS encryption, using Azure Front Door as the Internet Facing TLS endpoint and an Nginx Ingress controller running inside an AKS cluster as the backend. \\n\\nWe\'ll use Azure Key Vault to store the TLS certificate, and will use the Key Vault CSI Driver to get the secrets into the ingress controller. The Key Vault CSI Driver will use Azure Workload Identity to safely retrieve the certificate.\\n\\n\x3c!-- truncate --\x3e\\n\\nLet\'s get to it....\\n\\n## Network Setup\\n\\nFirst, we\'ll need to establish the network where our AKS cluster will be deployed. Nothing special in our network design, other than the fact that I\'m creating an Azure Network Security Group at the subnet level for added security.\\n\\n```bash\\n# Resource Group Creation\\nRG=E2ETLSLab\\nLOC=eastus2\\n\\n# Create the Resource Group\\naz group create -g $RG -l $LOC\\n\\n# Get the resource group id\\nRG_ID=$(az group show -g $RG -o tsv --query id)\\n\\n# Set an environment variable for the VNet name\\nVNET_NAME=lablab-vnet\\nVNET_ADDRESS_SPACE=10.140.0.0/16\\nAKS_SUBNET_ADDRESS_SPACE=10.140.0.0/24\\n\\n# Create an NSG at the subnet level for security reasons\\naz network nsg create \\\\\\n--resource-group $RG \\\\\\n--name aks-subnet-nsg\\n\\n# Get the NSG ID\\nNSG_ID=$(az network nsg show -g $RG -n aks-subnet-nsg -o tsv --query id)\\n\\n# Create the Vnet along with the initial subet for AKS\\naz network vnet create \\\\\\n-g $RG \\\\\\n-n $VNET_NAME \\\\\\n--address-prefix $VNET_ADDRESS_SPACE \\\\\\n--subnet-name aks \\\\\\n--subnet-prefix $AKS_SUBNET_ADDRESS_SPACE \\\\\\n--network-security-group aks-subnet-nsg\\n\\n# Get a subnet resource ID, which we\'ll need when we create the AKS cluster\\nVNET_SUBNET_ID=$(az network vnet subnet show -g $RG --vnet-name $VNET_NAME -n aks -o tsv --query id)\\n```\\n\\n## Cluster Creation\\n\\nNow, lets create the AKS cluster where our workload and ingress controller will reside. This will be a very plain AKS cluster, however we will deploy to our above created subnet and will enable the following features:\\n\\n- *Workload Identity:* This will be used by the Key Vault CSI Driver to retrieve the cluster certificate\\n- *OIDC Issuer:* This is required by Workload Identity to be used during service account fedration\\n- *Key Vault CSI Driver:* This will be used to retrieve the cluster certificate\\n\\n> *Note:* Since I created a NSG at the subnet level, I\'ll need to create a custom role which will be used later for automated private link creation. If you don\'t have an NSG on the subnet, and just rely on the managed NSG that AKS owns, then you don\'t need to create the custom role documented below. \\n\\n```bash\\n# NOTE: Make sure you give your cluster a unique name\\nCLUSTER_NAME=e2etlslab\\n\\n# Cluster Creation Command\\naz aks create \\\\\\n-g $RG \\\\\\n-n $CLUSTER_NAME \\\\\\n--node-count 2 \\\\\\n--enable-oidc-issuer \\\\\\n--enable-workload-identity \\\\\\n--enable-addons azure-keyvault-secrets-provider \\\\\\n--vnet-subnet-id $VNET_SUBNET_ID\\n\\n# Get the cluster identity\\nCLUSTER_IDENTITY=$(az aks show -g $RG -n $CLUSTER_NAME -o tsv --query identity.principalId)\\n\\n###################################################################################################\\n# Grant the cluster identity rights on the cluster nsg, which we\'ll need later when we create the\\n# private link.\\n\\n# NOTE: These steps are only needed if you have a custom NSG on the cluster subnet.\\n\\n# Create the role definition file\\ncat << EOF > pl-nsg-role.json\\n{\\n    \\"Name\\": \\"Private Link AKS Role\\",\\n    \\"Description\\": \\"Grants the cluster rights on the NSG for Private Link Creation\\",\\n    \\"Actions\\": [\\n        \\"Microsoft.Network/networkSecurityGroups/join/action\\"\\n    ],\\n    \\"NotActions\\": [],\\n    \\"DataActions\\": [],\\n    \\"NotDataActions\\": [],\\n    \\"assignableScopes\\": [\\n        \\"${RG_ID}\\"\\n    ]\\n}\\nEOF\\n\\n# Create the role definition in Azure\\naz role definition create --role-definition @pl-nsg-role.json\\n\\n# Assign the role\\n# NOTE: New role propegation may take a minute or to, so retry as needed\\naz role assignment create \\\\\\n--role \\"Private Link AKS Role\\" \\\\\\n--assignee $CLUSTER_IDENTITY \\\\\\n--scope $NSG_ID\\n###################################################################################################\\n\\n\\n# Get credentials\\naz aks get-credentials -g $RG -n $CLUSTER_NAME\\n```\\n\\n## Setup Workload Identity\\n\\nNow that we have our cluster, lets finish setting up the workload identity that will be used to retrieve our certificate from Azure Key Vault.\\n\\n>*Note:* For simplicity, I\'m keeping all resources in the \'default\' namespace. You may want to modify this for your own deployment.\\n\\n```bash\\n# Set the namespace where we will deploy our app and ingress controller\\nNAMESPACE=default\\n\\n# Get the OIDC Issuer URL\\nexport AKS_OIDC_ISSUER=\\"$(az aks show -n $CLUSTER_NAME -g $RG --query \\"oidcIssuerProfile.issuerUrl\\" -otsv)\\"\\n\\n# Get the Tenant ID for later\\nexport IDENTITY_TENANT=$(az account show -o tsv --query tenantId)\\n\\n# Create the managed identity\\naz identity create --name nginx-ingress-identity --resource-group $RG --location $LOC\\n\\n# Get identity client ID\\nexport USER_ASSIGNED_CLIENT_ID=$(az identity show --resource-group $RG --name nginx-ingress-identity --query \'clientId\' -o tsv)\\n\\n# Create a service account to federate with the managed identity\\ncat <<EOF | kubectl apply -f -\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  annotations:\\n    azure.workload.identity/client-id: ${USER_ASSIGNED_CLIENT_ID}\\n  labels:\\n    azure.workload.identity/use: \\"true\\"\\n  name: nginx-ingress-sa\\n  namespace: ${NAMESPACE}\\nEOF\\n\\n# Federate the identity\\naz identity federated-credential create \\\\\\n--name nginx-ingress-federated-id \\\\\\n--identity-name nginx-ingress-identity \\\\\\n--resource-group $RG \\\\\\n--issuer ${AKS_OIDC_ISSUER} \\\\\\n--subject system:serviceaccount:${NAMESPACE}:nginx-ingress-sa\\n```\\n\\n## Create the Azure Key Vault and Upload Certificate\\n\\nNow, lets create our Azure Key Vault and then create and upload our certificate.\\n\\n```bash\\n# Create a key vault name\\nKEY_VAULT_NAME=e2elab$RANDOM\\n\\n# Create the key vaule\\naz keyvault create --name $KEY_VAULT_NAME --resource-group $RG --location $LOC --enable-rbac-authorization false\\n\\n# Grant access to the secret for the managed identity\\naz keyvault set-policy --name $KEY_VAULT_NAME -g $RG --certificate-permissions get --spn \\"${USER_ASSIGNED_CLIENT_ID}\\"\\naz keyvault set-policy --name $KEY_VAULT_NAME -g $RG --secret-permissions get --spn \\"${USER_ASSIGNED_CLIENT_ID}\\"\\n```\\n\\nFor certificate creation, I actually took two separate paths.\\n\\n- *Option 1:* Use Azure [App Service Certificates](https://learn.microsoft.com/en-us/azure/app-service/configure-ssl-app-service-certificate?tabs=portal) to create and manage the certificate.\\n- *Option 2:* Use \'[LetsEncrypt](https://letsencrypt.org/)\' and [Certbot](https://certbot.eff.org/) to create the certificate.\\n\\nBoth options are totally fine, but have slight differences in approach, which I\'ll highlight below. If you\'re working in a large enterprise, you\'ll likely have a completely separate internal process for getting a certificate. In the end, all we care about is that we have a valid cert and key file.\\n\\n\\n### Option 1: App Service Certificates\\n\\nWe won\'t cover all the details of setting up a certificate with App Service Certificates, but you can review the doc [here](https://learn.microsoft.com/en-us/azure/app-service/configure-ssl-app-service-certificate?tabs=portal) for those steps.\\n\\nWhen I created the certificate, I told App Svc Certs to store the cert in the Key Vault just created. We\'ll use the [Key Vault CSI Driver](https://learn.microsoft.com/en-us/azure/aks/csi-secrets-store-driver) to mount the certificate into the ingress controller, but to do that we need to get the certficate into a format that the CSI driver can read. App Svc Certs stores the certificate in an Azure Key Vault in pfx format as a secret, but for the Key Vault CSI Driver we need it stored as a certificate. We can export the PFX from the Azure Key Vault Secret and then import it as a certificate.\\n\\n```bash\\nAPP_CERT_NAME=e2elab\\n\\n# Get the secret name for the certificate in key vault\\nSECRET_NAME=$(az resource show --resource-group $RG --resource-type \\"Microsoft.CertificateRegistration/certificateOrders\\" --name $APP_CERT_NAME --query \\"properties.certificates.$APP_CERT_NAME.keyVaultSecretName\\" --output tsv)\\n\\n# Download the certificate \\naz keyvault secret download --file $APP_CERT_NAME.pfx --vault-name $KEY_VAULT_NAME --name $SECRET_NAME --encoding base64\\n\\n# Import the certificate\\naz keyvault certificate import --vault-name $KEY_VAULT_NAME --name $APP_CERT_NAME --file $APP_CERT_NAME.pfx\\n```\\n\\n### Option 2: LetsEncrypt/CertBot\\n\\nI\'m not going to get into all the specifics of using Certbot with LetsEncrypt, but the basic are as follows. The domain I\'ll be using is my \'crashoverride.nyc\' domain.\\n\\n1. Get an internet reachable host capable of running a webserver on ports 80 and 443\\n2. Create an A-record for your target domain to the public IP of the server. This is required for hostname validation used by Certbot\\n3. Run the certbot command as a priviledged user on that web server host mentioned in #1 above\\n\\nHere\'s a sample of the command I used to create a cert with two entries in the certificate Subject Alternate Names:\\n\\n```bash\\nsudo certbot certonly --key-type rsa --standalone -d e2elab.crashoverride.nyc -d www.crashoverride.nyc\\n```\\n\\nCertbot creates several files, all with the PEM file extension. This is misleading, as fullchain.pem is the \'crt\' file and the privkey.pem is the \'key\' file. To store these in Azure Key Vault as certs we\'ll need to package these files up in a PFX format.\\n\\n```bash\\nAPP_CERT_NAME=crashoverride\\n\\n# export to pfx\\n# skipping the Password prompt\\nopenssl pkcs12 -export -passout pass: -in fullchain.pem -inkey privkey.pem  -out ${APP_CERT_NAME}.pfx\\n\\n# Import the certificate\\naz keyvault certificate import --vault-name $KEY_VAULT_NAME --name $APP_CERT_NAME --file $APP_CERT_NAME.pfx\\n```\\n\\n## Set up the Key Vault CSI Secret Provider Class\\n\\nGreat! Now we have our network, cluster, key vault and secrets ready to go. We can now create our SecretProviderClass, which is the link between our Kubernetes resources and the secret in Azure Key Vault. We\'ll actually use this SecretProviderClass to mount the certificate into our ingress controller.\\n\\n>*Note:* The Key Vault CSI driver uses the Kubernetes Container Storage Interface to initiate it\'s connection to Key Vault. That means, even though we really only care about having the certificate as a Kubernetes secret for use in our ingress definition, we still need to mount the secret as a volume to create the Kubernetes Secret. We\'ll mount the certificate secret to the ingress controller, but you could mount it to your app alternatively, especially if you plan to use the certificate in your app directly as well.\\n\\nWhen you create a certificate in Azure Key Vault and then use the Key Vault CSI driver to access that certificate, you use the \'secret\' object type and the certificate name. The returned secret contains the certificate key and crt file using the names tls.key and tls.crt. \\n\\n```bash\\n# We\'ll cat the SecretProviderClass direclty into kubectl\\n# You could also just cat it out to a file and use that file to deploy\\ncat << EOF | kubectl apply -f -\\napiVersion: secrets-store.csi.x-k8s.io/v1\\nkind: SecretProviderClass\\nmetadata:\\n  name: crashoverride-tls\\n  namespace: ${NAMESPACE}\\nspec:\\n  provider: azure\\n  secretObjects:                            # secretObjects defines the desired state of synced K8s secret objects\\n    - secretName: crashoverride-tls-csi\\n      type: kubernetes.io/tls\\n      data: \\n        - objectName: crashoverride\\n          key: crashoverride.key\\n        - objectName: crashoverride\\n          key: crashoverride.crt\\n  parameters:\\n    usePodIdentity: \\"false\\"\\n    clientID: ${USER_ASSIGNED_CLIENT_ID}\\n    keyvaultName: ${KEY_VAULT_NAME}                 # the name of the AKV instance\\n    objects: |\\n      array:\\n        - |\\n          objectName: crashoverride\\n          objectType: secret\\n    tenantId: ${IDENTITY_TENANT}                    # the tenant ID of the AKV instance\\nEOF\\n```\\n\\n## Deploy the Ingress Controller\\n\\nNow we\'re ready to create our ingress controller. For our purposes, and for it\'s simplicity, we\'re going to use ingress-nginx. The approach will be roughly the same for any ingress controller.\\n\\nThere are a few key points to note in the deployment below.\\n\\n1. We want our ingress controller to be on an internal network with no public IP, since Azure Front Door will provide the public endpoint. To do that we\'ll need to apply the \'azure-load-balancer-internal\' annotation.\\n2. Azure Front Door can only connect to a private IP address using an Azure Private Link Service. Fortunately, AKS provides the \'azure-pls-create\' annotation which will automatically create and manage a private link for you.\\n3. As mentioned above, since we\'re using the Key Vault CSI Driver, we need to mount the Secret Provider Class using the secret-store driver. \\n\\n```bash\\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\\nhelm repo update\\n\\n# Generate the values file we\'ll use to deploy ingress-nginx\\ncat <<EOF > nginx-ingress-values.yaml\\nserviceAccount:\\n  create: false\\n  name: nginx-ingress-sa\\ncontroller:\\n  replicaCount: 2\\n  service:\\n    annotations:\\n      service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path: /healthz\\n      service.beta.kubernetes.io/azure-load-balancer-internal: \\"true\\"\\n      service.beta.kubernetes.io/azure-pls-create: \\"true\\"\\n  extraVolumes:\\n      - name: crashoverride-secret-store\\n        csi:\\n          driver: secrets-store.csi.k8s.io\\n          readOnly: true\\n          volumeAttributes:\\n            secretProviderClass: \\"crashoverride-tls\\"            \\n  extraVolumeMounts:\\n      - name: crashoverride-secret-store\\n        mountPath: \\"/mnt/crashoverride\\"\\n        readOnly: true        \\nEOF\\n\\n# Deploy ingress-nginx\\nhelm install e2elab-ic ingress-nginx/ingress-nginx \\\\\\n    --namespace $NAMESPACE \\\\\\n    -f nginx-ingress-values.yaml\\n```\\n\\n## Deploy a Sample App\\n\\nWe\'ll need a sample app to test our configuration. I personally like the \'[echoserver](https://github.com/cilium/echoserver)\' app from the cilium team. It\'s nice, as it returns the HTTP headers as the web response, which can be very useful in http request testing.\\n\\n```bash\\ncat <<EOF | kubectl apply -f -\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: aks-helloworld \\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app: aks-helloworld\\n  template:\\n    metadata:\\n      labels:\\n        app: aks-helloworld\\n    spec:\\n      containers:\\n      - name: aks-helloworld\\n        image: cilium/echoserver\\n        ports:\\n        - containerPort: 8080\\n        env:\\n        - name: PORT\\n          value: \'8080\'\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: aks-helloworld\\nspec:\\n  type: ClusterIP\\n  ports:\\n  - port: 8080\\n  selector:\\n    app: aks-helloworld\\nEOF\\n```\\n\\nNow we can deploy the ingress defintion. Look out for the following:\\n\\n1. The \'tls\' section maps the inbound TLS request to the appropriate certificate secret\\n2. The \'rules\' section maps the target host name to the backend service that should be targetted\\n\\n```bash\\ncat <<EOF|kubectl apply -f -\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: crashoverride-ingress-tls\\n  annotations:\\n    nginx.ingress.kubernetes.io/use-regex: \\"true\\"\\n    nginx.ingress.kubernetes.io/rewrite-target: /$1\\nspec:\\n  ingressClassName: nginx\\n  tls:\\n  - hosts:\\n    - e2elab.crashoverride.nyc\\n    secretName: crashoverride-tls-csi \\n  rules:\\n  - host: e2elab.crashoverride.nyc\\n    http:\\n      paths:\\n      - path: /hello-world\\n        pathType: Prefix\\n        backend:\\n          service:\\n            name: aks-helloworld\\n            port:\\n              number: 8080\\nEOF\\n```\\n\\nSince we haven\'t set up the Azure Front Door yet, we cant access the app on the public IP yet. We can fake this out, however, with some curl magic. curl lets you call a local endpoint and pretend like you\'re connecting to a different host name. We\'ll need this for our ingress certificate to work.\\n\\nFirst we port-forward and then we\'ll curl with some special options.\\n\\n>*Note:* If you prefer, you can also just edit your local \'hosts\' file to fake out the DNS lookup. Just create an entry that maps your local loopback address (127.0.0.1) to your DNS name.\\n\\n```bash\\n# In terminal 1, port-forward to the ingress nginx service name\\nkubectl port-forward svc/e2elab-ic-ingress-nginx-controller 8443:443\\n\\n# In terminal 2 run a curl like the following, changing out for your host name\\ncurl -v https://e2elab.crashoverride.nyc/hello-world --connect-to e2elab.crashoverride.nyc:443:127.0.0.1:8443\\n```\\n\\nYou should have seen a successful TLS handshake with your certificate and proper hostname.\\n\\n```bash\\n# Example\\n* Server certificate:\\n*  subject: CN=e2elab.crashoverride.nyc\\n*  start date: Oct 31 16:35:44 2024 GMT\\n*  expire date: Jan 29 16:35:43 2025 GMT\\n*  subjectAltName: host \\"e2elab.crashoverride.nyc\\" matched cert\'s \\"e2elab.crashoverride.nyc\\"\\n*  issuer: C=US; O=Let\'s Encrypt; CN=R10\\n*  SSL certificate verify ok.\\n```\\n\\n\\n## Create the Azure Front Door\\n\\nNow that the backend is working, lets wire up the Azure Front Door.\\n\\n```bash\\n# Create the Azure Front Door\\naz afd profile create \\\\\\n--profile-name e2elab \\\\\\n--resource-group $RG \\\\\\n--sku Premium_AzureFrontDoor\\n```\\n\\nWe\'ll do the rest in the Azure Portal, so open a browser to [https://portal.azure.com](https://portal.azure.com).\\n\\n### Link the certificate to the AFD.\\n\\nTo use our certificate with Azure Front Door, we need to attach the certificate in Azure Key Vault to an Front Door Secret. We do this in the \'Secrets\' pane under \'Security\'.\\n\\n![link certificate](/img/2024-11-05-afd-aks-ingress-tls/linkcert.jpg)\\n\\n### Create the Custom Domain Configuration\\n\\nNow we tell AFD what domain we\'d like to use and link that domain name to the associated secret that we just created for our certificate.\\n\\n![afd add domain 1](/img/2024-11-05-afd-aks-ingress-tls/afd-adddomain1.jpg)\\n\\nNext we select the appropriate secret for our domain.\\n\\n![afd add domain 2](/img/2024-11-05-afd-aks-ingress-tls/afd-adddomain2.jpg)\\n\\nFinally, we see the domain entry created and pending association with an endpoint.\\n\\n![afd add domain 3](/img/2024-11-05-afd-aks-ingress-tls/afd-adddomain3.jpg)\\n\\n\\n### Create the Origin Group\\n\\nFront Door is acting as the entry point to our backend, which is referred to as the \'Origin\'.\\n\\nCreating the origin group is a two step process. You create the origin group, but as part of that you also add the origin hostname configuration. As part of that origin hostname setup you will check the \'Enable Private Link Service\' option, which will allow you to select the private link that was automatically created by AKS for your ingress-nginx deployment. This is why the service annotation was so important when you deployed ingress-nginx.\\n\\nYou\'ll provide a message that will show up on the private link approval side. This message can be whatever you want.\\n\\n![origin setup 1](/img/2024-11-05-afd-aks-ingress-tls/origin-setup1.jpg)\\n\\nNow we complete our origin setup, making sure to set the right path to our ingress health probe. In our case, the URL will forward to \'/hello-world\', as we know this will return an HTTP 200 response. If you have your own health endpoint, you can set that here.\\n\\n![origin setup 2](/img/2024-11-05-afd-aks-ingress-tls/origin-setup2.jpg)\\n\\nNow we see that our origin is created, but still pending association with an endpoint.\\n\\n![origin setup 3](/img/2024-11-05-afd-aks-ingress-tls/origin-setup3.jpg)\\n\\n### Create the AFD Endpoint\\n\\nIn \'Front Door manager\' select \'Add an endpoint\' and give the endpoint a name. Make note of the FQDN is provides. This will be used in our DNS for the CNAME.\\n\\n![add an endpoint 1](/img/2024-11-05-afd-aks-ingress-tls/addendpoint1.jpg)\\n\\nNow we\'ll add a route by clicking \'Add a route\'.\\n\\n![add an endpoint 2](/img/2024-11-05-afd-aks-ingress-tls/addendpoint2.jpg)\\n\\nIn the \'add route\' screen, we\'ll select our origin and the associated domain, and set any additional options. At this point, you should also make note of the endpoint FQDN, which we\'ll need to use as our CNAME in our DNS for our host name.\\n\\n![add an endpoint 3](/img/2024-11-05-afd-aks-ingress-tls/addendpoint3.jpg)\\n\\n![add an endpoint 4](/img/2024-11-05-afd-aks-ingress-tls/addendpoint4.jpg)\\n\\nWhen finished, your endpoint should look as follows.\\n\\n![add an endpoint 5](/img/2024-11-05-afd-aks-ingress-tls/addendpoint5.jpg)\\n\\n### Update your DNS\\n\\nWe need our DNS name to point to the Azure Front Door endpoint, so we\'ll take that Front Door provided FQDN and create a CNAME record. \\n\\n![dns cname entry](/img/2024-11-05-afd-aks-ingress-tls/dnscname.jpg)\\n\\n### Approve the Private Link request\\n\\nOk, so we associated the Azure Front Door origin with our private link, but we never approved the private link association request. To do that, we\'ll need to go to the AKS managed cluster (MC_) resource group. Lets get that resource group name and then go approve the request.\\n\\n```bash\\n# Get the managed cluster resource group name\\nAKS_CLUSTER_MC_RG=$(az aks show -g $RG -n $CLUSTER_NAME -o tsv --query nodeResourceGroup)\\n```\\n\\nBack in the Azure Portal, navigate to the Managed Cluster Resource Group and find the private link.\\n\\n![approve private link 1](/img/2024-11-05-afd-aks-ingress-tls/approvepl1.jpg)\\n\\nClick on the \'Private endpoint connections\' where you should see a pending request.\\n\\n![approve private link 2](/img/2024-11-05-afd-aks-ingress-tls/approvepl2.jpg)\\n\\nSelect the private link and click \'Approve\'.\\n\\n![approve private link 3](/img/2024-11-05-afd-aks-ingress-tls/approvepl3.jpg)\\n\\nYou\'ll see a dialog box with the message you sent when creating the origin connection.\\n\\n![approve private link 4](/img/2024-11-05-afd-aks-ingress-tls/approvepl4.jpg)\\n\\n## Test\\n\\nYou should now be able to open a browser and navigate to your URL. You can also test with curl.\\n\\n```bash\\ncurl https://e2elab.crashoverride.nyc/hello-world\\n\\n##########################################\\n# Sample Output\\n##########################################\\nHostname: aks-helloworld-fbdf59bf-qtdks\\n\\nPod Information:\\n\\t-no pod information available-\\n\\nServer values:\\n\\tserver_version=nginx: 1.13.3 - lua: 10008\\n\\nRequest Information:\\n\\tclient_address=\\n\\tmethod=GET\\n\\treal path=/\\n\\tquery=\\n\\trequest_version=1.1\\n\\trequest_scheme=http\\n\\trequest_uri=http://e2elab.crashoverride.nyc:8080/\\n\\nRequest Headers:\\n\\taccept=*/*\\n\\thost=e2elab.crashoverride.nyc\\n\\tuser-agent=curl/8.7.1\\n\\tvia=HTTP/2.0 Azure\\n\\tx-azure-clientip=70.18.42.220\\n\\tx-azure-fdid=c7e0d3e0-830a-4770-acae-14d27c7726f8\\n\\tx-azure-ref=\\n\\tx-azure-requestchainv2=hops=2\\n\\tx-azure-socketip=\\n\\tx-forwarded-for=10.140.0.5\\n\\tx-forwarded-host=e2elab.crashoverride.nyc\\n\\tx-forwarded-port=443\\n\\tx-forwarded-proto=https\\n\\tx-forwarded-scheme=https\\n\\tx-original-forwarded-for=\\n\\tx-real-ip=10.140.0.5\\n\\tx-request-id=8f41e792d8f0f74e90ddca9d14ba896b\\n\\tx-scheme=https\\n\\nRequest Body:\\n\\t-no body in request-\\n```\\n\\n## Conclusion\\n\\n Congrats! You should now have a working Azure Front Door directing TLS secured traffic to an in cluster ingress controller!\\n\\n >*Note:* In this walk through we did not add encryption between the ingress controller and the backend deployment. This can be done by sharing the same, or different, certificate to the deployment pods. You then enable backend encryption on the ingress controller. Alternatively, you could use a service mesh between the ingress and the backend deployment."},{"id":"/2024/09/06/multi-cluster-layer-4-load-balancing-with-fleet-manager","metadata":{"permalink":"/2024/09/06/multi-cluster-layer-4-load-balancing-with-fleet-manager","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2024-09-06/multi-cluster-layer-4-load-balancing-with-fleet-manager/index.md","source":"@site/blog/2024-09-06/multi-cluster-layer-4-load-balancing-with-fleet-manager/index.md","title":"Multi-Cluster Layer 4 Load Balancing with Fleet Manager","description":"How to configure a multi-cluster layer 4 load balancer across multiple AKS clusters using Fleet Manager.","date":"2024-09-06T00:00:00.000Z","tags":[],"readingTime":9.34,"hasTruncateMarker":true,"authors":[{"name":"Diego Casati","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/dcasati","socials":{"x":"https://x.com/ve6dpc","github":"https://github.com/dcasati"},"imageURL":"https://github.com/dcasati.png","key":"diego_casati","page":null}],"frontMatter":{"authors":["diego_casati"],"date":"2024-09-06","description":"How to configure a multi-cluster layer 4 load balancer across multiple AKS clusters using Fleet Manager.","tags":[],"title":"Multi-Cluster Layer 4 Load Balancing with Fleet Manager"},"unlisted":false,"prevItem":{"title":"End to End TLS Encryption with AKS and AFD","permalink":"/2024/11/05/afd-aks-ingress-tls"},"nextItem":{"title":"Using Stream Analytics to Filter AKS Control Plane Logs","permalink":"/2024/08/15/aks-control-plane-log-filtering"}},"content":"This guide demonstrates how to set up layer 4 load balancing across multiple AKS clusters using \\nAzure Fleet Manager. We\u2019ll create two AKS clusters in different regions (East US and West US), \\nconfigure Virtual Network (VNet) peering between them, and deploy a demo application using \\nFleet Manager. The process covers AKS cluster setup, VNet peering, Fleet Manager configuration, \\nand application deployment across regions.\\n\\n\x3c!-- truncate --\x3e\\n\\n### Topology\\n\\n```\\n+-----------------------+          +-----------------------+\\n|    AKS Cluster (East) |          |    AKS Cluster (West) |\\n|  Region: East US      |          |  Region: West US      |\\n|                       |          |                       |\\n| +-------------------+ |          | +-------------------+ |\\n| |   Application     | |          | |   Application     | |\\n| +-------------------+ |          | +-------------------+ |\\n|                       |          |                       |\\n+-----------------------+          +-----------------------+\\n          |                                      |\\n          +--------------------------------------+\\n                        VNet Peering\\n\\n             +-----------------------------------+\\n             |    Fleet Manager (Hub Region)     |\\n             +-----------------------------------+\\n```\\n\\n- [x] AKS Cluster (East): A Kubernetes cluster deployed in the East US region.\\n- [x] AKS Cluster (West): A Kubernetes cluster deployed in the West US region.\\n- [x] VNet Peering: Virtual Network peering between the AKS clusters to enable communication.\\n- [x] Fleet Manager: Azure Fleet Manager deployed in the hub region, managing the application across both AKS clusters.\\n\\n### Create two AKS clusters\\n\\nFor this demo, we will create two AKS clusters in two regions: East and West.\\n\\n#### Create the cluster in East US\\n\\nThe first step is to setup all of the environment variables that we will use\\n```bash\\n# ======================\\n# Environment Variables\\n# ======================\\nexport LOCATION_EAST=\\"eastus2\\"\\nexport LOCATION_WEST=\\"westus2\\"\\nexport RESOURCE_GROUP_EAST=\\"rg-aks-$LOCATION_EAST\\"\\nexport RESOURCE_GROUP_WEST=\\"rg-aks-$LOCATION_WEST\\"\\nexport CLUSTER_EAST=\\"aks-$LOCATION_EAST\\"\\nexport CLUSTER_WEST=\\"aks-$LOCATION_WEST\\"\\nexport FLEET_RESOURCE_GROUP_NAME=\\"rg-fleet\\"\\nexport FLEET=\\"gbb-fleet\\"\\nexport FLEET_LOCATION=\\"westus\\"\\n\\n# VNET\\nexport VNET_EAST=\\"aks-vnet-east\\"\\nexport VNET_WEST=\\"aks-vnet-west\\"\\nexport VNET_EAST_PREFIX=\\"10.1.0.0/16\\"\\nexport VNET_WEST_PREFIX=\\"10.2.0.0/16\\"\\n\\n# Non-overlapping CIDR ranges\\nexport CIDR_EAST=\\"10.1.0.0/24\\"\\nexport CIDR_WEST=\\"10.2.0.0/24\\"\\n\\n# Subnet names\\nexport SUBNET_EAST=\\"aks-subnet-east\\"\\nexport SUBNET_WEST=\\"aks-subnet-west\\"\\n```\\n\\nWe can now proceed with the creation of the first cluster\\n\\n```bash\\n# Create a resource group for the cluster in East US\\naz group create \\\\\\n  --name ${RESOURCE_GROUP_EAST} \\\\\\n  --location ${LOCATION_EAST}\\n\\n# Create a vNet for the West US cluster\\naz network vnet create \\\\\\n  --resource-group ${RESOURCE_GROUP_EAST} \\\\\\n  --name ${VNET_EAST} \\\\\\n  --address-prefix ${VNET_EAST_PREFIX} \\\\\\n  --subnet-name ${SUBNET_EAST} \\\\\\n  --subnet-prefix ${CIDR_EAST}\\n\\n# Retrieve the subnet id\\nSUBNET_ID_EAST=$(az network vnet subnet show \\\\\\n  --resource-group ${RESOURCE_GROUP_EAST} \\\\\\n  --vnet-name ${VNET_EAST} \\\\\\n  --name ${SUBNET_EAST} \\\\\\n  --query \\"id\\" -o tsv)\\n\\n# Create an AKS cluster with Azure CNI\\naz aks create \\\\\\n  --resource-group ${RESOURCE_GROUP_EAST} \\\\\\n  --name ${CLUSTER_EAST} \\\\\\n  --network-plugin azure \\\\\\n  --vnet-subnet-id ${SUBNET_ID_EAST}\\n\\n# get the cluster credentials (East US)\\naz aks get-credentials \\\\\\n  --resource-group ${RESOURCE_GROUP_EAST} \\\\\\n  --name ${CLUSTER_EAST} \\\\\\n  --file ${CLUSTER_EAST}\\n```\\n\\nNow repeat the same process for the cluster in West US:\\n\\n```bash\\n# Create a resource group for the cluster in West US\\naz group create \\\\\\n  --name ${RESOURCE_GROUP_WEST} \\\\\\n  --location ${LOCATION_WEST}\\n\\n# Create a vNet for the West US cluster\\naz network vnet create \\\\\\n  --resource-group ${RESOURCE_GROUP_WEST} \\\\\\n  --name ${VNET_WEST} \\\\\\n  --address-prefix ${VNET_WEST_PREFIX} \\\\\\n  --subnet-name ${SUBNET_WEST} \\\\\\n  --subnet-prefix ${CIDR_WEST}\\n\\n# Retrieve the subnet id\\nSUBNET_ID_WEST=$(az network vnet subnet show \\\\\\n  --resource-group ${RESOURCE_GROUP_WEST} \\\\\\n  --vnet-name ${VNET_WEST} \\\\\\n  --name ${SUBNET_WEST} \\\\\\n  --query \\"id\\" -o tsv)\\n\\n# Create an AKS cluster with Azure CNI\\naz aks create \\\\\\n  --resource-group ${RESOURCE_GROUP_WEST} \\\\\\n  --name ${CLUSTER_WEST} \\\\\\n  --network-plugin azure \\\\\\n  --vnet-subnet-id ${SUBNET_ID_WEST}\\n\\n# get the cluster credentials (West US)\\naz aks get-credentials \\\\\\n  --resource-group ${RESOURCE_GROUP_WEST} \\\\\\n  --name ${CLUSTER_WEST} \\\\\\n  --file ${CLUSTER_WEST}\\n```\\n#### Create the VNets and peer them\\nPeer the VNets between East and West US:\\n\\n```bash\\n# Peer VNets between East and West\\nVNET_ID_EAST=$(az network vnet show --resource-group ${RESOURCE_GROUP_EAST} --name ${VNET_EAST} --query \\"id\\" -o tsv)\\nVNET_ID_WEST=$(az network vnet show --resource-group ${RESOURCE_GROUP_WEST} --name ${VNET_WEST} --query \\"id\\" -o tsv)\\n\\n# Create VNet peering from east to west\\naz network vnet peering create --name EastToWestPeering \\\\\\n    --resource-group ${RESOURCE_GROUP_EAST} \\\\\\n    --vnet-name ${VNET_EAST} \\\\\\n    --remote-vnet ${VNET_ID_WEST} \\\\\\n    --allow-vnet-access\\n\\n# Create VNet peering from west to east\\naz network vnet peering create --name WestToEastPeering \\\\\\n    --resource-group ${RESOURCE_GROUP_WEST} \\\\\\n    --vnet-name ${VNET_WEST} \\\\\\n    --remote-vnet ${VNET_ID_EAST} \\\\\\n    --allow-vnet-access\\n```\\n#### Create a Fleet Manager and add members to it\\nAdd the fleet extension to Azure CLI\\n\\n```bash\\naz extension add --name fleet\\n```\\n\\nCreate the Fleet Manager resource\\n\\n```bash\\n# create the resource group\\naz group create \\\\\\n  --name ${FLEET_RESOURCE_GROUP_NAME} \\\\\\n  --location ${FLEET_LOCATION}\\n\\n# create fleet resource\\naz fleet create \\\\\\n  --resource-group ${FLEET_RESOURCE_GROUP_NAME} \\\\\\n  --name ${FLEET} \\\\\\n  --location ${FLEET_LOCATION} \\\\\\n  --enable-hub\\n\\n# Fleet Manager credentials\\naz fleet get-credentials \\\\\\n  --resource-group ${FLEET_RESOURCE_GROUP_NAME} \\\\\\n  --name ${FLEET} \\\\\\n  --file ${FLEET}\\n\\nFLEET_ID=$(az fleet show --resource-group \\"$FLEET_RESOURCE_GROUP_NAME\\" --name \\"$FLEET\\" -o tsv --query=id)\\n\\nIDENTITY=$(az ad signed-in-user show --query \\"id\\" --output tsv)\\nROLE=\\"Azure Kubernetes Fleet Manager RBAC Cluster Admin\\"\\naz role assignment create \\\\\\n  --role \\"$ROLE\\" \\\\\\n  --assignee \\"$IDENTITY\\" \\\\\\n  --scope ${FLEET_ID}\\n```\\n\\nRetrieve the Cluster IDs for East and West clusters:\\n\\n```bash\\n# Retrieve Cluster IDs (East and West)\\nexport AKS_EAST_ID=$(az aks show --resource-group ${RESOURCE_GROUP_EAST} --name ${CLUSTER_EAST} --query \\"id\\" -o tsv)\\n\\nexport AKS_WEST_ID=$(az aks show --resource-group ${RESOURCE_GROUP_WEST} --name ${CLUSTER_WEST} --query \\"id\\" -o tsv)\\n\\necho \\"AKS EAST cluster id: ${AKS_EAST_ID}\\"\\necho \\"AKS WEST cluster id: ${AKS_WEST_ID}\\"\\n```\\n\\nNow join both clusters to the Fleet:\\n\\n```bash\\n# join the East US cluster\\naz fleet member create \\\\\\n  --resource-group ${FLEET_RESOURCE_GROUP_NAME} \\\\\\n  --fleet-name ${FLEET} \\\\\\n  --name ${CLUSTER_EAST} \\\\\\n  --member-cluster-id ${AKS_EAST_ID}\\n\\n# join the West US cluster\\naz fleet member create \\\\\\n  --resource-group ${FLEET_RESOURCE_GROUP_NAME} \\\\\\n  --fleet-name ${FLEET} \\\\\\n  --name ${CLUSTER_WEST} \\\\\\n  --member-cluster-id ${AKS_WEST_ID}\\n```\\n\\nCheck if everything was setup correctly \\n```bash\\nKUBECONFIG=gbb-fleet kubectl get memberclusters\\n```\\n\\nYou should see an output similar to this:\\n\\n```bash\\nNAME          JOINED   AGE     MEMBER-AGENT-LAST-SEEN   NODE-COUNT   AVAILABLE-CPU   AVAILABLE-MEMORY\\naks-eastus2   True     11m     5s                       3            4196m           17827580Ki\\naks-westus2   True     9m24s   11s                      3            4196m           17827580Ki\\n```\\n#### Deploy the AKS store application\\n\\nFor this next step, we will deploy the AKS Store demo application to both clusters, \\nEast and West, using Fleet. Fleet Manager will work as a centralized hub, sending the\\nconfiguration and deployment files to its member clusters.\\n\\n```bash\\n# create the namespace for the application\\nKUBECONFIG=${FLEET} kubectl create ns aks-store-demo\\n\\n# deploy the application on both clusters thru Fleet\\nKUBECONFIG=${FLEET} kubectl apply -n aks-store-demo -f  https://raw.githubusercontent.com/Azure-Samples/aks-store-demo/main/aks-store-ingress-quickstart.yaml\\n```\\n\\nLet\'s pause for a moment and see what we have done. At this stage, we have done the following:\\n\\n- Two AKS cluster, one in East US and another in West US.\\n- We have connected the vNets of these two clusters using vNet Peering.\\n- A hub AKS Fleet Manager was deployed and the two clusters were added as its members. \\n- The AKS Store Demo application was deployed on both clusters (East US and West US) through Fleet.\\n\\nOur next step now is to leverage three components from AKS Fleet Manager: Service Export, Multi Cluster Service and Cluster Resource Placement.\\n\\n| Feature | Purpose | Use Case | How It Works |\\n|-|-|-|-|\\n| **ServiceExport** | Exports a service from a member cluster to other clusters as a Fleet resource. This can then be used for cross-cluster service load balancing. | Exposing a backend service from Cluster A to Cluster B within the AKS Fleet. | A Kubernetes service is marked as \\"exported\\" so it can be discovered and imported by other clusters. e.g.: exporting the `store-front` service from the `aks-store-demo` namespace |\\n| **ClusterResourcePlacement** | Allows the deployment of Kubernetes resources across fleet members. | Automatically deploying an application, config maps, or secrets to all clusters in a region. | Selects target member clusters based on labels and ensures the specified resources are synchronized across them. e.g.: match on `fleet.azure.com/location` being `eastus2` or `westus2` |\\n| **MultiClusterService** | A resource that allows the user to setup a Layer 4 multi-cluster load balancing solution across the fleet. | Load balancing requests to a frontend service running in multiple AKS clusters. | Automatically detects exported services and provides a unified endpoint that distributes traffic across clusters. e.g.: expose the `store-front` service |\\n\\nCreate the Service Export:\\n\\n```bash\\ncat <<EOF > aks-store-serviceexport.yaml\\napiVersion: networking.fleet.azure.com/v1alpha1\\nkind: ServiceExport\\nmetadata:\\n  name: store-front\\n  namespace: aks-store-demo\\nEOF\\n\\nKUBECONFIG=${FLEET} kubectl apply -n aks-store-demo -f aks-store-serviceexport.yaml\\n```\\n\\nVerify that the service export was deployed:\\n\\n```bash\\nKUBECONFIG=${FLEET} kubectl -n aks-store-demo get serviceexport\\n```\\n\\nYou should see an output similar to this:\\n\\n```bash\\nNAME          IS-VALID   IS-CONFLICTED   AGE\\nstore-front                              2m4s\\n```\\n\\nCreate the ClusterResourcePlacement (CRP):\\n\\n```bash\\ncat <<EOF > cluster-resource-placement.yaml\\napiVersion: placement.kubernetes-fleet.io/v1beta1\\nkind: ClusterResourcePlacement\\nmetadata:\\n  name: aks-store-demo\\nspec:\\n  resourceSelectors:\\n    - group: \\"\\"\\n      version: v1\\n      kind: Namespace\\n      name: aks-store-demo\\n  policy:\\n    affinity:\\n      clusterAffinity:\\n        requiredDuringSchedulingIgnoredDuringExecution:\\n          clusterSelectorTerms:\\n            - labelSelector:\\n                matchExpressions:\\n                  - key: fleet.azure.com/location\\n                    operator: In\\n                    values:\\n                      - ${LOCATION_EAST}\\n                      - ${LOCATION_WEST}\\nEOF\\n\\nKUBECONFIG=${FLEET} kubectl apply -f cluster-resource-placement.yaml\\n```\\nVerify that the CRP was created:\\n\\n```bash\\nKUBECONFIG=${FLEET} kubectl get ClusterResourcePlacement\\nNAME             GEN   SCHEDULED   SCHEDULED-GEN   AVAILABLE   AVAILABLE-GEN   AGE\\naks-store-demo   2     True        2               True        2               110s\\n```\\n\\nCreate and deploy MultiClusterService (MCS):\\n\\n```bash\\ncat <<EOF > aks-store-mcs.yaml\\napiVersion: networking.fleet.azure.com/v1alpha1\\nkind: MultiClusterService\\nmetadata:\\n  name: store-front\\n  namespace: aks-store-demo\\nspec:\\n  serviceImport:\\n    name: store-front\\nEOF\\n\\n# Deploy the MultiClusterService resource to the East US cluster\\nKUBECONFIG=${CLUSTER_EAST} kubectl apply -f aks-store-mcs.yaml\\n```\\n\\nVerify that the MCS was deployed and that the `IS_VALID` field is showing `True`:\\n\\n```bash\\nKUBECONFIG=${CLUSTER_EAST} kubectl -n aks-store-demo get mcs\\nNAME          SERVICE-IMPORT   EXTERNAL-IP    IS-VALID   AGE\\nstore-front   store-front      20.7.120.195   True       55s\\n```\\n\\n#### Testing the Application\\n\\nOnce the MultiClusterService (MCS) has been successfully deployed across the AKS clusters, you can test the application to ensure it\'s working properly. Follow these steps to verify the setup:\\n\\n**Get the external IP address of the service**:\\n\\nAfter deploying the MultiClusterService, you need to retrieve the external IP address to access the service. Run the following command to get the external IP for the East-US cluster:\\n\\n```bash\\nKUBECONFIG=${CLUSTER_EAST} kubectl get services -n fleet-system\\n```\\nLook for the external IP under the EXTERNAL-IP column for the store-front service.\\n\\n**Access the application**:\\n\\nOnce you have the external IP addresses from both clusters, open a browser or use curl to access the application using the IP addresses:\\n\\n```bash\\ncurl http://<EXTERNAL_IP>\\n```\\n\\nReplace `<EXTERNAL_IP>` with the actual external IP you retrieved from the previous step. The application should be accessible through either of the IPs.\\n\\nValidate cross-region load balancing:\\n\\nSince the `MultiClusterService` has been deployed across multiple regions, traffic can be balanced between the AKS clusters. You can simulate \\ntraffic from different regions using tools like curl, Postman, or load-testing utilities to confirm that the service is responding from both regions.\\n\\n**Verify service status**:\\n\\nYou can check the status of the deployed services and pods on both clusters to ensure everything is running correctly:\\n\\n```bash\\nKUBECONFIG=${CLUSTER_EAST} kubectl get pods -n aks-store-demo\\nKUBECONFIG=${CLUSTER_WEST} kubectl get pods -n aks-store-demo\\n```\\nEnsure that all services and pods show a Running status, indicating that the application is running across both clusters.\\n\\n#### Remove the setup\\nTo remove this setup, you can run the following set of commands:\\n\\n```bash\\n# East cluster\\naz group delete --name ${RESOURCE_GROUP_EAST} --yes --no-wait\\n\\n# West cluster\\naz group delete --name ${RESOURCE_GROUP_WEST} --yes --no-wait\\n\\n# Fleet Hub\\naz group delete --name ${FLEET_RESOURCE_GROUP_NAME} --yes --no-wait\\n```\\n\\n### Conclusion\\nIn this guide, we successfully set up a multi-cluster layer 4 load balancer across \\nAKS clusters using Azure Fleet Manager. By configuring AKS clusters in different regions, \\nestablishing VNet peering, and utilizing Fleet Manager, we enabled centralized management \\nand deployment of services across clusters. This approach ensures improved availability and \\nscalability for applications deployed across multiple regions.\\n\\nFor the full deployment script used in this tutorial, you can access \\nthe App Innovation GBB GitHub repository: [Pattern - Multi-Cluster Layer 4 Load Balancer with Azure Fleet Manager](https://github.com/appdevgbb/pattern-fleet-manager/tree/main)."},{"id":"/2024/08/15/aks-control-plane-log-filtering","metadata":{"permalink":"/2024/08/15/aks-control-plane-log-filtering","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2024-08-15/aks-control-plane-log-filtering/index.md","source":"@site/blog/2024-08-15/aks-control-plane-log-filtering/index.md","title":"Using Stream Analytics to Filter AKS Control Plane Logs","description":"Using Azure Stream Analytics to filter Kubernetes control plane log data from AKS diagnostics in order to isolate critical data and reduce log retention cost.","date":"2024-08-15T00:00:00.000Z","tags":[],"readingTime":10.14,"hasTruncateMarker":true,"authors":[{"name":"Steve Griffith","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/swgriffith","socials":{"x":"https://x.com/SteveGriffith","github":"https://github.com/swgriffith"},"imageURL":"https://github.com/swgriffith.png","key":"steve_griffith","page":null}],"frontMatter":{"authors":["steve_griffith"],"date":"2024-08-15","description":"Using Azure Stream Analytics to filter Kubernetes control plane log data from AKS diagnostics in order to isolate critical data and reduce log retention cost.","tags":[],"title":"Using Stream Analytics to Filter AKS Control Plane Logs"},"unlisted":false,"prevItem":{"title":"Multi-Cluster Layer 4 Load Balancing with Fleet Manager","permalink":"/2024/09/06/multi-cluster-layer-4-load-balancing-with-fleet-manager"},"nextItem":{"title":"Using Project KAITO in AKS","permalink":"/2024/04/16/aks-kaito"}},"content":"## Introduction\\n\\nWhile AKS does NOT provide access to the cluster\'s managed control plane, it does provide access to the control plane component logs via [diagnostic settings](https://learn.microsoft.com/en-us/azure/aks/monitor-aks#aks-control-planeresource-logs). The easiest option to persist and search this data is to send it directly to Azure Log Analytics, however there is a LOT of data in those logs, which makes it cost prohibitive in Log Analytics. Alternatively, you can send all the data to an Azure Storage Account, but then searching and alerting can be challenging. \\n\\nTo address the above challenge, one option is to stream the data to Azure Event Hub, which then gives you the option to use Azure Stream Analytics to filter out events that you deem important and then just store the rest in cheaper storage (ex. Azure Storage) for potential future diagnostic needs.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn this walkthrough we\'ll create an AKS cluster, enable diagnostic logging to Azure Stream Analytics and then demonstrate how to filter out some key records.\\n\\n## Cluster & Stream Analytics Setup\\n\\nIn this setup, the cluster will be a very basic single node AKS cluster that will simply have diagnostic settings enabled. We\'ll also create the Event Hub instance that will be used in the diagnostic settings. \\n\\n```bash\\n# Set some environment variables\\nRG=LogFilteringLab\\nLOC=eastus2\\nCLUSTER_NAME=logfilterlab\\nNAMESPACE_NAME=\\"akscontrolplane$RANDOM\\"\\nEVENT_HUB_NAME=\\"logfilterhub$RANDOM\\"\\nDIAGNOSTIC_SETTINGS_NAME=\\"demologfilter\\"\\n\\n# Create a resource group\\naz group create -n $RG -l $LOC\\n\\n# Create the AKS Cluster\\naz aks create \\\\\\n-g $RG \\\\\\n-n $CLUSTER_NAME \\\\\\n-c 1\\n\\n# Get the cluster credentials\\naz aks get-credentials -g $RG -n $CLUSTER_NAME\\n\\n# Create an Event Hub Namespace\\naz eventhubs namespace create --name $NAMESPACE_NAME --resource-group $RG -l $LOC\\n\\n# Create an event hub\\naz eventhubs eventhub create --name $EVENT_HUB_NAME --resource-group $RG --namespace-name $NAMESPACE_NAME\\n\\nAKS_CLUSTER_ID=$(az aks show -g $RG -n $CLUSTER_NAME -o tsv --query id)\\nEVENT_HUB_NAMESPACE_ID=$(az eventhubs namespace show -g $RG -n $NAMESPACE_NAME -o tsv --query id)\\n\\n# Apply the diagnostic settings to the AKS cluster to enable Kubernetes audit log shipping\\n# to our Event Hub\\naz monitor diagnostic-settings create \\\\\\n--resource $AKS_CLUSTER_ID \\\\\\n-n $DIAGNOSTIC_SETTINGS_NAME \\\\\\n--event-hub $EVENT_HUB_NAME \\\\\\n--event-hub-rule \\"${EVENT_HUB_NAMESPACE_ID}/authorizationrules/RootManageSharedAccessKey\\" \\\\\\n--logs \'[ { \\"category\\": \\"kube-audit\\", \\"enabled\\": true, \\"retentionPolicy\\": { \\"enabled\\": false, \\"days\\": 0 } } ]\' \\n```\\n\\n## Stream Analytics\\n\\nAs we\'ll use Stream Analytics to filter through the log messages for what we want to capture, we\'ll need to create a Stream Analytics Job. This job will take the Event Hub as it\'s input source, will run a query and will send the query results to an output target. This output target can be a number of options, but for the purposes of our test we\'ll write the filtered records out to a Service Bus Queue, which we can watch in real time.\\n\\nWe have the Event Hub already, now lets create the Azure Service Bus Queue and then the Stream Analytics Job to tie it all together.\\n\\n### Create the Service Bus Queue\\n\\n```bash\\nSERVICE_BUS_NAMESPACE_NAME=kubecontrolplanelogs\\nSERVICE_BUS_QUEUE_NAME=kubeaudit\\n\\n# Create the service bus namespace\\naz servicebus namespace create --resource-group $RG --name $SERVICE_BUS_NAMESPACE_NAME --location $LOC\\n\\n# Create the service bus queue\\naz servicebus queue create --resource-group $RG --namespace-name $SERVICE_BUS_NAMESPACE_NAME --name $SERVICE_BUS_QUEUE_NAME\\n\\n```\\n\\n### Stream Analytics Job\\n\\nFor the Stream Analytics Job we\'ll switch over to the portal, so go ahead and open [https://portal.azure.com](https://portal.azure.com) and navigate to your resource group.\\n\\n1. Click on the \'Create\' button at the top of your resource group:\\n   \\n    ![Create](/img/2024-08-15-aks-control-plane-log-filtering/rg-create.jpg)\\n\\n2. Search for \'Stream Analytics Job\'\\n   \\n    ![Search](/img/2024-08-15-aks-control-plane-log-filtering/sa-job-search.jpg)\\n\\n3. Click \'Create\' on the Stream Analytics Job search result\\n   \\n    ![Search Result](/img/2024-08-15-aks-control-plane-log-filtering/sa-job-search-result.jpg)\\n\\n4. Leave all defaults, but provide a name under the \'Instance Details\' section and then click \'Review + Create\'\\n   \\n    ![Create Instance](/img/2024-08-15-aks-control-plane-log-filtering/sa-job-create-instance.jpg)\\n\\n5. After the validation is complete, just click \'Create\'. This typically completes very quickly.\\n\\n6. Click on \'Go to Resource\' or navigate back to your resource group and click on the Stream Analytics Job you just created.\\n\\n7. In the stream analytics job, expand \'Job topology\' and then click on \'Inputs\' so we can add our Event Hub input\\n   \\n    ![Stream Analytics Job - Add Input](/img/2024-08-15-aks-control-plane-log-filtering/sa-job-inputs.jpg)\\n\\n8. Click on \'Add Input\' and select \'Event Hub\'\\n   \\n    ![Create Input](/img/2024-08-15-aks-control-plane-log-filtering/sa-job-create-input.jpg)\\n\\n9.  The Event Hub\'s new input creation pane should auto-populate with your Event Hub details as well as default to creation of a new access policy, but verify that all of the details are correct and then click \'Save\'.\\n    \\n    ![Event Hub Config Details](/img/2024-08-15-aks-control-plane-log-filtering/sa-event-hub-config.jpg)\\n\\n10. Now we need to attach the Service Bus we created as the output target, so under \'Job topology\' click on \'Outputs\'.\\n    \\n11. In the \'Outputs\' window, click on \'Add output\' and select \'Service Bus queue\'\\n    \\n    ![Add Output](/img/2024-08-15-aks-control-plane-log-filtering/sa-add-output.jpg)\\n\\n12. Again, it should bring up a window with the queue configuration details already pre-populated, but verify all the details and update as needed and then click \'Save\'.\\n    \\n    ![Service Bus Config](/img/2024-08-15-aks-control-plane-log-filtering/sa-servicebus-config.jpg)\\n\\n13. To process the records from AKS we\'ll need to parse some JSON, so we need to add a function to the Stream Analytics Job to parse JSON. Under \'Job topology\' click on \'Functions\'.\\n\\n14. In the functions window, click on \'Add Function\' and then select \'Javascript UDF\' for Javascript User Defined Function\\n    \\n    ![Create Function](/img/2024-08-15-aks-control-plane-log-filtering/sa-create-function.jpg)\\n\\n15. In the \'Function alias\' name the function \'jsonparse\' and in the editor window add the following:\\n    ```javascript\\n    function main(x) {\\n    var json = JSON.parse(x);  \\n    return json;\\n    }\\n    ```\\n\\n    ![Function](/img/2024-08-15-aks-control-plane-log-filtering/sa-javascript-udf.jpg)\\n\\n16. Click on \'Save\' to save the function\\n\\n17. Now, under \'Job topology\' in the stream analytics job, click on \'Query\' to start adding a query. When loaded, the inputs, outputs and functions should pre-populate for you.\\n    \\n18. We\'ll first create a basic query to select all records and ship them to the output target. In the query window paste the following, updating the input and output values to match the names of your input and output. The function name should be the same unless you changed it.\\n\\n    ```sql\\n    WITH DynamicCTE AS (\\n    SELECT UDF.jsonparse(individualRecords.ArrayValue.properties.log) AS log\\n    FROM [logfilterhub28026]\\n    CROSS APPLY GetArrayElements(records) AS individualRecords\\n    )\\n    SELECT *\\n    INTO [kubeauditlogs]\\n    FROM DynamicCTE\\n    ```\\n\\n19. Click \'Save Query\' at the top of the query window\\n\\n    ![Save Query](/img/2024-08-15-aks-control-plane-log-filtering/sa-save-query.jpg)\\n\\n20. In the top left of the query window, click on \'Start Job\' to kick off the stream analytics job.\\n\\n21. In the \'Start job\' window, leave the start time set to \'Now\' and click \'Start\'\\n\\n    ![Start Job](/img/2024-08-15-aks-control-plane-log-filtering/sa-start-job.jpg)\\n    \\n22. Click on the \'Overview\' tab in the stream analytics job, and refresh every once in a while until the job \'Status\' says \'Running\'\\n\\n    ![Job Running](/img/2024-08-15-aks-control-plane-log-filtering/sa-job-status-running.jpg)\\n\\n23. Navigate back to your Resource Group and then click on your service bus namespace. \\n\\n24. Assuming everything worked as expected you should now be seeing a lot of messages coming through the Service Bus Queue\\n\\n    ![Service Bus Namespace with Data](/img/2024-08-15-aks-control-plane-log-filtering/sb-namespace-live.jpg)\\n\\n25. Click on the queue at the bottom of the screen to open the Queue level view\\n\\n26. At the queue level, click on \'Service Bus Explorer\' to view the live records\\n\\n27. To view the records already created\' click on \'Peek from start\' and then choose a record to view\\n\\n    ![Live Audit Record](/img/2024-08-15-aks-control-plane-log-filtering/sb-audit-record.jpg)\\n\\n28. Navigate back to the stream analytics job and click on \'Stop job\' to stop sending records through to the service bus.\\n\\nGreat! You should now have a very basic stream analytics job that takes the control plane \'kube-audit\' log from an AKS cluster through Event Hub, queries that data and then pushes it to a Service Bus Queue. While this is great, the goal is to filter out some records, so lets move on to that!\\n\\n## Setup a test workload to trigger audit log entries\\n\\nTo test out our stream analytics query, we need some test data we can filter on. Let\'s create some requests to the API server that will be denied. To do that we\'ll create a service account with no rights and then create a test pod using that service account. We\'ll then use the service account token to try to reach the Kubernetes API server.\\n\\n```bash\\n# Create a new namespace\\nkubectl create ns demo-ns\\n\\n# Create a service account in the namespace\\nkubectl create sa demo-user -n demo-ns\\n\\n# Create a test secret\\nkubectl create secret generic demo-secret -n demo-ns --from-literal \'message=hey-there\'\\n\\n# Check that you can read the secret\\nkubectl get secret demo-secret -n demo-ns -o jsonpath=\'{.data.message}\'|base64 --decode\\n\\n# Create a test pod to try to query the API server\\nkubectl run curlpod --rm -it \\\\\\n--image=curlimages/curl -n demo-ns \\\\\\n--overrides=\'{ \\"spec\\": { \\"serviceAccount\\": \\"demo-user\\" }  }\' -- sh\\n\\n#############################################\\n# From within the pod run the following\\n#############################################\\n# Point to the internal API server hostname\\nexport APISERVER=https://kubernetes.default.svc\\n\\n# Path to ServiceAccount token\\nexport SERVICEACCOUNT=/var/run/secrets/kubernetes.io/serviceaccount\\n\\n# Read this Pod\'s namespace\\nexport NAMESPACE=$(cat ${SERVICEACCOUNT}/namespace)\\n\\n# Read the ServiceAccount bearer token\\nexport TOKEN=$(cat ${SERVICEACCOUNT}/token)\\n\\n# Reference the internal certificate authority (CA)\\nexport CACERT=${SERVICEACCOUNT}/ca.crt\\n\\n# Explore the API with TOKEN \\n# This call will pass\\ncurl --cacert ${CACERT} --header \\"Authorization: Bearer ${TOKEN}\\" -X GET ${APISERVER}/api\\n\\n# This call to get secrets will fail\\ncurl --cacert ${CACERT} --header \\"Authorization: Bearer ${TOKEN}\\" -X GET ${APISERVER}/api/v1/namespaces/$NAMESPACE/secrets/\\n\\n# Now run it under a watch to trigger continuous deny errors\\nwatch \'curl --cacert ${CACERT} --header \\"Authorization: Bearer ${TOKEN}\\" -X GET ${APISERVER}/api/v1/namespaces/$NAMESPACE/secrets/\'\\n```\\n\\n\\n## Update Stream Analytics to Look for Forbidden Requests\\n\\nSo, we have a user trying to execute requests against our cluster for which they are not authorized. We can easily update our stream analytics query to filter out forbidden requests against our namespace. \\n\\n1. Navigate back to your \'Stream Analytics\' instances in the Azure Portal\\n   \\n2. If the job is still running, make sure you click \'Stop job\' as you cannot edit queries while the job is running\\n   \\n3. Click on the \'Query\' tab\\n   \\n4. Update the query as follows, to filter out audit messages about our \'demo-ns\' namespace that also have a status code of 403 (Forbidden)\\n   \\n    > **Note:** Be sure that your \'FROM\' still points to your Event Hub input target and that your \'INTO\' still points to your Service Bus output target.\\n\\n   ```sql\\n    WITH DynamicCTE AS (\\n    SELECT UDF.jsonparse(individualRecords.ArrayValue.properties.log) AS log\\n    FROM [logfilterhub28026]\\n    CROSS APPLY GetArrayElements(records) AS individualRecords\\n    )\\n    SELECT *\\n    INTO [kubeaudit]\\n    FROM DynamicCTE\\n    WHERE log.objectRef.namespace = \'demo-ns\'\\n    AND log.responseStatus.code = 403\\n    ```\\n    ![Updated Query Window](/img/2024-08-15-aks-control-plane-log-filtering/sa-403-query.jpg)\\n\\n5. Click \'Save query\'\\n6. Once the save completes click \'Start Job\'\\n\\nOnce your job is started, you should be able to navigate back to your Service Bus Queue and watch the messages flowing through.\\n\\n![Filtered Messages](/img/2024-08-15-aks-control-plane-log-filtering/sb-filtered-messages.jpg)\\n\\n\\n## Conclusion\\n\\nCongratulations! You now have an end-to-end fully working Stream Analytics instance that can filter AKS control plane logs to extract specific messages. You can manipulate the diagnostic settings to add additional logs to the input and modify the query to extract the exact messages critical to your cluster\'s health and security. This is an extremely versatile solution that is also capable of handling log records of multiple clusters across your enterprise."},{"id":"/2024/04/16/aks-kaito","metadata":{"permalink":"/2024/04/16/aks-kaito","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2024-04-16/aks-kaito/index.md","source":"@site/blog/2024-04-16/aks-kaito/index.md","title":"Using Project KAITO in AKS","description":"Setting up an AKS cluster with the Kubernetes AI Toolchain Operator (KAITO) managed add-on and then deploying an inference model from your own private Azure Containter Registry.","date":"2024-04-16T00:00:00.000Z","tags":[],"readingTime":6.2,"hasTruncateMarker":true,"authors":[{"name":"Steve Griffith","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/swgriffith","socials":{"x":"https://x.com/SteveGriffith","github":"https://github.com/swgriffith"},"imageURL":"https://github.com/swgriffith.png","key":"steve_griffith","page":null}],"frontMatter":{"authors":["steve_griffith"],"date":"2024-04-16","description":"Setting up an AKS cluster with the Kubernetes AI Toolchain Operator (KAITO) managed add-on and then deploying an inference model from your own private Azure Containter Registry.","tags":[],"title":"Using Project KAITO in AKS"},"unlisted":false,"prevItem":{"title":"Using Stream Analytics to Filter AKS Control Plane Logs","permalink":"/2024/08/15/aks-control-plane-log-filtering"},"nextItem":{"title":"Using the Azure Key Vault CSI Driver with Workload Identity","permalink":"/2024/03/05/workload-identity-kv-csi"}},"content":"The Kubernetes AI Toolchain Operator, also known as Project KAITO, is an open-source solution to simplify the deployment of inference models in a Kubernetes cluster. In particular, the focus is on simplifying the operation of the most popular models available (ex. Falcon, Mistral and Llama2).\\n\\nKAITO provides operators to manage validation of the requested model against the requested nodepool hardware, deployment of the nodepool and the deployment of the model itself along with a REST endpoint to reach the model.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn this walkthrough we\'ll deploy an AKS cluster with the KAITO managed add-on. Next, we\'ll deploy and test an inference  model, which we\'ll pull from our own private container registry. We\'ll be following the setup guide from the AKS product docs [here](https://learn.microsoft.com/en-us/azure/aks/ai-toolchain-operator) with some of my own customizations and extensions to simplify tasks.\\n\\n## Cluster Creation\\n\\nIn this setup we\'ll be creating a very basic AKS cluster via the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/), but this managed add-on will work in any AKS cluster, assuming you meet the [pre-reqs](https://learn.microsoft.com/en-us/azure/aks/ai-toolchain-operator#prerequisites). \\n\\nWe\'ll also be creating an Azure Container Registry to demonstrate replicating a KAITO model to your own private registry and using it in the model deployment, which would be a security best practice.\\n\\n```bash\\n# Set Variables\\nRG=KaitoLab\\nLOC=westus3\\nACR_NAME=kaitolab\\nCLUSTER_NAME=kaito\\n\\n# Create the resource group\\naz group create -n $RG -l $LOC\\n\\n# Create the Azure Container Registry\\naz acr create -g $RG -n $ACR_NAME --sku Standard\\n\\n# Create the AKS Cluster\\naz aks create \\\\\\n-g $RG \\\\\\n-n $CLUSTER_NAME \\\\\\n--enable-oidc-issuer \\\\\\n--enable-ai-toolchain-operator\\n\\n# Get the Cluster Credentials\\naz aks get-credentials -g $RG -n $CLUSTER_NAME\\n```\\n\\n## Setup the KAITO Identity\\n\\nKAITO uses the node auto-provisioner to add nodepools to the AKS cluster. To do this it needs rights on the cluster resource group. At this time the rights are broad, but as KAITO reaches general availabiliy we should see those roles refined.\\n\\n```bash\\n# Get the Cluster Resource Group\\nexport RG_ID=$(az group show -n $RG -o tsv --query id)\\n\\n# Get the managed cluster Resource Group\\nexport MC_RESOURCE_GROUP=$(az aks show --resource-group ${RG} --name ${CLUSTER_NAME} --query nodeResourceGroup -o tsv)\\n\\n# Set a variable for the KAITO IDentity name\\nexport KAITO_IDENTITY_NAME=\\"ai-toolchain-operator-${CLUSTER_NAME}\\"\\n\\n# Get the principal ID for the KAITO managed identity\\nexport PRINCIPAL_ID=$(az identity show --name \\"${KAITO_IDENTITY_NAME}\\" --resource-group \\"${MC_RESOURCE_GROUP}\\" --query \'principalId\' -o tsv)\\n\\n# Grant contributor on the cluster resource group\\naz role assignment create --role \\"Contributor\\" --assignee \\"${PRINCIPAL_ID}\\" --scope $RG_ID\\n\\n# Get the OIDC Issuer URL\\nexport AKS_OIDC_ISSUER=$(az aks show --resource-group \\"${RG}\\" --name \\"${CLUSTER_NAME}\\" --query \\"oidcIssuerProfile.issuerUrl\\" -o tsv)\\n\\n# Create the federation between the KAITO service account and the KAITO Azure Managed Identity\\naz identity federated-credential create --name \\"kaito-federated-identity\\" --identity-name \\"${KAITO_IDENTITY_NAME}\\" -g \\"${MC_RESOURCE_GROUP}\\" --issuer \\"${AKS_OIDC_ISSUER}\\" --subject system:serviceaccount:\\"kube-system:kaito-gpu-provisioner\\" --audience api://AzureADTokenExchange\\n\\n# If you check the kaito-gpu-provisioner pod, you\'ll see it\'s in CrashLoopBackOff\\n# due to the identity not yet having been configured with proper rights.\\nkubectl get pods -l app=ai-toolchain-operator -n kube-system\\n\\n# Restart the GPU provisioner to reload authorization\\nkubectl rollout restart deployment/kaito-gpu-provisioner -n kube-system\\n\\n# Check the pod again to confirm it\'s now running\\nkubectl get pods -l app=ai-toolchain-operator -n kube-system\\n```\\n\\n## Set up the Azure Container Registry\\n\\nThe KAITO team builds and hosts the most popular inference models for you. These models are available in the Microsoft Container Registry (MCR) and if you run a KAITO workspace for one of those models it will pull that image for you automatically. However, as noted above, security best practice is to only pull images from your own trusted repository. Fortunately, KAITO gives you this option.\\n\\nLet\'s pull the image from the MCR into our Azure Container Registry, and link that registry to our AKS cluster. The image for the model in the MCR follows a standard format, as seen below. We just need the model name and version and we can import it into our private registry. We\'ll use Mistral-7B.\\n\\n>**NOTE:** If you aren\'t already aware, Large Language Models are LARGE. This import will take some time. Assume 10-20 minutes for many models.\\n\\n```bash\\nMODELNAME=mistral-7b-instruct\\nTAG=\\"0.0.2\\"\\n\\n# Copy over the mistral image to our ACR\\naz acr import -g $RG --name $ACR_NAME --source  mcr.microsoft.com/aks/kaito/kaito-$MODELNAME:$TAG --image $MODELNAME:$TAG\\n```\\n\\nWhile the import is running, we can go ahead and start another terminal window to attach the Azure Container Registry to our AKS cluster. \\n\\nWe don\'t need to attach the ACR, if we prefer to use admin credentials and an image pull secret, but using the attach feature is more secure as it authenticates to ACR with the kubelet managed identity.\\n\\n```bash\\n# If we\'re in a new terminal window we\'ll need to set our environment variables\\nRG=KaitoLab\\nCLUSTER_NAME=kaitocluster\\nACR_NAME=kaitolab\\n\\n# Attach the ACR\\naz aks update -g $RG -n $CLUSTER_NAME --attach-acr $ACR_NAME\\n```\\n\\n## Deploy a model!\\n\\nNow that our cluster and registry are all set, we\'re ready to deploy our first model. We\'ll generate our \'Workspace\' manifest ourselves, but you can also pull from the [examples](https://github.com/Azure/kaito/blob/main/presets/README.md) in the KAITO repo and update as needed. The model below is actually directly from the examples; however I added the \'presetOptions\' section to set the source of the model image.\\n\\n>**NOTE:** Make sure you validate you have quota on the target subscription for the machine type you select below.\\n\\n```bash\\n# Set the target machine size\\nMACHINE_SIZE=Standard_NC64as_T4_v3\\n\\n# Generate the model manifest\\ncat <<EOF >${MODELNAME}-${TAG}.yaml\\napiVersion: kaito.sh/v1alpha1\\nkind: Workspace\\nmetadata:\\n  name: workspace-${MODELNAME}\\nresource:\\n  instanceType: \\"${MACHINE_SIZE}\\"\\n  labelSelector:\\n    matchLabels:\\n      apps: ${MODELNAME}\\ninference:\\n  preset:\\n    name: \\"${MODELNAME}\\"\\n    presetOptions:\\n      image: ${ACR_NAME}.azurecr.io/${MODELNAME}:${TAG}\\nEOF\\n\\n# OPTIONAL: In another terminal, if you wish, watch the gpu and workspace provisioner logs\\nkubectl logs -f -l app=ai-toolchain-operator -n kube-system\\n\\n# Deploy Mistral\\nkubectl apply -f ${MODELNAME}-${TAG}.yaml\\n\\n# Watch the deployment \\n# This will take some time as the node provisions and the model is pulled\\nwatch kubectl get workspace,nodes,svc,pods\\n```\\n\\n## Test your inference endpoint\\n\\nNow that our model is running, we can send it a request. By default, the model is only accessible via a ClusterIP inside the Kubernetes cluster, so you\'ll need to access the endpoint from a test pod. We\'ll use a public \'curl\' image, but you can use whatever you prefer.\\n\\nYou do have the option to expose the model via a Kubernetes Service of type \'LoadBalancer\' via the workspace configuration, but that generally isn\'t recommended. Typically, you\'d be calling the model from another service inside the cluster, or placing the endpoint behind an ingress controller.\\n\\n```bash\\n# Get the model cluster IP\\nCLUSTERIP=$(kubectl get svc workspace-${MODELNAME} -o jsonpath=\'{.spec.clusterIP}\')\\n\\n# Curl the model service\\nkubectl run -it --rm --restart=Never curl --image=curlimages/curl -- \\\\\\ncurl -X POST http://$CLUSTERIP/chat \\\\\\n-H \\"accept: application/json\\" \\\\\\n-H \\"Content-Type: application/json\\" \\\\\\n-d \\"{\\\\\\"prompt\\\\\\":\\\\\\"Who is Inigo Montoya and from what movie?\\\\\\",\\\\\\"generate_kwargs\\\\\\":{\\\\\\"max_length\\\\\\":200}}\\"\\n```\\n\\n## Conclustion\\n\\nCongratulations! You should now have a working AKS cluster with the Kubernetes AI Toolchain Operator up and running. As you explore KAITO please feel free to reach out to the KAITO team via the [open-source project](https://github.com/Azure/kaito/issues) for any questions or feature requests."},{"id":"/2024/03/05/workload-identity-kv-csi","metadata":{"permalink":"/2024/03/05/workload-identity-kv-csi","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2024-03-05/workload-identity-kv-csi/index.md","source":"@site/blog/2024-03-05/workload-identity-kv-csi/index.md","title":"Using the Azure Key Vault CSI Driver with Workload Identity","description":"Setting up an AKS cluster with Workload Identity and the Key Vault CSI driver and using them from a SecretProviderClass and Pod to load secrets.","date":"2024-03-05T00:00:00.000Z","tags":[],"readingTime":6.8,"hasTruncateMarker":true,"authors":[{"name":"Steve Griffith","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/swgriffith","socials":{"x":"https://x.com/SteveGriffith","github":"https://github.com/swgriffith"},"imageURL":"https://github.com/swgriffith.png","key":"steve_griffith","page":null}],"frontMatter":{"authors":["steve_griffith"],"date":"2024-03-05","description":"Setting up an AKS cluster with Workload Identity and the Key Vault CSI driver and using them from a SecretProviderClass and Pod to load secrets.","tags":[],"title":"Using the Azure Key Vault CSI Driver with Workload Identity"},"unlisted":false,"prevItem":{"title":"Using Project KAITO in AKS","permalink":"/2024/04/16/aks-kaito"},"nextItem":{"title":"Securing your AKS cluster with a Linux Firewall VM","permalink":"/2024/02/02/securing-your-aks-cluster-with-a-linux-firewall-vm"}},"content":"When working with secrets in an application running in Kubernetes, you can use native Kubernetes secrets, however there are limitations in the security of those secrets. A better practice is to use a secure vault, like Azure Key Vault. \\n\\nAzure Key Vault can be accessed via a direct SDK call, as demonstrated in our previous [Workload Identity](https://azureglobalblackbelts.com/2023/09/21/workload-identity-example.html) post. However, in some cases you may not have the option to use the SDK, like in cases where you dont have access to source code. In those cases you may prefer to load secrets directly into an environment variable or a file. In these cases, the Azure Key Vault CSI driver is here to save the day. \\n\\n\x3c!-- truncate --\x3e\\n\\nThe following walkthrough shows how you can using [Azure Workload Identity](https://azure.github.io/azure-workload-identity/docs/) with the [AKS Workload Identity](https://learn.microsoft.com/en-us/azure/aks/workload-identity-overview) add-on along with the Key Vault CSI Driver to mount secrets and volumes in your pods.\\n\\n### Cluster Creation\\n\\nFirst, lets create the AKS cluster with the OIDC Issuer, Workload Identity and the Key Vault CSI Driver add-ons enabled. \\n\\n>*NOTE:* Both the OIDC Issuer and Workload Identity add-ons are needed if you want to use workload identities, however the Key Vault CSI driver can also work with service principals or even the managed identity assigned to the cluster itself. Using Workload Identity, however, is the most versatile and secure approach.\\n\\n```bash\\nRG=WorkloadIdentityKVCSIRG\\nLOC=eastus\\nCLUSTER_NAME=wikvcsilab\\nUNIQUE_ID=$CLUSTER_NAME$RANDOM\\nKEY_VAULT_NAME=$UNIQUE_ID\\n\\n# Create the resource group\\naz group create -g $RG -l $LOC\\n\\n# Create the cluster with the OIDC Issuer and Workload Identity enabled\\naz aks create -g $RG -n $CLUSTER_NAME \\\\\\n--node-count 1 \\\\\\n--enable-oidc-issuer \\\\\\n--enable-workload-identity \\\\\\n--enable-addons azure-keyvault-secrets-provider \\\\\\n--generate-ssh-keys\\n\\n# Get the cluster credentials\\naz aks get-credentials -g $RG -n $CLUSTER_NAME\\n```\\n\\n### Set up the identity \\n\\nIn order to federate a managed identity with a Kubernetes Service Account we need to get the AKS OIDC Issure URL, create the Managed Identity and Service Account and then create the federation.\\n\\n```bash\\n# Get the OIDC Issuer URL\\nexport AKS_OIDC_ISSUER=\\"$(az aks show -n $CLUSTER_NAME -g $RG --query \\"oidcIssuerProfile.issuerUrl\\" -otsv)\\"\\n\\n# Get the Tenant ID for later\\nexport IDENTITY_TENANT=$(az account show -o tsv --query tenantId)\\n\\n# Create the managed identity\\naz identity create --name kvcsi-demo-identity --resource-group $RG --location $LOC\\n\\n# Get identity client ID\\nexport USER_ASSIGNED_CLIENT_ID=$(az identity show --resource-group $RG --name kvcsi-demo-identity --query \'clientId\' -o tsv)\\n\\n# Create a service account to federate with the managed identity\\ncat <<EOF | kubectl apply -f -\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  annotations:\\n    azure.workload.identity/client-id: ${USER_ASSIGNED_CLIENT_ID}\\n  labels:\\n    azure.workload.identity/use: \\"true\\"\\n  name: kvcsi-demo-sa\\n  namespace: default\\nEOF\\n\\n# Federate the identity\\naz identity federated-credential create \\\\\\n--name kvcsi-demo-federated-id \\\\\\n--identity-name kvcsi-demo-identity \\\\\\n--resource-group $RG \\\\\\n--issuer ${AKS_OIDC_ISSUER} \\\\\\n--subject system:serviceaccount:default:kvcsi-demo-sa\\n```\\n\\n### Create the Key Vault and Secret\\n\\n```bash\\n# Create a key vault\\naz keyvault create --name $KEY_VAULT_NAME --resource-group $RG --location $LOC --enable-rbac-authorization false\\n\\n# Create a secret\\naz keyvault secret set --vault-name $KEY_VAULT_NAME --name \\"TestSecret\\" --value \\"Hello from Key Vault\\"\\n\\n# Grant access to the secret for the managed identity\\naz keyvault set-policy --name $KEY_VAULT_NAME -g $RG --secret-permissions get --spn \\"${USER_ASSIGNED_CLIENT_ID}\\"\\n\\n#####################################################################\\n# OPTIONAL\\n# We\'ll get the version ID for the secret but this is not mandatory\\n#####################################################################\\n\\n# Get the version ID\\naz keyvault secret show --vault-name $KEY_VAULT_NAME --name \\"TestSecret\\" -o tsv --query id\\nhttps://wi-demo-keyvault.vault.azure.net/secrets/Secret/ded8e5e3b3e040e9bfa5c47d0e28848a\\n\\n# The version ID is the last part of the resource id above\\n# We\'ll use this later\\nVERSION_ID=ded8e5e3b3e040e9bfa5c47d0e28848a\\n```\\n\\nThe [SecretProviderClass](https://secrets-store-csi-driver.sigs.k8s.io/getting-started/usage) is what is used to create the binding between the Kubernetes cluster and the Azure Key Vault. It allows you to define the identity accessing the Key Vault, the target Key Vault name and the details about the keys and secrets being accessed. It will also let you link Key Vault Secrets to Kubernetes Secrets, but we\'ll see more on that later.\\n\\n```bash\\n# Create a secret provider instance attached to the vault and secret\\ncat <<EOF | kubectl apply -f -\\napiVersion: secrets-store.csi.x-k8s.io/v1\\nkind: SecretProviderClass\\nmetadata:\\n  name: azure-kvcsi-wi # needs to be unique per namespace\\nspec:\\n  provider: azure\\n  parameters:\\n    usePodIdentity: \\"false\\"\\n    clientID: \\"${USER_ASSIGNED_CLIENT_ID}\\" # Setting this to use workload identity\\n    keyvaultName: ${KEY_VAULT_NAME}       # Set to the name of your key vault\\n    cloudName: \\"\\"                         # [OPTIONAL for Azure] if not provided, the Azure environment defaults to AzurePublicCloud\\n    objects:  |\\n      array:\\n        - |\\n          objectName: TestSecret             # Set to the name of your secret\\n          objectType: secret              # object types: secret, key, or cert\\n          objectVersion: \\"${VERSION_ID}\\"               # [OPTIONAL] object versions, default to latest if empty\\n    tenantId: \\"${IDENTITY_TENANT}\\"        # The tenant ID of the key vault\\nEOF\\n```\\n\\nNow deploy a pod that gets the value using the service account identity.\\n\\n```bash\\n# Create a pod to mount the secret\\ncat <<EOF | kubectl apply -f -\\nkind: Pod\\napiVersion: v1\\nmetadata:\\n  name: secrets-store-inline-wi\\n  labels:\\n    azure.workload.identity/use: \\"true\\"\\nspec:\\n  serviceAccountName: \\"kvcsi-demo-sa\\"\\n  containers:\\n    - name: ubuntu\\n      image: ubuntu:20.04\\n      command: [ \\"/bin/bash\\", \\"-c\\", \\"--\\" ]\\n      args: [ \\"while true; do sleep 30; done;\\" ]\\n      volumeMounts:\\n      - name: secrets-store01-inline\\n        mountPath: \\"/mnt/secrets-store\\"\\n        readOnly: true\\n  volumes:\\n    - name: secrets-store01-inline\\n      csi:\\n        driver: secrets-store.csi.k8s.io\\n        readOnly: true\\n        volumeAttributes:\\n          secretProviderClass: \\"azure-kvcsi-wi\\"\\nEOF\\n\\n# Check the secret loaded\\nkubectl exec -it secrets-store-inline-wi -- cat /mnt/secrets-store/TestSecret\\n```\\n\\n### Sync the same secret to a Kubernetes Secret\\n\\nAs mentioned above, you do have the option to synchronize Azure Key Vault secrets to Kubernetes secrets, which can be very useful. Just keep in mind that this driver uses the Container Storage Interface (CSI) so you still need to ensure a pod mounts a volume associated to the SecretProviderClass to initiate the Kubernetes secret creation.\\n\\nLets add a new SecretProviderClass that includes the secret synchronization configuration.\\n\\n```bash\\n# Create a secret provider instance attached to the vault and secret\\ncat <<EOF | kubectl apply -f -\\napiVersion: secrets-store.csi.x-k8s.io/v1\\nkind: SecretProviderClass\\nmetadata:\\n  name: azure-kvcsi-wi-sync # needs to be unique per namespace\\nspec:\\n  provider: azure\\n  parameters:\\n    usePodIdentity: \\"false\\"\\n    clientID: \\"${USER_ASSIGNED_CLIENT_ID}\\" # Setting this to use workload identity\\n    keyvaultName: ${KEY_VAULT_NAME}       # Set to the name of your key vault\\n    cloudName: \\"\\"                         # [OPTIONAL for Azure] if not provided, the Azure environment defaults to AzurePublicCloud\\n    objects:  |\\n      array:\\n        - |\\n          objectName: TestSecret             # Set to the name of your secret\\n          objectType: secret              # object types: secret, key, or cert\\n          objectVersion: \\"${VERSION_ID}\\"               # [OPTIONAL] object versions, default to latest if empty\\n    tenantId: \\"${IDENTITY_TENANT}\\"        # The tenant ID of the key vault\\n  secretObjects:                              # [OPTIONAL] SecretObjects defines the desired state of synced Kubernetes secret objects\\n  - data:\\n    - key: secretvalue                           # data field to populate\\n      objectName: TestSecret                        # name of the mounted content to sync; this could be the object name or the object alias\\n    secretName: foosecret                     # name of the Kubernetes secret object\\n    type: Opaque     \\nEOF\\n```\\n\\nNow, create a pod that loads the secret both as a volume and an environment variable from our synchronized Kubernetes secret.\\n\\n```bash\\n# Create a pod to mount the secret\\ncat <<EOF | kubectl apply -f -\\nkind: Pod\\napiVersion: v1\\nmetadata:\\n  name: secrets-store-inline-wi-sync\\n  labels:\\n    azure.workload.identity/use: \\"true\\"\\nspec:\\n  serviceAccountName: \\"kvcsi-demo-sa\\"\\n  containers:\\n    - name: ubuntu\\n      image: ubuntu:20.04\\n      command: [ \\"/bin/bash\\", \\"-c\\", \\"--\\" ]\\n      args: [ \\"while true; do sleep 30; done;\\" ]\\n      volumeMounts:\\n      - name: secrets-store01-inline\\n        mountPath: \\"/mnt/secrets-store\\"\\n        readOnly: true\\n      env:\\n      - name: SECRET_DATA\\n        valueFrom:\\n          secretKeyRef:\\n            name: foosecret\\n            key: secretvalue        \\n  volumes:\\n    - name: secrets-store01-inline\\n      csi:\\n        driver: secrets-store.csi.k8s.io\\n        readOnly: true\\n        volumeAttributes:\\n          secretProviderClass: \\"azure-kvcsi-wi-sync\\"\\nEOF\\n```\\n\\nFinally, test to make sure the secret is properly loaded to the volume and environment variable.\\n\\n```bash\\n# Check that the secret was properly mounted as a volume\\nkubectl exec -it secrets-store-inline-wi-sync -- cat /mnt/secrets-store/TestSecret\\n\\n# Check that the Kubernetes Secret was created\\nkubectl get secret foosecret -o jsonpath=\'{.data.secretvalue}\'|base64 --decode\\n\\n# Check that the secret was properly mounted from the kubernetes secret as an enviornment variable\\nkubectl exec -it secrets-store-inline-wi-sync -- /bin/bash -c \'echo $SECRET_DATA\'\\n```\\n\\n### Conclusion\\n\\nCongrats! You should now have a working pod that mounts a key vault secret via the CSI driver and another pod that mounts the secret as an environment variable from a sync\'d Kubernetes Secret."},{"id":"/2024/02/02/securing-your-aks-cluster-with-a-linux-firewall-vm","metadata":{"permalink":"/2024/02/02/securing-your-aks-cluster-with-a-linux-firewall-vm","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2024-02-02/securing-your-aks-cluster-with-a-linux-firewall-vm/index.md","source":"@site/blog/2024-02-02/securing-your-aks-cluster-with-a-linux-firewall-vm/index.md","title":"Securing your AKS cluster with a Linux Firewall VM","description":"How to configure the firewall VM, the virtual network, and the network security group to route traffic to your AKS endpoints.","date":"2024-02-02T00:00:00.000Z","tags":[],"readingTime":8.36,"hasTruncateMarker":true,"authors":[{"name":"Diego Casati","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/dcasati","socials":{"x":"https://x.com/ve6dpc","github":"https://github.com/dcasati"},"imageURL":"https://github.com/dcasati.png","key":"diego_casati","page":null}],"frontMatter":{"authors":["diego_casati"],"date":"2024-02-02","description":"How to configure the firewall VM, the virtual network, and the network security group to route traffic to your AKS endpoints.","tags":[],"title":"Securing your AKS cluster with a Linux Firewall VM"},"unlisted":false,"prevItem":{"title":"Using the Azure Key Vault CSI Driver with Workload Identity","permalink":"/2024/03/05/workload-identity-kv-csi"},"nextItem":{"title":"Using Workload Idenity to Access Azure Blob Storage","permalink":"/2023/12/19/workload-identity-blob-example"}},"content":"A network virtual appliance (NVA) is a virtual machine that performs network functions such as firewalling. In this post, I will walk you through how to use a Linux VM as an NVA in Azure and route traffic to an endpoint running on Azure Kubernetes Service (AKS). I will cover two scenarios: one where the traffic goes through an internal load balancer, and another where the traffic goes directly to a pod.\\n\\nThis assumes you already have a Linux VM and an AKS Cluster created - both in their own VNET. The cluster and the Linux VM need to be VNET peered already. [Here is a quick start on VNET peering if you need a refresher](https://learn.microsoft.com/en-us/azure/virtual-network/virtual-network-manage-peering?tabs=peering-cli).\\n\\n\x3c!-- truncate --\x3e\\n\\nFor this example we will be using Ubuntu 22.04. Since we have a few excellent tutorials on how to create a Linux VM on Azure I will not be describing that process here. If you need an example, [here\'s a good one that describe the creation process using the Azure CLI](https://learn.microsoft.com/en-us/azure/virtual-machines/linux/tutorial-manage-vm).\\n\\nAfter your VM is created, `ssh` into it and allow IP forwarding:\\n\\n```bash\\nsysctl -w net.ipv4.ip_forward=1\\n```\\n\\nFor that change to be persistent, make sure you add it to `/etc/sysctl.conf`. Moving on, let\'s look into our first scenario.\\n\\n### Scenario 1: NVA routing to a pod through an internal load balancer\\n\\n![scenario 1](/img/2024-02-02-securing-your-aks-cluster-with-a-linux-firewall-vm/scenario1.jpg)\\n\\nIn this scenario, we will route traffic from the Linux VM to a pod in the AKS cluster through an internal load balancer. The internal load balancer will balance the traffic among the pods that match a certain label selector. The diagram below shows the network topology for this scenario:\\n\\nTo implement this scenario, we will need to do the following steps:\\n\\n* Create a sample workload on the AKS cluster\\n* Create an internal load balancer on the AKS cluster\\n* Set up the firewall rules on the Linux VM\\n\\n#### Create a sample workload\\n\\nFirst, we will create a sample workload on the AKS cluster to test our traffic routing. We will use a simple Python web server that listens on port 80 and returns a hello message. We will deploy a pod that runs this web server and label it with `run: tmp-shell`. To do this, run the following commands:\\n\\n```bash\\n# Connect to the AKS cluster\\naz aks get-credentials --resource-group <your-resource-group> --name <your-aks-cluster>\\n\\n# Deploy a pod for testing\\nkubectl run tmp-shell --image nicolaka/netshoot --labels run=tmp-shell\\n\\n# Run a simple Python web server on port 80\\nkubectl exec -it tmp-shell -- python3 -m http.server 80\\n```\\n\\nYou should see something like this:\\n\\n```bash\\nServing HTTP on 0.0.0.0 port 80 (http://0.0.0.0:80/) ...\\n```\\n\\n#### Create an internal load balancer\\n\\nNext, we will create an internal load balancer on the AKS cluster that will expose the pod on port `49180`. The internal load balancer will use the label selector `run: tmp-shell` to find the pod and balance the traffic among the pods that match this selector. To do this, run the following commands:\\n\\n```bash\\n# Create a service of type LoadBalancer with an annotation to make it internal\\ncat <<EOF | kubectl apply -f -\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: tmp-shell\\n  annotations:\\n    service.beta.kubernetes.io/azure-load-balancer-internal: \\"true\\"\\nspec:\\n  type: LoadBalancer\\n  ports:\\n  - port: 49180\\n  selector:\\n    run: tmp-shell\\nEOF\\n```\\n\\nThis will create a service named tmp-shell of type LoadBalancer with an annotation to make it internal. The service will expose port 49180 and forward the traffic to port 80 of the pod that matches the label selector `run: tmp-shell`.\\n\\nTo verify that the service is created and has an internal IP address, run the following command:\\n\\n```bash\\nkubectl get service tmp-shell\\n```\\n\\nYou should see something like this:\\n\\n```bash\\nNAME         TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)           AGE\\nkubernetes   ClusterIP      10.0.0.1       <none>         443/TCP           2d1h\\ntmp-shell    LoadBalancer   10.0.166.113   10.224.0.222   49180:30265/TCP   47h\\n```\\n\\nThis means that the service has an internal IP address of `10.224.0.222` and is listening on port 49180.\\n\\n#### Setup the firewall\\n\\nFinally, we will set up the firewall rules on the Linux VM to route the traffic from the VM to the internal load balancer. We will use iptables to configure the firewall rules. To do this, run the following commands on the Linux VM:\\n\\n```bash\\niptables -A FORWARD -p tcp --syn --dport 49181 -m conntrack --ctstate NEW -j ACCEPT\\niptables -A FORWARD -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT\\niptables -P FORWARD DROP\\niptables -t nat -A PREROUTING -i eth0 -p tcp --dport 49181 -j DNAT --to-destination 10.224.0.222\\niptables -t nat -A POSTROUTING -o eth0 -p tcp --dport 49181 -d 10.224.0.222 -j SNAT --to-source 10.3.0.4\\n```\\n\\nExplanation:\\n\\n| Rule | Notes\\n| - | -\\n| iptables -A PREROUTING -i eth0 -p tcp -m tcp --dport 49181 -j DNAT --to-destination 10.224.0.222  | Packet arrived on eth0 (coming from the public IP of the VM) and get\'s DNAT\'ed to the Internal Load Balancer private IP and to the service in AKS that will send traffic to the pod on port 80.\\n| iptables -A FORWARD -p tcp -m tcp --dport 49181 -m conntrack --ctstate NEW -j ACCEPT | Allow forwarding on port 49181.\\n| iptables -A FORWARD -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT | Match any packet that is related to an existing connection, using the conntrack module. This is necessary to allow the established and related connections to the internal service.\\n| iptables -P FORWARD DROP | Set the policy to drop packets in the FORWARD chain. This means, unless specified (see line above) we will drop traffic.\\n| iptables -A POSTROUTING -d 10.224.0.222/32 -o eth0 -p tcp -m tcp --dport 49181 -j SNAT --to-source 10.3.0.4  | Send traffic to the Internal Load Balancer on port 49181. SNAT the source IP with the private IP of the firewall (10.3.0.4).\\n\\nAt this point in time, our rules are in place but they will not persist a reboot. To make sure the Firewall loads these rules on boot time we need to install the `iptables-persistent` package:\\n\\n```bash\\n$ sudo apt install iptables-persistent\\n```\\n\\nOnce that\'s installed, save your current configuration so iptables-persistent can load them during boot time:\\n\\n```bash\\n$ sudo iptables-save > /etc/iptables/rules.v4\\n```\\n\\n### Scenario 2: NVA routing directly to a pod\\n\\nIn this second scenario, we will route traffic from the Linux VM directly to a pod in the AKS cluster without going through an internal load balancer. The diagram below shows the network topology for this scenario:\\n\\n![scenario 2](/img/2024-02-02-securing-your-aks-cluster-with-a-linux-firewall-vm/scenario2.jpg)\\n\\nTo implement this scenario, we will need to do the following steps:\\n\\n* Create a sample workload on the AKS cluster. Follow the steps described earlier to the deploy the POD using `netshoot` as your base image. You do not need to deploy the Internal Load Balancer as we will not be using it for this scenario.\\n* Set up the firewall rules on the Linux VM\\n\\nThe new firewall rules are:\\n\\n```bash\\niptables -A FORWARD -p tcp --syn --dport 80 -m conntrack --ctstate NEW -j ACCEPT\\niptables -A FORWARD -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT\\niptables -P FORWARD DROP\\niptables -t nat -A PREROUTING -i eth0 -p tcp --dport 49181 -j DNAT --to-destination 10.224.0.118:80\\niptables -t nat -A POSTROUTING -o eth0 -p tcp --dport 80 -d 10.224.0.118 -j SNAT --to-source 10.3.0.4\\n```\\n\\nExplanation:\\n\\n| Rule | Notes\\n| - | - \\n| iptables -A PREROUTING -i eth0 -p tcp -m tcp --dport 49181 -j DNAT --to-destination 10.224.0.118:80  | Packet arrived on eth0 (coming from the public IP of the VM) and get\'s DNAT\'ed to the POD private IP on port 80.\\n| iptables -A FORWARD -p tcp -m tcp --dport 80 -m conntrack --ctstate NEW -j ACCEPT | At this point, the original packets are now going to be send to port 80. This rule allows that flow.\\n| iptables -A FORWARD -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT | Match any packet that is related to an existing connection, using the conntrack module. This is necessary to allow the established and related connections to the internal service.\\n| iptables -P FORWARD DROP | Set the policy to drop packets in the FORWARD chain. This means, unless specified (see line above) we will drop traffic.\\n| iptables -A POSTROUTING -d 10.224.0.118/32 -o eth0 -p tcp -m tcp --dport 80 -j SNAT --to-source 10.3.0.4  | The last rule before the packet leaves the kernel, we run a Source NAT (SNAT) and add the Firewall private IP so the pod knows where to return this packet to. Once that packet hits the firewall, we can return it to the sender on the Internet.\\n\\n#### Testing and validating\\n\\nTo test and verify that the port forwarding rules are working - for both scenarios - you can use a tool like telnet or curl to connect to the external port 49181 from another host. You should see the response from the internal service on port 80. For example, the following command will send a HTTP request to the external port 49181 and display the response:\\n\\n| Test case | Target | Note\\n|- | - | -\\n| From the firewall to the pod | ``curl http://${POD_IP_ADDRESS}`` | Pod listens on port 80\\n| From the Internet to the public IP of the firewall | ``curl http://${PUBLIC_IP}:49181`` | This connection will be DNATed to proper endpoint (pod or ILB)\\n\\nYou should see something like this:\\n\\n```bash\\ntmp-shell:~# python3 -m http.server 80\\nServing HTTP on 0.0.0.0 port 80 (http://0.0.0.0:80/) ...\\n10.3.0.4 - - [02/Feb/2024 20:50:32] \\"GET / HTTP/1.1\\" 200 -\\n10.3.0.4 - - [02/Feb/2024 20:50:33] \\"GET / HTTP/1.1\\" 200 -\\n10.3.0.4 - - [02/Feb/2024 20:50:34] \\"GET / HTTP/1.1\\" 200 -\\n10.3.0.4 - - [02/Feb/2024 20:50:35] \\"GET / HTTP/1.1\\" 200 -\\n```\\n\\n### Conclusion\\n\\nIn this article we\'ve explored how create your own - basic - network virtual appliance and route traffic to an AKS cluster. from here, you can take this as a base for more complex and custom examples. Good luck !"},{"id":"/2023/12/19/workload-identity-blob-example","metadata":{"permalink":"/2023/12/19/workload-identity-blob-example","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2023-12-19/workload-identity-blob-example/index.md","source":"@site/blog/2023-12-19/workload-identity-blob-example/index.md","title":"Using Workload Idenity to Access Azure Blob Storage","description":"In this post we\'ll show the steps needed to create an AKS cluster, enabled with Azure Workload Identity and then we\'ll build a sample dotnet app that writes files to Azure Blob Storage","date":"2023-12-19T00:00:00.000Z","tags":[],"readingTime":4.52,"hasTruncateMarker":true,"authors":[{"name":"Steve Griffith","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/swgriffith","socials":{"x":"https://x.com/SteveGriffith","github":"https://github.com/swgriffith"},"imageURL":"https://github.com/swgriffith.png","key":"steve_griffith","page":null}],"frontMatter":{"authors":["steve_griffith"],"date":"2023-12-19","description":"In this post we\'ll show the steps needed to create an AKS cluster, enabled with Azure Workload Identity and then we\'ll build a sample dotnet app that writes files to Azure Blob Storage","tags":[],"title":"Using Workload Idenity to Access Azure Blob Storage"},"unlisted":false,"prevItem":{"title":"Securing your AKS cluster with a Linux Firewall VM","permalink":"/2024/02/02/securing-your-aks-cluster-with-a-linux-firewall-vm"},"nextItem":{"title":"Using External DNS in AKS with Azure Workload Identity","permalink":"/2023/12/18/external-dns-workload-identity"}},"content":"The following walkthrough shows how you can using [Azure Workload Identity](https://azure.github.io/azure-workload-identity/docs/) with the [AKS Workload Identity](https://learn.microsoft.com/en-us/azure/aks/workload-identity-overview) add-on along with [MSAL](https://learn.microsoft.com/en-us/azure/active-directory/develop/reference-v2-libraries) to access an Azure Blob Storage Account.\\n\\n\x3c!-- truncate --\x3e\\n\\n### Cluster Creation\\n\\nNow lets create the AKS cluster with the OIDC Issuer and Workload Identity add-on enabled.\\n\\n```bash\\nRG=WorkloadIdentityRG\\nLOC=eastus\\nCLUSTER_NAME=wilab\\nUNIQUE_ID=$CLUSTER_NAME$RANDOM\\nACR_NAME=$UNIQUE_ID\\nSTORAGE_ACCT_NAME=griffdemo\\n\\n# Create the resource group\\naz group create -g $RG -l $LOC\\n\\n# Create the cluster with the OIDC Issuer and Workload Identity enabled\\naz aks create -g $RG -n $CLUSTER_NAME \\\\\\n--node-count 1 \\\\\\n--enable-oidc-issuer \\\\\\n--enable-workload-identity \\\\\\n--generate-ssh-keys\\n\\n# Get the cluster credentials\\naz aks get-credentials -g $RG -n $CLUSTER_NAME\\n```\\n\\n### Set up the identity \\n\\nIn order to federate a managed identity with a Kubernetes Service Account we need to get the AKS OIDC Issure URL, create the Managed Identity and Service Account and then create the federation.\\n\\n```bash\\n# Get the OIDC Issuer URL\\nexport AKS_OIDC_ISSUER=\\"$(az aks show -n $CLUSTER_NAME -g $RG --query \\"oidcIssuerProfile.issuerUrl\\" -otsv)\\"\\n\\n# Create the managed identity\\naz identity create --name wi-demo-identity --resource-group $RG --location $LOC\\n\\n# Get identity client ID\\nexport USER_ASSIGNED_CLIENT_ID=$(az identity show --resource-group $RG --name wi-demo-identity --query \'clientId\' -o tsv)\\n\\n# Create a service account to federate with the managed identity\\ncat <<EOF | kubectl apply -f -\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  annotations:\\n    azure.workload.identity/client-id: ${USER_ASSIGNED_CLIENT_ID}\\n  labels:\\n    azure.workload.identity/use: \\"true\\"\\n  name: wi-demo-sa\\n  namespace: default\\nEOF\\n\\n# Federate the identity\\naz identity federated-credential create \\\\\\n--name wi-demo-federated-id \\\\\\n--identity-name wi-demo-identity \\\\\\n--resource-group $RG \\\\\\n--issuer ${AKS_OIDC_ISSUER} \\\\\\n--subject system:serviceaccount:default:wi-demo-sa\\n```\\n\\n### Create the Blob Storage Account\\n\\n```bash\\n# Create a blob storage account\\naz storage account create \\\\\\n--name $STORAGE_ACCT_NAME \\\\\\n--resource-group $RG \\\\\\n--location $LOC \\\\\\n--sku Standard_LRS \\\\\\n--encryption-services blob\\n\\n# Get the resource ID of the storage account\\nSTORAGE_ACCT_ID=$(az storage account show -g $RG -n $STORAGE_ACCT_NAME --query id -o tsv)\\n\\n# Get the current signed in user ID\\nCURRENT_USER=$(az ad signed-in-user show --query id -o tsv)\\n\\n# Grant the current user contributor rights for testing\\naz role assignment create \\\\\\n--role \\"Storage Blob Data Contributor\\" \\\\\\n--assignee $CURRENT_USER \\\\\\n--scope \\"${STORAGE_ACCT_ID}\\"\\n\\n# Grant the managed identity contributor rights\\naz role assignment create \\\\\\n--role \\"Storage Blob Data Contributor\\" \\\\\\n--assignee $USER_ASSIGNED_CLIENT_ID \\\\\\n--scope \\"${STORAGE_ACCT_ID}\\"\\n\\n# Create a storage account container with login auth mode enabled\\naz storage container create --account-name $STORAGE_ACCT_NAME --name data --auth-mode login\\n```\\n\\n## Create the sample app\\n\\n```bash\\n# Create and test a new console app\\ndotnet new console -n blob-console-app\\ncd blob-console-app\\ndotnet run\\n\\n# Add the Key Vault and Azure Identity Packages\\ndotnet add package Azure.Storage.Blobs\\ndotnet add package Azure.Identity\\n```\\n\\nEdit the app as follows:\\n\\n```csharp\\nusing Azure.Storage.Blobs;\\nusing Azure.Storage.Blobs.Models;\\nusing System;\\nusing System.IO;\\nusing Azure.Identity;\\n\\nclass Program\\n    {\\n        static void Main(string[] args)\\n        {\\n          // Get Storage Account Name\\n          string? storageAcctName = Environment.GetEnvironmentVariable(\\"STORAGE_ACCT_NAME\\");;\\n          // Get the Storage Container Name\\n          string? containerName = Environment.GetEnvironmentVariable(\\"CONTAINER_NAME\\");;\\n\\n          // Check values for null or empty\\n          if (string.IsNullOrEmpty(storageAcctName)||string.IsNullOrEmpty(containerName))\\n          {\\n            Console.WriteLine(\\"Storage Account or Container Name are null or empty\\");\\n            Environment.Exit(0);\\n          }\\n\\n          while (true)\\n          {\\n            MainAsync(storageAcctName,containerName).Wait();\\n            System.Threading.Thread.Sleep(5000);\\n          }\\n          \\n        }\\n\\n        static async Task MainAsync(string storageAcctName, string containerName)\\n        {\\n          var blobServiceClient = new BlobServiceClient(\\n                  new Uri(String.Format(\\"https://{0}.blob.core.windows.net\\",storageAcctName)),\\n                  new DefaultAzureCredential());\\n\\n          BlobContainerClient containerClient = blobServiceClient.GetBlobContainerClient(containerName);\\n\\n          // Create a local file in the ./data/ directory for uploading and downloading\\n          string localPath = \\"data\\";\\n          Directory.CreateDirectory(localPath);\\n          string fileName = Guid.NewGuid().ToString() + \\".txt\\";\\n          string localFilePath = Path.Combine(localPath, fileName);\\n\\n          // Write text to the file\\n          await File.WriteAllTextAsync(localFilePath, \\"Hello, World!\\");\\n\\n          // Get a reference to a blob\\n          BlobClient blobClient = containerClient.GetBlobClient(fileName);\\n\\n          Console.WriteLine(\\"Uploading to Blob storage as blob:\\\\n\\\\t {0}\\\\n\\", blobClient.Uri);\\n\\n          // Upload data from the local file\\n          await blobClient.UploadAsync(localFilePath, true);\\n        }\\n\\n    }\\n```\\nCreate a new Dockerfile with the following:\\n\\n```bash\\nFROM mcr.microsoft.com/dotnet/sdk:7.0 AS build-env\\nWORKDIR /App\\n\\n# Copy everything\\nCOPY . ./\\n# Restore as distinct layers\\nRUN dotnet restore\\n# Build and publish a release\\nRUN dotnet publish -c Release -o out\\n\\n# Build runtime image\\nFROM mcr.microsoft.com/dotnet/aspnet:7.0\\nWORKDIR /App\\nCOPY --from=build-env /App/out .\\nENTRYPOINT [\\"dotnet\\", \\"blob-console-app.dll\\"]\\n```\\n\\nBuild the image. I\'ll create an Azure Container Registry and build there, and then link that ACR to my AKS cluster.\\n\\n```bash\\n# Create the ACR\\naz acr create -g $RG -n $ACR_NAME --sku Standard\\n\\n# Build the image\\naz acr build -t wi-blob-test -r $ACR_NAME .\\n\\n# Link the ACR to the AKS cluster\\naz aks update -g $RG -n $CLUSTER_NAME --attach-acr $ACR_NAME\\n```\\n\\nNow deploy a pod that runs our blob storage app using the service account identity.\\n\\n```bash\\n\\ncat <<EOF | kubectl apply -f -\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: wi-blob-test\\n  namespace: default\\n  labels:\\n    azure.workload.identity/use: \\"true\\"  \\nspec:\\n  serviceAccountName: wi-demo-sa\\n  containers:\\n    - image: ${ACR_NAME}.azurecr.io/wi-blob-test\\n      name: wi-blob-test\\n      env:\\n      - name: STORAGE_ACCT_NAME\\n        value: ${STORAGE_ACCT_NAME}\\n      - name: CONTAINER_NAME\\n        value: data      \\n  nodeSelector:\\n    kubernetes.io/os: linux\\nEOF\\n\\n# Check the pod logs\\nkubectl logs -f wi-blob-test\\n\\n# Sample Output\\nUploading to Blob storage as blob:\\n\\t https://griffdemo.blob.core.windows.net/data/quickstart3efa9a81-9672-4617-a6ff-f11fb93d7c84.txt\\n\\nUploading to Blob storage as blob:\\n\\t https://griffdemo.blob.core.windows.net/data/quickstart23968d6b-80c5-4c82-8bcf-860fa00edbd3.txt\\n\\nUploading to Blob storage as blob:\\n\\t https://griffdemo.blob.core.windows.net/data/quickstart0e20e7ef-c3ba-4fd3-a3d5-c27579d2ba96.txt\\n```\\n\\n### Conclusion\\n\\nCongrats! You should now have a working pod that uses MSAL along with a Kubernetes Service Account federated to an Azure Managed Identity to access Azure Blob Storage."},{"id":"/2023/12/18/external-dns-workload-identity","metadata":{"permalink":"/2023/12/18/external-dns-workload-identity","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2023-12-18/external-dns-workload-identity/index.md","source":"@site/blog/2023-12-18/external-dns-workload-identity/index.md","title":"Using External DNS in AKS with Azure Workload Identity","description":"The External DNS project supports auto record creation in both Azure public and Private DNS Zones. In this walk through we\'ll show how to configure an install External DNS in an AKS cluster using Workload Identity to update A records in an Azure Private DNS Zone","date":"2023-12-18T00:00:00.000Z","tags":[],"readingTime":6.28,"hasTruncateMarker":true,"authors":[{"name":"Steve Griffith","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/swgriffith","socials":{"x":"https://x.com/SteveGriffith","github":"https://github.com/swgriffith"},"imageURL":"https://github.com/swgriffith.png","key":"steve_griffith","page":null}],"frontMatter":{"authors":["steve_griffith"],"date":"2023-12-18","description":"The External DNS project supports auto record creation in both Azure public and Private DNS Zones. In this walk through we\'ll show how to configure an install External DNS in an AKS cluster using Workload Identity to update A records in an Azure Private DNS Zone","tags":[],"title":"Using External DNS in AKS with Azure Workload Identity"},"unlisted":false,"prevItem":{"title":"Using Workload Idenity to Access Azure Blob Storage","permalink":"/2023/12/19/workload-identity-blob-example"},"nextItem":{"title":"Using Linux Capabilities in AKS","permalink":"/2023/11/27/capabilities-in-aks"}},"content":"The following walk-through shows how to use the [External DNS](https://github.com/kubernetes-sigs/external-dns) project to monitor for services with DNS annotation to automatically create DNS records in an Azure Private Zone. The system user authorized to update the private zone will be an Azure Managed Identity, using AKS Workload Identity support.\\n\\n\x3c!-- truncate --\x3e\\n\\n### Set Environment Variables\\n\\nUpdate the following with your own prefered values.\\n\\n```bash\\nRG=EphExternalDNSDemo\\nLOC=eastus\\nAZURE_DNS_ZONE=\\"griffdemo123.com\\" # DNS zone name like example.com or sub.example.com\\nCLUSTER_NAME=externaldns\\nTENANT_ID=$(az account show --query tenantId -o tsv)\\nSUB_ID=$(az account show --query id -o tsv)\\n```\\n\\n### Create the Resource Group and Vnet\\n\\nNow to create the resource group and Vnet which we will use for both the private zone and the AKS cluster. You can obviously substitute your own existing Vnet, but don\'t miss the next step where the private zone get\'s linked to the vnet.\\n\\n```bash\\n# Create the Resource Group\\naz group create -n $RG -l $LOC\\n\\n# Create the Vnet\\naz network vnet create \\\\\\n--name testvnet \\\\\\n--resource-group $RG \\\\\\n--address-prefix 10.2.0.0/16 \\\\\\n--subnet-name aks \\\\\\n--subnet-prefixes 10.2.0.0/24\\n\\n# Get the subnet id for later use\\nSUBNET_ID=$(az network vnet subnet show -g $RG --vnet-name testvnet -n aks -o tsv --query id)\\n```\\n\\n### Create the Private DNS Zone\\n\\nNow to create the private zone and link it to the vnet.\\n\\n```bash\\n# Create the DNS Zone\\naz network private-dns zone create \\\\\\n--resource-group $RG \\\\\\n--name $AZURE_DNS_ZONE\\n\\n# Link the private zone to the vnet\\naz network private-dns link vnet create \\\\\\n-g $RG \\\\\\n-n zonelink \\\\\\n-z $AZURE_DNS_ZONE \\\\\\n-v testvnet \\\\\\n--registration-enabled false\\n```\\n\\n### Create the AKS Cluster\\n\\nWe\'ll create the AKS cluster in the above created subnet, and enable the flags for Workload Identity Support (i.e. OIDC Issuer and Workload Identity). \\n\\n```bash\\n# Create the AKS Cluster\\naz aks create \\\\\\n-g $RG \\\\\\n-n $CLUSTER_NAME \\\\\\n--vnet-subnet-id $SUBNET_ID \\\\\\n--enable-oidc-issuer \\\\\\n--enable-workload-identity \\\\\\n--enable-managed-identity\\n\\n# Get the cluster credentials\\naz aks get-credentials \\\\\\n-g $RG \\\\\\n-n $CLUSTER_NAME \\n```\\n\\n### Setup with Workload Identity\\n\\nAzure Workload Identity for Kubernetes enables the finest grained control of the user that will be managing the DNS records, because we can isolate the rights specific to those needed to control the private zone. In the following steps, we\'ll get the OIDC issuer URL, create the managed identity, and create the Azure AD Federation for the service account used by external DNS.\\n\\n>**NOTE:** In our setup we will assume external DNS resides in the \'default\' namespace. If you move external DNS to it\'s own namespace, you\'ll need to make sure the \'az identity federated-credential create\' command below references the right namespace in the fully qualified service account name in the \'--subject\' field. \\n\\n```bash\\n# Get the OIDC Issuer URL\\nexport AKS_OIDC_ISSUER=\\"$(az aks show -n $CLUSTER_NAME -g $RG --query \\"oidcIssuerProfile.issuerUrl\\" -otsv)\\"\\n\\n# Create the managed identity\\naz identity create --name external-dns-identity --resource-group $RG --location $LOC\\n\\n# Get identity client ID\\nexport USER_ASSIGNED_CLIENT_ID=$(az identity show --resource-group $RG --name external-dns-identity --query \'clientId\' -o tsv)\\n\\n# Get the resource group ID\\nRG_ID=$(az group show -n $RG -o tsv --query id)\\n\\n# Get the DNS Zone ID\\nDNS_ID=$(az network private-dns zone show --name $AZURE_DNS_ZONE \\\\\\n --resource-group $RG --query \\"id\\" --output tsv)\\n\\n# Give the kubelet identity DNS Contributor rights\\naz role assignment create \\\\\\n--assignee $USER_ASSIGNED_CLIENT_ID \\\\\\n--role \\"Private DNS Zone Contributor\\" \\\\\\n--scope \\"$DNS_ID\\"\\n\\naz role assignment create \\\\\\n--role \\"Reader\\" \\\\\\n--assignee $USER_ASSIGNED_CLIENT_ID \\\\\\n--scope $RG_ID\\n\\n# Federate the identity\\naz identity federated-credential create \\\\\\n--name external-dns-identity \\\\\\n--identity-name external-dns-identity \\\\\\n--resource-group $RG \\\\\\n--issuer ${AKS_OIDC_ISSUER} \\\\\\n--subject system:serviceaccount:default:external-dns\\n\\n```\\n\\n\\n### Install External DNS\\n\\nWe\'ll install External DNS using it\'s helm chart, setting the values to ensure we\'re using the Azure Private DNS provider and pass in the managed identity details.\\n\\nFirst, lets create the values file. \\n\\n>**NOTE:** We default to the \'debug\' log level for testing, but you can remove that line to minimize logging once you\'re up and running. The default value for logLevel is \'info\'.\\n\\n```bash\\ncat <<EOF > values.yaml\\nfullnameOverride: external-dns\\n\\nserviceAccount:\\n  annotations:\\n    azure.workload.identity/client-id: ${USER_ASSIGNED_CLIENT_ID}\\n\\npodLabels:\\n  azure.workload.identity/use: \\"true\\"\\n\\nprovider: azure-private-dns\\n\\nazure:\\n  resourceGroup: \\"${RG}\\"\\n  tenantId: \\"${TENANT_ID}\\"\\n  subscriptionId: \\"${SUB_ID}\\"\\n  useWorkloadIdentityExtension: true\\n\\nlogLevel: debug\\n\\nEOF\\n```\\n\\nRun the helm install.\\n\\n```bash\\n# Add the helm repo\\nhelm repo add bitnami https://charts.bitnami.com/bitnami\\n\\n# Update the helm repo in case you already have it\\nhelm repo update bitnami\\n\\n# Install external dns\\nhelm install external-dns bitnami/external-dns -f values.yaml\\n\\n```\\n\\n### Test External DNS\\n\\nFinally, lets make sure everything is working. We\'ll create a deployment and a service with both a private load balancer annotation and the annotation used by External DNS to trigger record creation.\\n\\n```bash\\n# Test ExternalDNS\\ncat << EOF | kubectl apply -f -\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      annotations:\\n        external-dns.alpha.kubernetes.io/hostname: pod.griffdemo123.com\\n      labels:\\n        app: nginx\\n    spec:\\n      hostNetwork: true\\n      containers:\\n      - image: nginx\\n        name: nginx\\n        ports:\\n        - containerPort: 80\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: nginx-svc\\n  annotations:\\n    service.beta.kubernetes.io/azure-load-balancer-internal: \\"true\\"\\n    external-dns.alpha.kubernetes.io/hostname: hello.griffdemo123.com\\n    external-dns.alpha.kubernetes.io/internal-hostname: hello-clusterip.griffdemo123.com\\nspec:\\n  ports:\\n  - port: 80\\n    protocol: TCP\\n    targetPort: 80\\n  selector:\\n    app: nginx\\n  type: LoadBalancer\\nEOF\\n```\\n\\nYou can check the logs for external DNS as follows:\\n\\n```bash\\n# To dump the current pod logs\\nkubectl logs -l app.kubernetes.io/instance=external-dns\\n\\n# To follow the pod logs\\nkubectl logs -f -l app.kubernetes.io/instance=external-dns\\n```\\n\\nShow the records created.\\n\\n```bash\\naz network private-dns record-set a list -g $RG -z $AZURE_DNS_ZONE -o yaml\\n\\n# SAMPLE OUTPUT\\n- aRecords:\\n  - ipv4Address: 10.2.0.7\\n  etag: 1f85f096-4036-408f-aa54-2d58d0523a96\\n  fqdn: hello.griffdemo123.com.\\n  id: /subscriptions/XXXXXX-XXXX/resourceGroups/ephexternaldnsdemo/providers/Microsoft.Network/privateDnsZones/griffdemo123.com/A/hello\\n  isAutoRegistered: false\\n  name: hello\\n  resourceGroup: ephexternaldnsdemo\\n  ttl: 300\\n  type: Microsoft.Network/privateDnsZones/A\\n- aRecords:\\n  - ipv4Address: 10.0.11.167\\n  etag: bf268fab-6678-4ae4-ae50-0cf49b314a0c\\n  fqdn: hello-clusterip.griffdemo123.com.\\n  id: /subscriptions/XXXXXX-XXXX/resourceGroups/ephexternaldnsdemo/providers/Microsoft.Network/privateDnsZones/griffdemo123.com/A/hello-clusterip\\n  isAutoRegistered: false\\n  name: hello-clusterip\\n  resourceGroup: ephexternaldnsdemo\\n  ttl: 300\\n  type: Microsoft.Network/privateDnsZones/A\\n```\\n\\n### Create a Record for a Pod IP\\n\\nIf you need to create a DNS record for a pod IP, you can do this by creating a headless service that is annotated for external-dns. External DNS will see the service and then go and retrieve the pod IP, as documented in this PR: \\n[for headless services use podip instead of hostip #498](https://github.com/kubernetes-sigs/external-dns/pull/498)\\n\\n```bash\\n# Test ExternalDNS\\ncat << EOF | kubectl apply -f -\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx\\nspec:\\n  selector:\\n    matchLabels:\\n      app: nginx\\n  template:\\n    metadata:\\n      labels:\\n        app: nginx\\n    spec:\\n      containers:\\n      - image: nginx\\n        name: nginx\\n        ports:\\n        - containerPort: 80\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: nginx-svc\\n  annotations:\\n    external-dns.alpha.kubernetes.io/hostname: pod.griffdemo123.com\\nspec:\\n  ports:\\n  - port: 80\\n    protocol: TCP\\n    targetPort: 80\\n  selector:\\n    app: nginx\\n  clusterIP: None\\nEOF\\n```\\n\\nCheck the record was created via CLI, or you can use the portal\\n\\n```bash\\n# Get the pod IP from kubernetes\\nkubectl get pod -l app=nginx -o jsonpath=\'{.items[0].status.podIP}\'\\n\\n# Sample Output\\n10.244.2.14\\n\\n# Get the IP from the Azure Private Zone\\naz network private-dns record-set a list -g $RG -z $AZURE_DNS_ZONE -o yaml --query \\"[?fqdn == \'pod.$AZURE_DNS_ZONE.\'].aRecords\\"\\n\\n# Sample Output\\n- - ipv4Address: 10.244.2.14\\n```\\n\\n## Conclusion\\n\\nIn the above example we set up and AKS cluster, enabled with Azure Workload Identity and installed external-dns to automatically managed private DNS records in and Azure Private Zone. We demonstrated this for both Services and Pods. There are other options for managing the rights of the external-dns user, but I believe that Workload Identity gives you the best overall granularity of control."},{"id":"/2023/11/27/capabilities-in-aks","metadata":{"permalink":"/2023/11/27/capabilities-in-aks","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2023-11-27/capabilities-in-aks/index.md","source":"@site/blog/2023-11-27/capabilities-in-aks/index.md","title":"Using Linux Capabilities in AKS","description":"An exploration on how to use Linux capabilities(7) in AKS.","date":"2023-11-27T00:00:00.000Z","tags":[],"readingTime":2.22,"hasTruncateMarker":true,"authors":[{"name":"Diego Casati","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/dcasati","socials":{"x":"https://x.com/ve6dpc","github":"https://github.com/dcasati"},"imageURL":"https://github.com/dcasati.png","key":"diego_casati","page":null}],"frontMatter":{"authors":["diego_casati"],"date":"2023-11-27","description":"An exploration on how to use Linux capabilities(7) in AKS.","tags":[],"title":"Using Linux Capabilities in AKS"},"unlisted":false,"prevItem":{"title":"Using External DNS in AKS with Azure Workload Identity","permalink":"/2023/12/18/external-dns-workload-identity"},"nextItem":{"title":"Image Verification Part 1 - Notation CLI","permalink":"/2023/11/09/part1-notation-usage"}},"content":"## Introduction\\n\\nRight after Kubernetes 1.21, the way [Capabilities(7)](https://man7.org/linux/man-pages/man7/capabilities.7.html) worked in Kubernetes changed. At that version, a change in the upstream code enforced that Capabilities will only work when `runAsUser` is set to `0` - meaning `root`. This is somewhat counterintuitive to what most of us would expect but code goes into the reasoning and how to work with Capabilities after 1.21.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe [code](https://github.com/containerd/containerd/blob/main/pkg/cri/server/container_create_linux.go#L260-L267) that prevents any user other than `root` to have capabilities. This was added by the commit referenced [here](https://github.com/containerd/containerd/commit/50c73e6dc550c2cdb579e303ac26394497f9f331):\\n\\n```golang\\n\\t// Clear all ambient capabilities. The implication of non-root + caps\\n\\t// is not clearly defined in Kubernetes.\\n\\t// See https://github.com/kubernetes/kubernetes/issues/56374\\n\\t// Keep docker\'s behavior for now.\\n\\tspecOpts = append(specOpts,\\n\\t\\tcustomopts.WithoutAmbientCaps,\\n\\t\\tcustomopts.WithSelinuxLabels(processLabel, mountLabel),\\n\\t)\\n```\\n\\nOn the previous note, we can add/remove capabilities to `root` - which essentially removes a lot of the superpowers that `root` have on by default (e.g.: cap_net_admin).\\n\\n## Approaches\\n\\n1. Using `RunAsUser: 0` but restricting the capabilities in the account\\n\\n    In the example below we will be granting `cap_ipc_lock` to the running user (root) and nothing else.\\n\\n    ```yaml\\n    apiVersion: apps/v1\\n    kind: Deployment\\n    metadata:\\n    labels:\\n        app: gbbapp\\n    name: gbbapp\\n    namespace: ns-gbb\\n    spec:\\n    replicas: 1\\n    selector:\\n        matchLabels:\\n        app: gbbapp\\n    template:\\n        metadata:\\n        labels:\\n        app: gbbapp\\n        spec:\\n        containers:\\n        - name: gbbapp\\n            image: gbbapp/k8s:cfa\\n            command: [\\"/bin/bash\\"]\\n            args: [\\"-c\\", \\"sleep 3600\\"]\\n            securityContext:\\n            runAsUser: 0\\n            capabilities:\\n                drop: [\\"ALL\\"]\\n                add: [\\"IPC_LOCK\\"]\\n    ```\\n\\n    2. Adding capabilities to binaries during the build process\\n\\n    It is possible to add capabilities during the build process with docker/podman. With this approach you can remove the `RunAsUser` parameter altogether. The capabilities added there will persist when the image runs as a container. The following example adds `cap_ipc_lock` to python3.8\\n\\n    1. Create a Dockerfile\\n        ```Dockerfile\\n        FROM ubuntu\\n        \\n        RUN apt-get update && apt-get install -y libcap2-bin && \\\\\\n            setcap cap_ipc_lock+eip /usr/bin/python3.8\\n        \\n        CMD [\\"/bin/bash\\"]\\n        ```\\n\\n    1. Create an ACR instance\\n        \\n        ```bash\\n        RESOURCE_GROUP_NAME=rg-setcomp\\n        LOCATION=westus\\n        ACR_NAME=myacrname\\n        \\n        az group create --name ${RESOURCE_GROUP_NAME} --location ${LOCATION}\\n        az acr create -n ${ACR_NAME} -l ${LOCATION}\\n        ```\\n\\n    1. Add the container to ACR\\n        ```bash\\n        az acr build -r ${MY_ACR}/setcomp:{{.Run.ID}} .\\n        ```\\n\\n    1. Exec into the container and verify that the capability was added by running the `getcap` command against a binary, which in this case is `python3.8`:\\n        \\n        ```bash\\n        $ getcap /usr/bin/python3.8\\n        /usr/bin/python3.8 = cap_ipc_lock+eip\\n        ```\\n\\n## Conclusion\\n\\nIn this article we\'ve explored how to enable Capabilities for a container and how to limit it\'s scope. You now have the pieces and bits needed to enable the minimum amount of capabilities for any given container."},{"id":"/2023/11/09/part1-notation-usage","metadata":{"permalink":"/2023/11/09/part1-notation-usage","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2023-11-09/part1-notation-usage/index.md","source":"@site/blog/2023-11-09/part1-notation-usage/index.md","title":"Image Verification Part 1 - Notation CLI","description":"How to use the Notary project\'s \'notation\' cli tool to sign and verify container images","date":"2023-11-09T00:00:00.000Z","tags":[],"readingTime":6.25,"hasTruncateMarker":true,"authors":[{"name":"Steve Griffith","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/swgriffith","socials":{"x":"https://x.com/SteveGriffith","github":"https://github.com/swgriffith"},"imageURL":"https://github.com/swgriffith.png","key":"steve_griffith","page":null}],"frontMatter":{"authors":["steve_griffith"],"date":"2023-11-09","description":"How to use the Notary project\'s \'notation\' cli tool to sign and verify container images","tags":[],"title":"Image Verification Part 1 - Notation CLI"},"unlisted":false,"prevItem":{"title":"Using Linux Capabilities in AKS","permalink":"/2023/11/27/capabilities-in-aks"},"nextItem":{"title":"Image Verification Part 2 - Image Verification with Gatekeeper and Ratify","permalink":"/2023/11/09/part2-aks-image-verification"}},"content":"## Introduction\\n\\nThis is part one of a two part post on container image signing and runtime verification. In this post, we\'ll walk through the [notation](https://github.com/notaryproject/notation) project and its ability to sign container images, using the [Notary](https://github.com/notaryproject/specifications) project specification. In the next post, we\'ll walk through setting up [gatekeeper](https://open-policy-agent.github.io/gatekeeper/website/) and [ratify](https://github.com/deislabs/ratify/blob/main/README.md) to perform policy based runtime verification of images.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Installation\\n\\nFor this walk through you\'ll need to have Docker running locally, and have the notation cli installed. You can find the install steps for each in the list below:\\n\\n* [Docker](https://www.docker.com/get-started/)\\n* [Notation CLI](https://notaryproject.dev/docs/user-guides/installation/cli/)\\n\\n## Azure Container Registry\\n\\nYou can really use any OCI compatible registry, but since we\'ll later be using AKS for verification, we\'ll create an Azure Container Registery for use in the following steps.\\n\\n```bash\\n# Setup Environment Variables\\nRG=EphNotationTesting\\nLOC=eastus\\nACR_NAME=mynotationlab\\n\\n# Create the resource group\\naz group create -n $RG -l $LOC\\n\\n# Create the Azure Container Registry\\naz acr create -g $RG -n $ACR_NAME --sku Standard\\n\\n# Login to the ACR\\naz acr login -n $ACR_NAME\\n```\\n\\nWe could build our own container image from scratch, but the image signing process doesnt really change, so let\'s just import an existing image to use. This is a good demonstration of the best practice of never directly pulling public images. Instead of direclty pulling public images into your cluster, you should import and verify any public images you wish to use. We\'ll take it a step further by also signing the imported image.\\n\\n```bash\\n# For later testing, lets also import an unsigned image\\naz acr import --name $ACR_NAME --source docker.io/library/nginx:1.25.3 --image nginx:1.25.3\\n```\\n\\nNow we can play around with notation. Notation does require you use the image SHA instead of tags, so we\'ll get that first.\\n\\n```bash\\n# Get the nginx image SHA\\nIMAGE_SHA=$(az acr repository show -n $ACR_NAME --image \\"nginx:1.25.3\\" -o tsv --query digest)\\n```\\n\\nNow we\'ll use the notation cli to check the image for existing signatures and then create and apply a new signature. The notation project does provide the ability to generate a test certificate locally to get you up and running quickly. We\'ll try that first and then move on to using Azure Key Vault for the certificate.\\n\\n```bash\\n# List the signatures on the image\\n# You should see the image has no signatures\\nnotation ls $ACR_NAME.azurecr.io/nginx@$IMAGE_SHA\\n\\n# Generate a test RSA key and self-signed certificat\\nnotation cert generate-test --default \\"brooklyn.io\\"\\n\\n# List certs to confirm the cert exists\\nnotation cert ls\\n\\n# Sign the image\\nnotation sign $ACR_NAME.azurecr.io/nginx@$IMAGE_SHA\\n\\n# Now check the image signatures\\n# You should now see that the image is signed\\nnotation ls $ACR_NAME.azurecr.io/nginx@$IMAGE_SHA\\n```\\n\\nYou should now have a successfully signed image. Next, we can use notation to verify the image has been signed. For that we\'ll need to create a local trust policy for notation.\\n\\n```bash\\n# Create a trust policy for notation\\ncat <<EOF > ./trustpolicy.json\\n{\\n    \\"version\\": \\"1.0\\",\\n    \\"trustPolicies\\": [\\n        {\\n            \\"name\\": \\"brooklyn-images\\",\\n            \\"registryScopes\\": [ \\"*\\" ],\\n            \\"signatureVerification\\": {\\n                \\"level\\" : \\"strict\\"\\n            },\\n            \\"trustStores\\": [ \\"ca:brooklyn.io\\" ],\\n            \\"trustedIdentities\\": [\\n                \\"*\\"\\n            ]\\n        }\\n    ]\\n}\\nEOF\\n\\n# Import the policy\\nnotation policy import ./trustpolicy.json\\n\\n# Show the policy\\nnotation policy show\\n\\n# Verify the image meets the policy\\n# You should get a message that the signature was verified\\nnotation verify $ACR_NAME.azurecr.io/nginx@$IMAGE_SHA\\n```\\n\\n## Cleanup\\n\\nBefore moving on, we should delete and re-import the image, since we dont want the local test certificate on the image any more. You can also remove the signature with [oras](https://notaryproject.dev/docs/user-guides/how-to/manage-signatures/#delete-a-signature-on-an-artifact), but we\'ll keep it simple for now.\\n\\n```bash\\n# Delete and re-import the image\\naz acr repository delete -n $ACR_NAME --repository nginx -y\\naz acr import --name $ACR_NAME --source docker.io/library/nginx:1.25.3 --image nginx:1.25.3\\nIMAGE_SHA=$(az acr repository show -n $ACR_NAME --image \\"nginx:1.25.3\\" -o tsv --query digest)\\n```\\n\\n## Sign Images with a Cert from Azure Key Vault\\n\\nUsing the notation locally generated test certificate does give you some level of comfort about the source of the image, but you really want to use a certificate from a trusted store. It\'s also important to have an accessible store that can be used by remote processes, like your devops pipeline. Fortunately, notation provides a plugin for Azure Key Vault which can be used to sign images with certifiates source from Azure Key Vault.\\n\\nFirst you need to install the Azure Key Vault plug-in for notation. You can find instructions for your OS [here](https://github.com/Azure/notation-azure-kv#installation-the-akv-plugin).\\n\\n```bash\\n# Confirm you successfuly installed the plugin\\nnotation plugin list\\n\\n# Sample Output\\nNAME       DESCRIPTION                       VERSION   CAPABILITIES                ERROR\\nazure-kv   Notation Azure Key Vault plugin   1.0.1     [SIGNATURE_GENERATOR.RAW]   <nil>\\n```\\n\\nNow we\'ll create an Azure Key Vault instance for our test and configure the signature. It\'s important that the secret properties specify that the content type  is \'application/x-pem-file\' as ratify, which we\'ll use later, cannot read the default PFX format.\\n\\n```bash\\nAKV_NAME=mynotationtest\\n\\n# Create the key vault\\naz keyvault create --name $AKV_NAME --resource-group $RG --enable-rbac-authorization false\\n\\n# Set some variables for the cert creation\\n# Name of the certificate created in AKV\\nCERT_NAME=brooklyn-io\\nCERT_SUBJECT=\\"CN=brooklyn.io,O=Notation,L=Brooklyn,ST=NY,C=US\\"\\nCERT_PATH=./${CERT_NAME}.pem\\n\\n# Set the access policy for yourself to create and get certs\\nUSER_ID=$(az ad signed-in-user show --query id -o tsv)\\naz keyvault set-policy -n $AKV_NAME --certificate-permissions create get --key-permissions sign --object-id $USER_ID\\n\\n# Create the Key Vault certificate policy file\\ncat <<EOF > ./brooklyn_io_policy.json\\n{\\n    \\"issuerParameters\\": {\\n    \\"certificateTransparency\\": null,\\n    \\"name\\": \\"Self\\"\\n    },\\n    \\"keyProperties\\": {\\n      \\"exportable\\": false,\\n      \\"keySize\\": 2048,\\n      \\"keyType\\": \\"RSA\\",\\n      \\"reuseKey\\": true\\n    },\\n    \\"secretProperties\\": {\\n        \\"contentType\\": \\"application/x-pem-file\\"\\n    },\\n    \\"x509CertificateProperties\\": {\\n    \\"ekus\\": [\\n        \\"1.3.6.1.5.5.7.3.3\\"\\n    ],\\n    \\"keyUsage\\": [\\n        \\"digitalSignature\\"\\n    ],\\n    \\"subject\\": \\"${CERT_SUBJECT}\\",\\n    \\"validityInMonths\\": 12\\n    }\\n}\\nEOF\\n\\n# Create the signing certificate\\naz keyvault certificate create -n $CERT_NAME --vault-name $AKV_NAME -p @brooklyn_io_policy.json\\n\\n# Get the Key ID of the signing key\\nKEY_ID=$(az keyvault certificate show -n $CERT_NAME --vault-name $AKV_NAME --query \'kid\' -o tsv)\\n\\n# Now sign the previosly imported nginx image\\n# You should get a confirmation that the image was successfully signed\\nnotation sign --signature-format cose --id $KEY_ID --plugin azure-kv --plugin-config self_signed=true $ACR_NAME.azurecr.io/nginx@$IMAGE_SHA\\n\\n# Confirm the signature\\nnotation ls $ACR_NAME.azurecr.io/nginx@$IMAGE_SHA\\n```\\n\\nFinally, now that we have the image signed with our certifcate from Azure Key Vault, lets set up our local environment trust policy to allow us to verify the image signature is valid locally. We\'ll need to download the certificate and add a local trust store.\\n\\n```bash\\n# Download the cert from Azure Key Vault so we can verify the image locally with the AKV cert\\naz keyvault certificate download --name $CERT_NAME --vault-name $AKV_NAME --file $CERT_PATH\\n\\nSTORE_TYPE=\\"ca\\"\\nSTORE_NAME=\\"brooklyn.io\\"\\nnotation cert add --type $STORE_TYPE --store $STORE_NAME $CERT_PATH\\n\\ncat <<EOF > ./trustpolicy.json\\n{\\n    \\"version\\": \\"1.0\\",\\n    \\"trustPolicies\\": [\\n        {\\n            \\"name\\": \\"brooklyn-images\\",\\n            \\"registryScopes\\": [ \\"$ACR_NAME.azurecr.io/nginx\\" ],\\n            \\"signatureVerification\\": {\\n                \\"level\\" : \\"strict\\" \\n            },\\n            \\"trustStores\\": [ \\"$STORE_TYPE:$STORE_NAME\\" ],\\n            \\"trustedIdentities\\": [\\n                \\"x509.subject: $CERT_SUBJECT\\"\\n            ]\\n        }\\n    ]\\n}\\nEOF\\n\\n# Import the policy and show it\\nnotation policy import ./trustpolicy.json\\nnotation policy show\\n\\n# Test image verification\\nnotation verify $ACR_NAME.azurecr.io/nginx@$IMAGE_SHA\\n\\n# Sample success message!!!\\nSuccessfully verified signature for mynotationlab.azurecr.io/nginx@sha256:86e53c4c16a6a276b204b0fd3a8143d86547c967dc8258b3d47c3a21bb68d3c6\\n```\\n\\n## Conclusion\\n\\nYou should now have some familiarity with the notation cli tool and how to use it to sign container images in a container registry with both self-signed certificates and certificates from Azure Key Vault. The next step would be to enable verification in kubernetes.\\n\\n**Next:** [Part 2 - Image Verification in AKS](./part2-aks-image-verification.html)"},{"id":"/2023/11/09/part2-aks-image-verification","metadata":{"permalink":"/2023/11/09/part2-aks-image-verification","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2023-11-09/part2-aks-image-verification/index.md","source":"@site/blog/2023-11-09/part2-aks-image-verification/index.md","title":"Image Verification Part 2 - Image Verification with Gatekeeper and Ratify","description":"How to manually setup Gatekeeper and Ratify on an AKS cluster to enable runtime image signature verification.","date":"2023-11-09T00:00:00.000Z","tags":[],"readingTime":4.04,"hasTruncateMarker":true,"authors":[{"name":"Steve Griffith","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/swgriffith","socials":{"x":"https://x.com/SteveGriffith","github":"https://github.com/swgriffith"},"imageURL":"https://github.com/swgriffith.png","key":"steve_griffith","page":null}],"frontMatter":{"authors":["steve_griffith"],"date":"2023-11-09","description":"How to manually setup Gatekeeper and Ratify on an AKS cluster to enable runtime image signature verification.","tags":[],"title":"Image Verification Part 2 - Image Verification with Gatekeeper and Ratify"},"unlisted":false,"prevItem":{"title":"Image Verification Part 1 - Notation CLI","permalink":"/2023/11/09/part1-notation-usage"},"nextItem":{"title":"AKS Custom Policy","permalink":"/2023/10/26/aks-custom-policy"}},"content":"## Introduction\\n\\n>**NOTE:** The manifest files used in this post can be found [here](https://github.com/swgriffith/azure-guides/tree/master/image-signing)\\n\\nIn the prior post, we ran through using the notation cli tool to sign images in Azure Container Registry. If you havent gone through that post, I recommend you start there at [Part 1 - Image Signing with Notation](./part1-notation-usage.html)\\n\\n\x3c!-- truncate --\x3e\\n\\nIn this post, we\'ll walk through the steps to manually configure AKS with Gatekeeper and the Ratify project to enforce an image signature verification policy.\\n\\n## Cluster Creation and Setup\\n\\nFor Ratify to work with Gatekeeper, we\'ll need a cluster with both the OIDC Issuer and Workload Idenitty add-ons enabled. In the last step below, we\'ll grab the OIDC Issuer URL for the cluster, which is used when you want to federate a Kubernetes Service Account with an Azure Active Directory Identity.\\n\\n```bash\\n# Set environment variables\\nRG=EphNotationTesting\\nLOC=eastus\\nACR_NAME=mynotationlab\\nCLUSTER_NAME=imagesigninglab\\n\\n# Create the AKS Cluster\\naz aks create -g $RG -n $CLUSTER_NAME \\\\\\n--attach-acr $ACR_NAME \\\\\\n--enable-oidc-issuer \\\\\\n--enable-workload-identity\\n\\n# Get the cluster credentials\\naz aks get-credentials -g $RG -n $CLUSTER_NAME\\n\\n# Get the OIDC Issuer URL\\nexport AKS_OIDC_ISSUER=\\"$(az aks show -n ${CLUSTER_NAME} -g ${RG} --query \\"oidcIssuerProfile.issuerUrl\\" -otsv)\\"\\n```\\n\\n## Managed Identity Setup\\n\\nRatify will need to be able to read from the Key Vault, so we\'ll need to create a managed identity and grant it the proper rights on Azure Container Registry. The only right needed is \'acrpull\', as ratify will pull the image to check it\'s signature.\\n\\n```bash\\nSUBSCRIPTION=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx\\nTENANT_ID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx\\nIDENTITY_NAME=ratify-identity\\nRATIFY_NAMESPACE=gatekeeper-system\\n\\n# Create the ratify identity\\naz identity create --name \\"${IDENTITY_NAME}\\" --resource-group \\"${RG}\\" --location \\"${LOC}\\" --subscription \\"${SUBSCRIPTION}\\"\\n\\n# Get the identity IDs\\nexport IDENTITY_OBJECT_ID=\\"$(az identity show --name \\"${IDENTITY_NAME}\\" --resource-group \\"${RG}\\" --query \'principalId\' -otsv)\\"\\nexport IDENTITY_CLIENT_ID=$(az identity show --name ${IDENTITY_NAME} --resource-group ${RG} --query \'clientId\' -o tsv)\\n\\n# Grant the ratify identity acr pull rights\\naz role assignment create \\\\\\n--assignee-object-id ${IDENTITY_OBJECT_ID} \\\\\\n--role acrpull \\\\\\n--scope subscriptions/${SUBSCRIPTION}/resourceGroups/${RG}/providers/Microsoft.ContainerRegistry/registries/${ACR_NAME}\\n\\n# Federate the managed identity to the service account used by ratify\\naz identity federated-credential create \\\\\\n--name ratify-federated-credential \\\\\\n--identity-name \\"${IDENTITY_NAME}\\" \\\\\\n--resource-group \\"${RG}\\" \\\\\\n--issuer \\"${AKS_OIDC_ISSUER}\\" \\\\\\n--subject system:serviceaccount:\\"${RATIFY_NAMESPACE}\\":\\"ratify-admin\\"\\n```\\n\\nSince Ratify will be checking the signature, it also needs the ability to get the secret from Azure Key Vault.\\n\\n```bash\\n# Grant the ratify identity rights\\naz keyvault set-policy --name ${AKV_NAME} \\\\\\n--secret-permissions get \\\\\\n--object-id ${IDENTITY_OBJECT_ID}\\n```\\n\\n## Install Gatekeeper and Ratify\\n\\nWhile AKS does, now in preview, have a [managed add-on for gatekeeper and ratify](https://learn.microsoft.com/en-us/azure/aks/image-integrity?tabs=azure-cli), it\'s still in preview. For this post, we\'ll manually install both Gatekeeper and Ratify so that we can see all the moving parts and more easily debug any issues. \\n\\n```bash\\n# Install Gatekeeper\\nhelm repo add gatekeeper https://open-policy-agent.github.io/gatekeeper/charts\\n\\nhelm install gatekeeper/gatekeeper  \\\\\\n    --name-template=gatekeeper \\\\\\n    --namespace ${RATIFY_NAMESPACE} --create-namespace \\\\\\n    --set enableExternalData=true \\\\\\n    --set validatingWebhookTimeoutSeconds=5 \\\\\\n    --set mutatingWebhookTimeoutSeconds=2\\n\\n# Get the key vault URI which ratify will need\\nexport VAULT_URI=$(az keyvault show --name ${AKV_NAME} --resource-group ${RG} --query \\"properties.vaultUri\\" -o tsv)\\n\\n# Install Ratify\\nhelm repo add ratify https://ratify-project.github.io/ratify\\n\\nhelm install ratify \\\\\\n    ratify/ratify --atomic \\\\\\n    --namespace ${RATIFY_NAMESPACE} --create-namespace \\\\\\n    --set featureFlags.RATIFY_CERT_ROTATION=true \\\\\\n    --set akvCertConfig.enabled=true \\\\\\n    --set akvCertConfig.vaultURI=${VAULT_URI} \\\\\\n    --set akvCertConfig.cert1Name=${CERT_NAME} \\\\\\n    --set akvCertConfig.tenantId=${TENANT_ID} \\\\\\n    --set oras.authProviders.azureWorkloadIdentityEnabled=true \\\\\\n    --set azureWorkloadIdentity.clientId=${IDENTITY_CLIENT_ID}\\n```\\n\\nNow that gatekeeper and ratify are running, lets apply a new constraint and policy template for the image verification policy. You should inspect the two files in the commands below for your own knowledge of how they work.\\n\\n```bash\\n# Create the gatekeeper policy template\\nkubectl apply -f https://raw.githubusercontent.com/swgriffith/azure-guides/master/image-signing/ratify-policy-template.yaml\\n\\n# Apply the policy with a gatekeeper constraint\\nkubectl apply -f https://raw.githubusercontent.com/swgriffith/azure-guides/master/image-signing/ratify-policy-constraint.yaml\\n```\\n\\n## Test the policy!\\n\\nOur setup is complete. We can now try to create a pod using an unsigned and signed container image.\\n\\n```bash\\n# First try to use the docker hub nginx image, which is unsigned\\n# This should fail\\nkubectl run demo --namespace default --image=nginx:latest\\n\\n# Sample Error Message\\nError from server (Forbidden): admission webhook \\"validation.gatekeeper.sh\\" denied the request: [ratify-constraint] Subject failed verification: docker.io/library/nginx@sha256:86e53c4c16a6a276b204b0fd3a8143d86547c967dc8258b3d47c3a21bb68d3c6\\n\\n# Now try using our container image\\n# This pod should be successfully created!\\nkubectl run demo --namespace default --image=$ACR_NAME.azurecr.io/nginx@$IMAGE_SHA\\n```\\n\\n## Conclusion\\n\\nBetween this post and [Part 1](./part1-notation-usage.html), we learned about the notation cli tool, which can be used to sign container images via the notary specification. We signed images with both a local test certificate, as well as a certificate managed by Azure Key Vault. Finally, we enabled Gatekeeper and Ratify on an AKS cluster to provide an image signature verification policy."},{"id":"/2023/10/26/aks-custom-policy","metadata":{"permalink":"/2023/10/26/aks-custom-policy","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2023-10-26/aks-custom-policy/index.md","source":"@site/blog/2023-10-26/aks-custom-policy/index.md","title":"AKS Custom Policy","description":"Creating an AKS custom Gatekeeper policy to restrict ingress host names","date":"2023-10-26T00:00:00.000Z","tags":[],"readingTime":4.68,"hasTruncateMarker":true,"authors":[{"name":"Steve Griffith","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/swgriffith","socials":{"x":"https://x.com/SteveGriffith","github":"https://github.com/swgriffith"},"imageURL":"https://github.com/swgriffith.png","key":"steve_griffith","page":null}],"frontMatter":{"authors":["steve_griffith"],"date":"2023-10-26","description":"Creating an AKS custom Gatekeeper policy to restrict ingress host names","tags":[],"title":"AKS Custom Policy"},"unlisted":false,"prevItem":{"title":"Image Verification Part 2 - Image Verification with Gatekeeper and Ratify","permalink":"/2023/11/09/part2-aks-image-verification"},"nextItem":{"title":"AKS User Minimum Roles","permalink":"/2023/10/12/aks-user-min-roles"}},"content":"## Introduction\\n\\nIn this walkthrough we\'ll use the [Gatekeeper](https://open-policy-agent.github.io/gatekeeper/website/) project and [AKS Policy](https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for-kubernetes) to create a policy that resticts the host name on a Kubernetes Ingress. The host names used for validation will be provided via parameters on the Azure Policy assignment.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe YAML manifest files for this walk through are located [here](https://github.com/swgriffith/azure-guides/tree/master/aks-custom-policy)\\n\\n## Cluster Setup\\n\\nFirst we\'ll create an AKS cluster with Azure Policy for AKS enabled.\\n\\n```bash\\n# Define variables\\nRG=PolicyTestRG\\nLOC=eastus\\nCLUSTER_NAME=policytest\\n\\n# Create the resource group\\naz group create -n $RG -l $LOC\\n\\n# Create the AKS cluster with Azure Policy enabled\\naz aks create -g $RG -n $CLUSTER_NAME --enable-addons azure-policy\\n\\n# Get the cluster credentials\\naz aks get-credentials -g $RG -n $CLUSTER_NAME\\n```\\n\\n## Constraint Template Creation\\n\\nWhile the cluster creates we can work on the contraint template. You can read more about constraint templates here, but they are the CRD that provides Gatekeeper the policy details, including the evaluation which is written in [rego](https://www.openpolicyagent.org/docs/latest/policy-language/).\\n\\nIn our example, we\'ll keep it simple and just restrict the ingress host name to one of the values in an \'allowedhosts\' array parameter. The allowedhosts parameter will be set via the constraint definition, or via the policy definition when we create the Azure Policy.\\n\\nCreate a new file named IngressHostnameConstraintTemplate.yaml with the following contents. Or use the doc in [manifests/IngressHostnameConstraintTemplate.yaml](./manifests/IngressHostnameConstraintTemplate.yaml)\\n\\n```yaml\\napiVersion: templates.gatekeeper.sh/v1beta1\\nkind: ConstraintTemplate\\nmetadata:\\n  name: k8srestrictingresshostname\\n  annotations:\\n    description: Restricts hostname for ingress\\nspec:\\n  crd:\\n    spec:\\n      names:\\n        kind: K8sRestrictIngressHostname # this must be the same name as the name on metadata.name (line 4)            \\n      validation:\\n        # Schema for the `parameters` field\\n        openAPIV3Schema:\\n          properties:\\n            allowedhosts:\\n              type: array\\n              items:\\n                type: string        \\n  targets:\\n    - target: admission.k8s.gatekeeper.sh\\n      rego: |\\n        package k8srestrictingresshostname\\n\\n        violation[{\\"msg\\": msg}] {\\n          host := input.review.object.spec.rules[_].host\\n          not input_allowed_hosts(host)\\n          msg := sprintf(\\"invalid ingress host %q\\", [host])\\n        }\\n\\n        input_allowed_hosts(field) {\\n          endswith(field, input.parameters.allowedhosts[_])\\n        }\\n```\\n\\n## Azure Policy Creation\\n\\nTo create a policy in Azure that uses the above constraint template, we need to base64 encode the above file and pass that into a new policy definition.\\n\\n```bash\\ncat ./manifests/IngressHostnameConstraintTemplate.yaml|base64\\n\\n# Sample Output\\nYXBpVmVyc2lvbjogdGVtcGxhdGVzLmdhdGVrZWVwZXIuc2gvdjFiZXRhMQpraW5kOiBDb25zdHJhaW50VGVtcGxhdGUKbWV0YWRhdGE6CiAgbmFtZTogazhzcmVzdHJpY3RpbmdyZXNzaG9zdG5hbWUKICBhbm5vdGF0aW9uczoKICAgIGRlc2NyaXB0aW9uOiBSZXN0cmljdHMgaG9zdG5hbWUgZm9yIGluZ3Jlc3MKc3BlYzoKICBjcmQ6CiAgICBzcGVjOgogICAgICBuYW1lczoKICAgICAgICBraW5kOiBLOHNSZXN0cmljdEluZ3Jlc3NIb3N0bmFtZSAjIHRoaXMgbXVzdCBiZSB0aGUgc2FtZSBuYW1lIGFzIHRoZSBuYW1lIG9uIG1ldGFkYXRhLm5hbWUgKGxpbmUgNCkgICAgICAgICAgICAKICAgICAgdmFsaWRhdGlvbjoKICAgICAgICAjIFNjaGVtYSBmb3IgdGhlIGBwYXJhbWV0ZXJzYCBmaWVsZAogICAgICAgIG9wZW5BUElWM1NjaGVtYToKICAgICAgICAgIHByb3BlcnRpZXM6CiAgICAgICAgICAgIGFsbG93ZWRob3N0czoKICAgICAgICAgICAgICB0eXBlOiBhcnJheQogICAgICAgICAgICAgIGl0ZW1zOgogICAgICAgICAgICAgICAgdHlwZTogc3RyaW5nICAgICAgICAKICB0YXJnZXRzOgogICAgLSB0YXJnZXQ6IGFkbWlzc2lvbi5rOHMuZ2F0ZWtlZXBlci5zaAogICAgICByZWdvOiB8CiAgICAgICAgcGFja2FnZSBrOHNyZXN0cmljdGluZ3Jlc3Nob3N0bmFtZQoKICAgICAgICB2aW9sYXRpb25beyJtc2ciOiBtc2d9XSB7CiAgICAgICAgICBob3N0IDo9IGlucHV0LnJldmlldy5vYmplY3Quc3BlYy5ydWxlc1tfXS5ob3N0CiAgICAgICAgICBub3QgaW5wdXRfYWxsb3dlZF9ob3N0cyhob3N0KQogICAgICAgICAgbXNnIDo9IHNwcmludGYoImludmFsaWQgaW5ncmVzcyBob3N0ICVxIiwgW2hvc3RdKQogICAgICAgIH0KCiAgICAgICAgaW5wdXRfYWxsb3dlZF9ob3N0cyhmaWVsZCkgewogICAgICAgICAgZW5kc3dpdGgoZmllbGQsIGlucHV0LnBhcmFtZXRlcnMuYWxsb3dlZGhvc3RzW19dKQogICAgICAgIH0=\\n```\\n\\nCreate a new file called \'policy-definition.json\', and paste the following contenxt, updating the base64 encoded value with your output from above.\\n\\n```json\\n{\\n    \\"properties\\": {\\n        \\"displayName\\": \\"Restrict Ingress Hostname\\",\\n        \\"policyType\\": \\"Custom\\",\\n        \\"mode\\": \\"Microsoft.Kubernetes.Data\\",\\n        \\"description\\": \\"Restrict the host names used in ingress objects\\",\\n        \\"metadata\\": {\\n            \\"version\\": \\"1.0.0\\",\\n            \\"category\\": \\"Kubernetes\\"\\n        },\\n        \\"parameters\\": {\\n            \\"effect\\": {\\n                \\"type\\": \\"String\\",\\n                \\"metadata\\": {\\n                    \\"displayName\\": \\"Effect\\",\\n                    \\"description\\": \\"Enable or disable the execution of the policy\\"\\n                },\\n                \\"allowedValues\\": [\\n                    \\"audit\\",\\n                    \\"deny\\",\\n                    \\"disabled\\"\\n                ],\\n                \\"defaultValue\\": \\"audit\\"\\n            },\\n            \\"excludedNamespaces\\": {\\n                \\"type\\": \\"Array\\",\\n                \\"metadata\\": {\\n                    \\"displayName\\": \\"Namespace exclusions\\",\\n                    \\"description\\": \\"List of Kubernetes namespaces to exclude from policy evaluation. Providing a value for this parameter is optional.\\"\\n                },\\n                \\"defaultValue\\": [\\n                    \\"kube-system\\",\\n                    \\"gatekeeper-system\\",\\n                    \\"azure-arc\\"\\n                ]\\n            },\\n            \\"allowedhosts\\": {\\n                \\"type\\": \\"Array\\",\\n                \\"metadata\\": {\\n                  \\"displayName\\": \\"Allowed Ingress Host Names\\",\\n                  \\"description\\": \\"List of allowed host names that can be used on Ingress objects.\\"\\n                },\\n                \\"defaultValue\\": []\\n              }\\n        },\\n        \\"policyRule\\": {\\n            \\"if\\": {\\n                \\"field\\": \\"type\\",\\n                \\"in\\": [\\n                    \\"AKS Engine\\",\\n                    \\"Microsoft.Kubernetes/connectedClusters\\",\\n                    \\"Microsoft.ContainerService/managedClusters\\"\\n                ]\\n            },\\n            \\"then\\": {\\n                \\"effect\\": \\"[parameters(\'effect\')]\\",\\n                \\"details\\": {\\n                    \\"templateInfo\\": {\\n                        \\"sourceType\\": \\"Base64Encoded\\",\\n                        \\"content\\": \\"YXBpVmVyc2lvbjogdGVtcGxhdGVzLmdhdGVrZWVwZXIuc2gvdjFiZXRhMQpraW5kOiBDb25zdHJhaW50VGVtcGxhdGUKbWV0YWRhdGE6CiAgbmFtZTogazhzcmVzdHJpY3RpbmdyZXNzaG9zdG5hbWUKICBhbm5vdGF0aW9uczoKICAgIGRlc2NyaXB0aW9uOiBSZXN0cmljdHMgaG9zdG5hbWUgZm9yIGluZ3Jlc3MKc3BlYzoKICBjcmQ6CiAgICBzcGVjOgogICAgICBuYW1lczoKICAgICAgICBraW5kOiBLOHNSZXN0cmljdEluZ3Jlc3NIb3N0bmFtZSAjIHRoaXMgbXVzdCBiZSB0aGUgc2FtZSBuYW1lIGFzIHRoZSBuYW1lIG9uIG1ldGFkYXRhLm5hbWUgKGxpbmUgNCkgICAgICAgICAgICAKICAgICAgdmFsaWRhdGlvbjoKICAgICAgICAjIFNjaGVtYSBmb3IgdGhlIGBwYXJhbWV0ZXJzYCBmaWVsZAogICAgICAgIG9wZW5BUElWM1NjaGVtYToKICAgICAgICAgIHByb3BlcnRpZXM6CiAgICAgICAgICAgIGFsbG93ZWRob3N0czoKICAgICAgICAgICAgICB0eXBlOiBhcnJheQogICAgICAgICAgICAgIGl0ZW1zOgogICAgICAgICAgICAgICAgdHlwZTogc3RyaW5nICAgICAgICAKICB0YXJnZXRzOgogICAgLSB0YXJnZXQ6IGFkbWlzc2lvbi5rOHMuZ2F0ZWtlZXBlci5zaAogICAgICByZWdvOiB8CiAgICAgICAgcGFja2FnZSBrOHNyZXN0cmljdGluZ3Jlc3Nob3N0bmFtZQoKICAgICAgICB2aW9sYXRpb25beyJtc2ciOiBtc2d9XSB7CiAgICAgICAgICBob3N0IDo9IGlucHV0LnJldmlldy5vYmplY3Quc3BlYy5ydWxlc1tfXS5ob3N0CiAgICAgICAgICBub3QgaW5wdXRfYWxsb3dlZF9ob3N0cyhob3N0KQogICAgICAgICAgbXNnIDo9IHNwcmludGYoImludmFsaWQgaW5ncmVzcyBob3N0ICVxIiwgW2hvc3RdKQogICAgICAgIH0KCiAgICAgICAgaW5wdXRfYWxsb3dlZF9ob3N0cyhmaWVsZCkgewogICAgICAgICAgZW5kc3dpdGgoZmllbGQsIGlucHV0LnBhcmFtZXRlcnMuYWxsb3dlZGhvc3RzW19dKQogICAgICAgIH0=\\"\\n                    },\\n                    \\"excludedNamespaces\\": \\"[parameters(\'excludedNamespaces\')]\\",\\n                    \\"values\\": {\\n                        \\"allowedhosts\\": \\"[parameters(\'allowedhosts\')]\\"\\n                    },\\n                    \\"apiGroups\\": [\\n                        \\"extensions\\", \\n                        \\"networking.k8s.io\\"\\n                    ],\\n                    \\"kinds\\": [\\n                        \\"Ingress\\"\\n                    ]\\n                }\\n            }\\n        }\\n    }\\n}\\n```\\n\\nNow in the Azure portal, go to Azure Policy and create a new policy using the tempalate json from above.\\n\\n1. In Azure Policy, open \'Definitions\' and click \'+ Policy Definition\'\\n\\n  ![create policy 1](/img/2023-10-26-aks-custom-policy/create_policy1.jpg)\\n\\n2. Fill in the details and paste in the contents from your policy-definition.json and click \'Save\'\\n\\n  ![create policy 2](/img/2023-10-26-aks-custom-policy/create-policy2.jpg)\\n\\n3. Once the policy is created you will be navigated to the policy where you can select \'Assign\'\\n\\n  ![assign policy 1](/img/2023-10-26-aks-custom-policy/create-policy3.jpg)\\n\\n4. Fill in the details in all the tabs, including the scope and optionally adjust the \'Effect\' option to drive if the policy will audit or block\\n\\n  ![assign policy 2](/img/2023-10-26-aks-custom-policy/create-policy4.jpg)\\n\\n5. After you\'ve reviewed the policy, click \'Create\'\\n\\n  ![assign policy 3](/img/2023-10-26-aks-custom-policy/create-policy5.jpg)\\n\\n## Test the policy\\n\\nIt can take 30 minutes or more for the policy to apply to the selected resource. You can accelerate the process with the following. The following assumes you set the policy scope to the resource group of the cluster we created above.\\n\\n```bash\\naz policy state trigger-scan --resource-group $RG\\n```\\n\\nWhile the above command runs, you can watch the cluster for constraint templates and constraints to be automatically created by Azure Policy.\\n\\n```bash\\nkubectl get constrainttemplates|grep k8srestrictingresshostname\\n\\n# Sample Output\\nk8srestrictingresshostname         49m\\n\\n# Check the Constraint \\n# This assume you have the jq command, but you can remove it\\nkubectl get constraint -o=jsonpath=\'{.items[?(@.kind==\\"K8sRestrictIngressHostname\\")]}\'|jq\\n```\\n\\nIf you apply this on a cluster that already has ingress instances that are not compliant, you will see them eventually in the portal, but can also look a the constraint itself.\\n\\n```bash\\nkubectl get constraint -o=jsonpath=\'{.items[?(@.kind==\\"K8sRestrictIngressHostname\\")].status.violations}\'|jq\\n\\n# Sample Output\\n[\\n  {\\n    \\"enforcementAction\\": \\"dryrun\\",\\n    \\"group\\": \\"networking.k8s.io\\",\\n    \\"kind\\": \\"Ingress\\",\\n    \\"message\\": \\"invalid ingress host \\\\\\"myapp.com\\\\\\"\\",\\n    \\"name\\": \\"ingress-policy-fail\\",\\n    \\"namespace\\": \\"default\\",\\n    \\"version\\": \\"v1\\"\\n  }\\n]\\n```\\n\\n## Test the policy\\n\\nNow lets try to create a new ingress that does not match the policy.\\n\\n```bash\\nkubectl apply -f manifests/sample-ingress.yaml\\n\\ningress.networking.k8s.io/ingress-policy-pass created\\nError from server (Forbidden): error when creating \\"manifests/sample-ingress.yaml\\": admission webhook \\"validation.gatekeeper.sh\\" denied the request: [azurepolicy-k8srestrictingresshostname-ada01c8bf99df9ddb0a8] invalid ingress host \\"myapp.com\\"\\n```\\n\\n## Conclusion\\n\\nYou should now understand the process for creating a basic custom ConstraintTemplate and Azure Policy using that template."},{"id":"/2023/10/12/aks-user-min-roles","metadata":{"permalink":"/2023/10/12/aks-user-min-roles","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2023-10-12/aks-user-min-roles/index.md","source":"@site/blog/2023-10-12/aks-user-min-roles/index.md","title":"AKS User Minimum Roles","description":"How to use Azure AD RBAC in Azure Kubernetes Service","date":"2023-10-12T00:00:00.000Z","tags":[],"readingTime":6.82,"hasTruncateMarker":true,"authors":[{"name":"Steve Griffith","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/swgriffith","socials":{"x":"https://x.com/SteveGriffith","github":"https://github.com/swgriffith"},"imageURL":"https://github.com/swgriffith.png","key":"steve_griffith","page":null}],"frontMatter":{"authors":["steve_griffith"],"date":"2023-10-12","description":"How to use Azure AD RBAC in Azure Kubernetes Service","tags":[],"title":"AKS User Minimum Roles"},"unlisted":false,"prevItem":{"title":"AKS Custom Policy","permalink":"/2023/10/26/aks-custom-policy"},"nextItem":{"title":"Using App Gateway for Containers with Egress Lockdown","permalink":"/2023/09/21/agc-egress-lockdown"}},"content":"The following provides guidance on the minimum roles needed by an AKS user to get their credentials and interact with a namespace we\'ll create called \'sample-app\'.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Setup\\n\\nFirst we\'ll create an AKS cluster with Azure AD integrated authentication and Azure AD RBAC enabled. We will leave \'local accounts\' enabled for simplicity, but that option would typically be disabled and you would provide AKS an admin user Group ID for cluster admin access.\\n\\n### Cluster Creation\\n\\n```bash\\nSUBSCRIPTION_ID=\'\'\\nRG=EphAADDemo\\nLOC=eastus\\nCLUSTER_NAME=aad-demo\\n\\n# Create the resource group\\naz group create -n $RG -l $LOC\\n\\n# Create the cluster\\naz aks create \\\\\\n-g $RG \\\\\\n-n $CLUSTER_NAME \\\\\\n--enable-aad \\\\\\n--enable-azure-rbac\\n\\n# Get the admin credential so we can create the namespace\\naz aks get-credentials -g $RG -n $CLUSTER_NAME --admin\\n\\n# Create the namespace\\nkubectl create ns sample-app\\n\\n# Deploy an app\\nkubectl create deployment nginx --image=nginx --namespace sample-app\\n\\n# Expose the deployment as a Kubernetes service\\nkubectl expose deployment nginx --port 8080 --target-port 80 --namespace sample-app\\n\\n# Create a secret\\nkubectl create secret generic sample-secret --from-literal=message=Hello -n sample-app\\n\\n# Check out the created resources\\nkubectl get all -n sample-app\\n```\\n\\n### Create the test service principal\\n\\nWe\'ll create a service principal with no rights to use as our user account.\\n\\n```bash\\n# Create the sample app user\\naz ad sp create-for-rbac --skip-assignment -o json > sample-app-team-user.json\\n\\n# Get the App ID and Password into some variables we can use\\nAPP_ID=$(cat sample-app-team-user.json|jq -r .appId)\\nAPP_SECRET=$(cat sample-app-team-user.json|jq -r .password)\\nTENANT=$(cat sample-app-team-user.json|jq -r .tenant)\\n```\\n\\n### Allow user to call \'az aks get-credentials\'\\n\\nWe need to be able to get the cluster access details, which will live in the \'kubeconfig\' file and include the cluster FQDN and API access certificates. We got the admin credentials above, but we dont want to use those. We\'ll use the admin credential again later, so dont delete it just yet.  \\n\\n```bash\\n# First we need the resource ID for the cluster\\nCLUSTER_RESOURCE_ID=$(az aks show -g $RG -n $CLUSTER_NAME -o tsv --query id)\\n\\n# Take a look at the role definition we\'ll apply\\n# In particular, look at the enabled actions\\naz role definition list --name \\"Azure Kubernetes Service Cluster User Role\\" -o yaml\\n\\n# Give our user the role\\naz role assignment create \\\\\\n--assignee $APP_ID \\\\\\n--scope $CLUSTER_RESOURCE_ID \\\\\\n--role \\"Azure Kubernetes Service Cluster User Role\\"\\n\\n# Login as the assigned user\\naz login --service-principal -u $APP_ID -p $APP_SECRET --tenant $TENANT\\n\\n# Check your basic access\\n# You should be able to list AKS clusters only\\naz aks list\\n\\n# You should not be able to see any resource groups, vms, etc\\naz group list\\naz vm list\\n\\n# Now try to get AKS cluster credentials\\naz aks get-credentials -g $RG -n $CLUSTER_NAME\\n```\\n\\nThis is where things get a little weird in our scenario. We\'re signing in as a service account. In the next step we\'ll try to use the kubernetes cli (kubectl), and for normal human users you would get a device login prompt, which will take you to your normal Azure AD multi-factor auth flow. However, we\'re using a service principal so we need to take a few extra steps and use the [kubelogin](https://azure.github.io/kubelogin/quick-start.html) project to sign in.\\n\\nYou\'ll likely need to install kubelogin. You can find the installation steps here: [Installation](https://azure.github.io/kubelogin/install.html)\\n\\n```bash\\n# Convert the kube config file with kubelogin\\nkubelogin convert-kubeconfig -l azurecli\\n```\\n\\n### Try to access kubernetes objects\\n\\nWhen you try to access any kubernetes resources you\'ll find your service account has no access. We\'ll just check for namespace list access, but you\'ll see the same across any resource as we have not done anything yet with Kubernetes level RBAC.\\n\\n```bash\\nkubectl get ns\\n\\n# Sample Error Message\\nError from server (Forbidden): namespaces is forbidden: User \\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx\\" cannot list resource \\"namespaces\\" in API group \\"\\" at the cluster scope: User does not have access to the resource in Azure. Update role assignment to allow access.\\n```\\n\\n### Enable sample-app namespace access with Azure AD RBAC\\n\\nWe\'ll need to jump out of our service principal login back to our normal user account, so we can create some new role assignments.\\n\\n```bash\\n# Login to Azure\\naz login\\n\\n# If needed, set your subscription\\naz account set -s <SUBSCRIPTION ID>\\n```\\n\\nLets assign read-only access to the \'sample-app\' namespace alone for our service principal.\\n\\n```bash\\naz role assignment create --role \\"Azure Kubernetes Service RBAC Reader\\" --assignee $APP_ID --scope $CLUSTER_RESOURCE_ID/namespaces/sample-app\\n\\n# Log back in as the service principal\\naz login --service-principal -u $APP_ID -p $APP_SECRET --tenant $TENANT\\n\\n# Try to access resources in the cluster\\n# This command will be forbidden\\nkubectl get ns\\n\\n# Try a few other commands, and you\'ll see them fail.\\n\\n# Now try to read resources from the permitted namespace\\n# This command will succeed\\nkubectl get all -n sample-app\\n\\n# We can also view the pod logs\\nkubectl logs -l app=nginx -n sample-app\\n\\n# Secrets and roles have special treatment, so they are not visible. Try to look at secrets and roles\\n# These will fail\\nkubectl get secrets -n sample-app\\nkubectl get roles\\n\\n# We only granted read-only, so lets try to create something\\n# This will fail\\nkubectl run nginx-fail --image=nginx -n sample-app\\n\\n```\\n\\nNow lets increase the scope beyond our namespace and give the service principal read access across the whole cluster. You\'ll have to log back in with your normal user account to grant the role.\\n\\n```bash\\n# Again, log back into your other account\\naz login\\n\\n# Delete the previous role assignment\\naz role assignment delete --role \\"Azure Kubernetes Service RBAC Reader\\" --assignee $APP_ID --scope $CLUSTER_RESOURCE_ID/namespaces/sample-app\\n\\n# Create the new role assignment\\naz role assignment create --role \\"Azure Kubernetes Service RBAC Reader\\" --assignee $APP_ID --scope $CLUSTER_RESOURCE_ID\\n\\n# Log back in as your service principal account\\naz login --service-principal -u $APP_ID -p $APP_SECRET --tenant $TENANT\\n\\n# Now run some commands to see if you can read the whole cluster\\nkubectl get all -A\\nkubectl get pods -A\\n\\n# Now try to delete something, like our sample-app namespace\\n# This command will be forbidden\\nkubectl delete ns sample-app\\n\\n# Again, as mentioned with the namespace scope, secrets and roles are special. Try to view secrets and roles\\n# These will fail\\nkubectl get secrets -A\\nkubectl get roles\\n```\\n\\n### Explore the Out of the Box Roles\\n\\n\\nAKS provides 4 out of the box roles for Azure RBAC in Kubernetes. You can read more [here](https://learn.microsoft.com/en-us/azure/aks/manage-azure-rbac#create-role-assignments-for-users-to-access-the-cluster)\\n\\n![OOB Roles](/img/2023-10-12-aks-user-min-roles/oob-roles.jpg)\\n\\n## Create a custom role definition\\n\\nIf we want to expand the role to include secrets reader access then we\'ll need a custom role.\\n\\n```bash\\n# Make sure you\'re logged in as your normal user account\\n\\n# Create the role definition file\\ncat << EOF > secrets-aad-role.json\\n{\\n    \\"Name\\": \\"AKS Sample App Secrets Reader\\",\\n    \\"Description\\": \\"Lets you view all Secrets in the sample-app namespace.\\",\\n    \\"Actions\\": [],\\n    \\"NotActions\\": [],\\n    \\"DataActions\\": [\\n        \\"Microsoft.ContainerService/managedClusters/secrets/read\\"\\n    ],\\n    \\"NotDataActions\\": [],\\n    \\"assignableScopes\\": [\\n        \\"subscriptions/$SUBSCRIPTION_ID\\"\\n    ]\\n}\\nEOF\\n\\n# Create the role definition in Azure\\naz role definition create --role-definition @secrets-aad-role.json\\n\\n#####################################################################\\n#\\n# NOTE!!! Role propegation may take a couple minutes, so if this fails\\n# wait a minute and try again.\\n#\\n#####################################################################\\n\\n# Assign the role\\naz role assignment create --role \\"AKS Sample App Secrets Reader\\" --assignee $APP_ID --scope $CLUSTER_RESOURCE_ID/namespaces/sample-app\\n```\\n\\nNow that the custom role has been created and applied, we can log back in as our service principal and test it\'s ability to view secrets.\\n\\n```bash\\n# Log in as the service principal\\naz login --service-principal -u $APP_ID -p $APP_SECRET --tenant $TENANT\\n\\n# Look at the secret created previously\\nkubectl get secrets -n sample-app\\n\\n# Get the value out of the secret...keeping mind that secrets are base64 encoded\\nkubectl get secret sample-secret -n sample-app -o jsonpath=\'{.data.message}\'|base64 --decode\\n```\\n\\n## Conclusion\\n\\nThis walk-through demonstrated setting up an AKS cluster with Azure AD authenitication and Azure AD RBAC enabled, and then how to use Azure built in roles, as well as custom roles, to interact with your cluster."},{"id":"/2023/09/21/agc-egress-lockdown","metadata":{"permalink":"/2023/09/21/agc-egress-lockdown","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2023-09-21/agc-egress-lockdown/index.md","source":"@site/blog/2023-09-21/agc-egress-lockdown/index.md","title":"Using App Gateway for Containers with Egress Lockdown","description":"Demonstrat how to setup the new Azure App Gateway for Containers (AGC) managed ingress controllers with egress lockdown","date":"2023-09-21T00:00:00.000Z","tags":[],"readingTime":7.18,"hasTruncateMarker":true,"authors":[{"name":"Steve Griffith","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/swgriffith","socials":{"x":"https://x.com/SteveGriffith","github":"https://github.com/swgriffith"},"imageURL":"https://github.com/swgriffith.png","key":"steve_griffith","page":null}],"frontMatter":{"authors":["steve_griffith"],"date":"2023-09-21","description":"Demonstrat how to setup the new Azure App Gateway for Containers (AGC) managed ingress controllers with egress lockdown","tags":[],"title":"Using App Gateway for Containers with Egress Lockdown"},"unlisted":false,"prevItem":{"title":"AKS User Minimum Roles","permalink":"/2023/10/12/aks-user-min-roles"},"nextItem":{"title":"Using Kubelogin with AKS via Terraform","permalink":"/2023/09/21/kubelogin-terraform"}},"content":"This walkthrough demonstates the setup of the new Azure App Gateway for Containers (hereafter AGC) managed ingress controller on a cluster configured with egress traffic forced to an Azure Firewall and with the cluster configured with outboundType Route Table.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe YAML files for this walk through are located [here](https://github.com/swgriffith/azure-guides/tree/master/agc-egresslockdown)\\n\\n## Setup\\n\\nFor this setup we\'ll need to create the following:\\n\\n- Resource Group\\n- Vnet with subnets for the Firewall, AGC and AKS Cluster\\n- Azure Firewall\\n- Firewall Rules needed for AKS to function\\n- Route Table with Default Route (0.0.0.0/0) to the firewall\\n- AKS Cluster\\n- AGC\\n- Test app deployed to the cluster\\n- Gateway Instance\\n- HTTP Route\\n\\n\\n### Resource Group and Vnet\\n\\nFirst create the Resource Group and virtual network we\'ll use for the deployment. We\'ll also create the subnet that the Azure Firewall will use, since we\'ll be deploying that next.\\n\\n```bash\\n# Resource Group Creation\\nRG=EphAGCEgressLock2\\nLOC=eastus\\naz group create -g $RG -l $LOC\\n\\n# Get the resource group id\\nRG_ID=$(az group show -g $RG -o tsv --query id)\\n\\n# Set an environment variable for the VNet name\\nVNET_NAME=reddog-vnet\\n\\n# Create the Vnet along with the initial subet for AKS\\naz network vnet create \\\\\\n-g $RG \\\\\\n-n $VNET_NAME \\\\\\n--address-prefix 10.140.0.0/16 \\\\\\n--subnet-name aks \\\\\\n--subnet-prefix 10.140.0.0/24\\n\\n# Get a subnet resource ID\\nVNET_SUBNET_ID=$(az network vnet subnet show -g $RG --vnet-name $VNET_NAME -n aks -o tsv --query id)\\n\\n# Adding a subnet for the Azure Firewall\\naz network vnet subnet create \\\\\\n--resource-group $RG \\\\\\n--vnet-name $VNET_NAME \\\\\\n--name AzureFirewallSubnet \\\\\\n--address-prefix 10.140.1.0/24\\n```\\n\\n### Firewall\\n\\nNow to create the Azure Firewall.\\n\\n```bash\\n# Create Azure Firewall Public IP\\naz network public-ip create -g $RG -n azfirewall-ip --sku \\"Standard\\"\\n\\n# Create Azure Firewall\\naz extension add --name azure-firewall\\nFIREWALLNAME=reddog-egress\\naz network firewall create -g $RG -n $FIREWALLNAME --enable-dns-proxy true\\n\\n# Configure Firewall IP Config\\naz network firewall ip-config create -g $RG -f $FIREWALLNAME -n aks-firewallconfig --public-ip-address azfirewall-ip --vnet-name $VNET_NAME\\n\\n```\\n\\n### Firewall Rules\\n\\nWith the firewall created, we\'ll add the rules needed to ensure AKS can operate. You can add additional rules here as needed.\\n\\n```bash\\naz network firewall network-rule create \\\\\\n-g $RG \\\\\\n-f $FIREWALLNAME \\\\\\n--collection-name \'aksfwnr\' \\\\\\n-n \'aksapiudp\' \\\\\\n--protocols \'UDP\' \\\\\\n--source-addresses \'*\' \\\\\\n--destination-addresses \\"AzureCloud.$LOC\\" \\\\\\n--destination-ports 1194 --action allow --priority 100\\n\\naz network firewall network-rule create \\\\\\n-g $RG \\\\\\n-f $FIREWALLNAME \\\\\\n--collection-name \'aksfwnr\' \\\\\\n-n \'aksapitcp\' \\\\\\n--protocols \'TCP\' \\\\\\n--source-addresses \'*\' \\\\\\n--destination-addresses \\"AzureCloud.$LOC\\" \\\\\\n--destination-ports 9000 443\\n\\naz network firewall network-rule create \\\\\\n-g $RG \\\\\\n-f $FIREWALLNAME \\\\\\n--collection-name \'aksfwnr\' \\\\\\n-n \'time\' \\\\\\n--protocols \'UDP\' \\\\\\n--source-addresses \'*\' \\\\\\n--destination-fqdns \'ntp.ubuntu.com\' \\\\\\n--destination-ports 123\\n\\n\\n# Add FW Application Rules\\naz network firewall application-rule create \\\\\\n-g $RG \\\\\\n-f $FIREWALLNAME \\\\\\n--collection-name \'aksfwar\' \\\\\\n-n \'fqdn\' \\\\\\n--source-addresses \'*\' \\\\\\n--protocols \'http=80\' \'https=443\' \\\\\\n--fqdn-tags \\"AzureKubernetesService\\" \\\\\\n--action allow --priority 100\\n\\nTARGET_FQDNS=(\'mcr.microsoft.com\' \\\\\\n\'*.data.mcr.microsoft.com\' \\\\\\n\'management.azure.com\' \\\\\\n\'login.microsoftonline.com\' \\\\\\n\'packages.microsoft.com\' \\\\\\n\'acs-mirror.azureedge.net\')\\n\\naz network firewall application-rule create \\\\\\n-g $RG \\\\\\n-f $FIREWALLNAME \\\\\\n--collection-name \'aksfwar2\' \\\\\\n-n \'fqdn\' \\\\\\n--source-addresses \'*\' \\\\\\n--protocols \'http=80\' \'https=443\' \\\\\\n--target-fqdns $TARGET_FQDNS[@] \\\\\\n--action allow --priority 101\\n```\\n\\n\\n### Route Table\\n\\nWith the firewall created, we\'ll set up the route table to ensure that egress traffic is sent to the firewall and then we\'ll attach this route table to the AKS cluster subnet.\\n\\n```bash\\n# First get the public and private IP of the firewall for the routing rules\\nFWPUBLIC_IP=$(az network public-ip show -g $RG -n azfirewall-ip --query \\"ipAddress\\" -o tsv)\\nFWPRIVATE_IP=$(az network firewall show -g $RG -n $FIREWALLNAME --query \\"ipConfigurations[0].privateIPAddress\\" -o tsv)\\n\\n# Create Route Table\\naz network route-table create \\\\\\n-g $RG \\\\\\n-n aksdefaultroutes\\n\\n# Create Route\\naz network route-table route create \\\\\\n-g $RG \\\\\\n--route-table-name aksdefaultroutes \\\\\\n-n firewall-route \\\\\\n--address-prefix 0.0.0.0/0 \\\\\\n--next-hop-type VirtualAppliance \\\\\\n--next-hop-ip-address $FWPRIVATE_IP\\n\\naz network route-table route create \\\\\\n-g $RG \\\\\\n--route-table-name aksdefaultroutes \\\\\\n-n internet-route \\\\\\n--address-prefix $FWPUBLIC_IP/32 \\\\\\n--next-hop-type Internet\\n\\n# Associate Route Table to AKS Subnet\\naz network vnet subnet update \\\\\\n-g $RG \\\\\\n--vnet-name $VNET_NAME \\\\\\n-n aks \\\\\\n--route-table aksdefaultroutes\\n```\\n\\n### Cluster Creation\\n\\nNext we\'ll create the AKS Cluster. We\'ll set this up with a single node, for testing purposes and will enable outboundType for userDefinedRouting. We\'ll also enable the OIDC Issuer and workload identity, as they\'re used by AGC later.\\n\\n> **NOTE:** At this time, AGC only supports Azure CNI in standard mode, not in \'Overlay\' mode. If you try another option here it will not work.\\n\\n```bash\\n# NOTE: Make sure you give your cluster a unique name\\nCLUSTER_NAME=acglab\\n\\n# Cluster Creation Command\\naz aks create \\\\\\n-g $RG \\\\\\n-n $CLUSTER_NAME \\\\\\n--nodepool-name systempool \\\\\\n--node-vm-size Standard_D2_v4 \\\\\\n--node-count 1 \\\\\\n--network-plugin azure \\\\\\n--network-policy calico \\\\\\n--vnet-subnet-id $VNET_SUBNET_ID \\\\\\n--outbound-type userDefinedRouting \\\\\\n--enable-managed-identity \\\\\\n--enable-oidc-issuer \\\\\\n--enable-workload-identity \\n\\n# Grab the cluster credentials\\naz aks get-credentials -g $RG -n $CLUSTER_NAME\\n```\\n\\n### Setup App Gateway for Containers\\n\\nNow that our cluster is working we can add a new subnet for the AGC and run through all the steps for setting up the application controler, based on the setup guide [here](https://learn.microsoft.com/en-us/azure/application-gateway/for-containers/quickstart-deploy-application-gateway-for-containers-alb-controller?tabs=install-helm-windows). That involved getting the subnet ID and Managed Cluster ID, creating a managed Identity, federating that managed identity with a Kubernetes Service Account, granting the identity the rights documented in the product setup and then finally installing the Application Load Balancer controller via Helm.\\n\\n```bash\\n# Create the AGC subnet\\naz network vnet subnet create \\\\\\n--resource-group $RG \\\\\\n--vnet-name $VNET_NAME \\\\\\n--name subnet-alb \\\\\\n--address-prefixes 10.140.2.0/24 \\\\\\n--delegations \'Microsoft.ServiceNetworking/trafficControllers\'\\n\\n# Get the AGC Subnet ID\\nALB_SUBNET_ID=$(az network vnet subnet show --name subnet-alb --resource-group $RG --vnet-name $VNET_NAME --query \'[id]\' --output tsv)\\n\\n# Get the Managed Cluster Resource Group and ID\\nMC_RG=$(az aks show --resource-group $RG --name $CLUSTER_NAME --query \\"nodeResourceGroup\\" -o tsv)\\nMC_RG_ID=$(az group show --name $MC_RG --query id -otsv)\\n\\n# Create a new managed identity and get its principal ID\\nIDENTITY_RESOURCE_NAME=\'azure-alb-identity\'\\naz identity create --resource-group $RG --name $IDENTITY_RESOURCE_NAME\\nPRINCIPAL_ID=\\"$(az identity show -g $RG -n $IDENTITY_RESOURCE_NAME --query principalId -otsv)\\"\\n\\n# Assign the managed identity reader rights on the managed cluster resource group\\naz role assignment create --assignee-object-id $PRINCIPAL_ID --assignee-principal-type ServicePrincipal --scope $MC_RG_ID --role \\"acdd72a7-3385-48ef-bd42-f606fba81ae7\\" # Reader role\\n\\n# Get the OIDC Issuer Name and federate a service account, which will be created later, with the managed identity we created above\\nAKS_OIDC_ISSUER=\\"$(az aks show -n \\"$CLUSTER_NAME\\" -g \\"$RG\\" --query \\"oidcIssuerProfile.issuerUrl\\" -o tsv)\\"\\n\\naz identity federated-credential create --name \\"azure-alb-identity\\" \\\\\\n--identity-name \\"$IDENTITY_RESOURCE_NAME\\" \\\\\\n--resource-group $RG \\\\\\n--issuer \\"$AKS_OIDC_ISSUER\\" \\\\\\n--subject \\"system:serviceaccount:azure-alb-system:alb-controller-sa\\"\\n\\n# Install the Application Load Balancer for AGC via Helm\\nhelm install alb-controller oci://mcr.microsoft.com/application-lb/charts/alb-controller \\\\\\n--version 0.4.023971 \\\\\\n--set albController.podIdentity.clientID=$(az identity show -g $RG -n azure-alb-identity --query clientId -o tsv)\\n\\n# Verify that the pods start and check that the gateway setup completed successfully\\nwatch kubectl get pods -n azure-alb-system\\nkubectl get gatewayclass azure-alb-external -o yaml\\n```\\n\\n### Deploy the Application LoadBalancer Instance\\n\\nWith the Application Load Balancer Controller running, we now want to create an instance of the ALB in Kubernetes. We\'ll need to give the managed identity some additional rights.\\n\\n```bash\\n# Delegate AppGw for Containers Configuration Manager role to AKS Managed Cluster RG\\naz role assignment create --assignee-object-id $PRINCIPAL_ID --assignee-principal-type ServicePrincipal --scope $MC_RG_ID --role \\"fbc52c3f-28ad-4303-a892-8a056630b8f1\\"  \\n# Delegate Network Contributor permission for join to association subnet\\naz role assignment create --assignee-object-id $PRINCIPAL_ID --assignee-principal-type ServicePrincipal --scope $ALB_SUBNET_ID --role \\"4d97b98b-1d4f-4787-a291-c67834d212e7\\" \\n\\n# Create the instance of the Application Load Balancer in the cluster.\\nkubectl apply -f - <<EOF\\napiVersion: alb.networking.azure.io/v1\\nkind: ApplicationLoadBalancer\\nmetadata:\\n  name: alb-test\\nspec:\\n  associations:\\n  - $ALB_SUBNET_ID\\nEOF\\n\\n# Monitor the state of the ALB Setup until it\'s ready\\nkubectl get applicationloadbalancer alb-test -o yaml -w\\n```\\n\\n### Deploy the test app\\n\\nNow that we have the environment all set up we can deploy a simple test app, gateway and http-route. For this, we\'ll test TLS offload, so we\'ll upload a self signed certificate as a secret and used that for the application ingress over https.\\n\\n```bash\\n# Deploy the test application\\nkubectl apply -f testapp.yaml\\n\\n# Create the TLS Secret and Gateway config\\nkubectl apply -f gateway.yaml\\n\\n# Create the http route\\nkubectl apply -f http-route.yaml\\n\\n# Get the FQDN for the gateway\\nFQDN=$(kubectl get gateway gateway-01 -o jsonpath=\'{.status.addresses[0].value}\')\\n\\n# Run a test curl to ensure you get a response\\ncurl --insecure https://$FQDN/\\n```"},{"id":"/2023/09/21/kubelogin-terraform","metadata":{"permalink":"/2023/09/21/kubelogin-terraform","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2023-09-21/kubelogin-terraform/index.md","source":"@site/blog/2023-09-21/kubelogin-terraform/index.md","title":"Using Kubelogin with AKS via Terraform","description":"How to use the Terraform AKS provider to create an AKS cluster for Azure Active Directory authentication and RBAC and disable local accounts","date":"2023-09-21T00:00:00.000Z","tags":[],"readingTime":3.76,"hasTruncateMarker":true,"authors":[{"name":"Steve Griffith","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/swgriffith","socials":{"x":"https://x.com/SteveGriffith","github":"https://github.com/swgriffith"},"imageURL":"https://github.com/swgriffith.png","key":"steve_griffith","page":null}],"frontMatter":{"authors":["steve_griffith"],"date":"2023-09-21","description":"How to use the Terraform AKS provider to create an AKS cluster for Azure Active Directory authentication and RBAC and disable local accounts","tags":[],"title":"Using Kubelogin with AKS via Terraform"},"unlisted":false,"prevItem":{"title":"Using App Gateway for Containers with Egress Lockdown","permalink":"/2023/09/21/agc-egress-lockdown"},"nextItem":{"title":"Workload Identity","permalink":"/2023/09/21/workload-identity-example"}},"content":"### Special Thanks!\\n\\nThanks to [Daniel Neumann](https://www.danielstechblog.io/about-me/) for the original article we used to get this started, which you can find [here](https://www.danielstechblog.io/azure-kubernetes-service-using-kubernetes-credential-plugin-kubelogin-with-terraform/).\\n\\nThanks to [Ray Kao](https://github.com/raykao) for working out a bunch of bugs we hit and dropping some sweet PRs.\\n\\nAll of the Terraform files can be found [here](https://github.com/swgriffith/azure-guides/tree/master/aks-tf-examples/kubelogin).\\n\\n\x3c!-- truncate --\x3e\\n\\n## Overview\\n\\nIn this walkthrough we\'ll use the Terraform AKS provider to create an AKS cluster that has Azure AD enabled, Azure AD RBAC enabled and has disabled local accounts. That means that this cluster will require Azure Active Directory authentication and RBAC and will explicitly not allow local accounts.\\n\\nWe\'ll then use the Terraform Kubernetes provider to create a deployment in the cluster using a valid service principal.\\n\\n### Real Talk\\n\\nAfter testing this out, in my opinion...as well as some of my peers, creating the cluster and deploying workloads in the same Terraform script adds some unecessary complexity and brittleness. You\'ll likely have a better exerience having a Terraform script for the infrastructure provisioning and a separate approach (ex. Flux, Argo, Other CD Pipeline) to deploy your workloads to the cluster.\\n\\nThat said...lets give this a go.\\n\\n## Requirements\\n\\nHere are the items we\'ll assume you\'ve already set up before running this deployment.\\n\\n#### 1. An Azure AD Group for your cluster Administrators\\nWhen you enable Azure AD on AKS and disable local accounts, you are required to provide at least one group ID for the \'Administrators\' of the cluster.\\n\\n#### 2. A service principal that is a member of the Administrators group\\nWe\'ll be running a deployment against the cluster, and to do that we\'ll need a valid user. For this example we\'re using a service principal. You could adjust this to use a managed identity as well if you prefer, but you\'ll need to modify the template. You will need both the \'client id\' and a \'client secret\' for this service principal, creation of which is outside the scope of this guide...for now.\\n\\n#### 3. Kubelogin must be installed on the machine running the terraform deployment\\nAccessing Azure Active Directory enabled clusters with a non-interactive login flow (ex. via automated deployment pipeline) requires that you use the [kubelogin](https://azure.github.io/kubelogin/index.html) project. Kubelogin will handle the OAuth flows needed to get the cluster access token.\\n\\n## Running the deployment\\n\\nFirst you need to create a new \'terraform.tfvars\' file at the same level as the main.tf file. That file should look like the following, but with your own values applied for the AAD tenant ID, Admins AAD Group ID, client id and client secret.\\n\\n```bash\\nadmin_group_object_ids = [\\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\\"]\\ntenant_id = \\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\\"\\nclient_id = \\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\\"\\nclient_secret = \\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\\"\\n```\\n\\nNext we\'ll run the deployment commands.\\n\\n```bash\\n# Initialize Terraform\\nterraform init\\n\\n# Run Terraform Plan\\nterraform plan\\n\\n# Deplouy\\nterraform apply --auto-approve\\n```\\n\\n## How this works\\n\\nWhile the deployment is running, lets cover how this works. As noted above our cluster is locked down to only Azure AD users and no local kubernetes accounts, like the default admin account. Additionally, since the cluster is AAD enabled for authentication, you typically would get a device code flow prompt when trying to run deployments via kubectl. Since the deployment will be done by the Terraform Kubernetes provider, we\'ll need to use kubelogin, which we can do via the provider\'s \'exec\' option.\\n\\nHere are the steps:\\n\\n1. Resource Group and Cluster are created\\n2. The Kubernetes provider runs \'kubelogin get-token\' passing in some of the details from the cluster creation (ex. API Server FQDN) as well as the service principal credentials. It also needs the application ID of the AKS cluster login server, which we get via the \'azuread_service_principal\' block.\\n3. The \'kubernetes_deployment\' block runs using the kubernetes provider, which now has it\'s access token, to run the nginx deployment\\n\\n## Conclusion\\n\\nOnce the above deployment completes, you should be able to connect to your cluster (Azure Portal or kubectl) and see the nginx deployment is running.\\n\\nTo clean up your deployment, there is a bit of a state management issue with this deployment, since both the cluster and deployment were in one run. I\'ll probably split that out later. For now, we\'ll just delete the nginx deployment from state before deleting the cluster.\\n\\n```bash\\n# Remove the nginx deployment from state\\nterraform state rm kubernetes_deployment.nginx\\n\\n# Destroy the deployment\\nterraform destroy --auto-approve\\n```"},{"id":"/2023/09/21/workload-identity-example","metadata":{"permalink":"/2023/09/21/workload-identity-example","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2023-09-21/workload-identity-example/index.md","source":"@site/blog/2023-09-21/workload-identity-example/index.md","title":"Workload Identity","description":"How to use Azure Workload Identity with the AKS Workload Identity add-on along with MSAL","date":"2023-09-21T00:00:00.000Z","tags":[],"readingTime":4.29,"hasTruncateMarker":true,"authors":[{"name":"Steve Griffith","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/swgriffith","socials":{"x":"https://x.com/SteveGriffith","github":"https://github.com/swgriffith"},"imageURL":"https://github.com/swgriffith.png","key":"steve_griffith","page":null}],"frontMatter":{"authors":["steve_griffith"],"date":"2023-09-21","description":"How to use Azure Workload Identity with the AKS Workload Identity add-on along with MSAL","tags":[],"title":"Workload Identity"},"unlisted":false,"prevItem":{"title":"Using Kubelogin with AKS via Terraform","permalink":"/2023/09/21/kubelogin-terraform"},"nextItem":{"title":"Accessing Azure SQL DB via Workload Identity and Managed Identity","permalink":"/2021/09/21/workload-identity-azuresql-example"}},"content":"The following walkthrough shows how you can using [Azure Workload Identity](https://azure.github.io/azure-workload-identity/docs/) with the [AKS Workload Identity](https://learn.microsoft.com/en-us/azure/aks/workload-identity-overview) add-on along with [MSAL](https://learn.microsoft.com/en-us/azure/active-directory/develop/reference-v2-libraries)\\n\\n\x3c!-- truncate --\x3e\\n\\n### Register for the preview\\n\\nThe managed add-on for Azure Workload Identity is still in preview, so we must first register for the preview.\\n\\n```bash\\n# Add or update the Azure CLI aks preview extention\\naz extension add --name aks-preview\\naz extension update --name aks-preview\\n\\n# Register for the preview feature\\naz feature register --namespace \\"Microsoft.ContainerService\\" --name \\"EnableWorkloadIdentityPreview\\"\\n\\n# Check registration status\\naz feature list -o table --query \\"[?contains(name, \'Microsoft.ContainerService/EnableWorkloadIdentityPreview\')].{Name:name,State:properties.state}\\"\\n\\n# Refresh the provider\\naz provider register --namespace Microsoft.ContainerService\\n```\\n\\n### Cluster Creation\\n\\nNow lets create the AKS cluster with the OIDC Issure and Workload Identity add-on enabled.\\n\\n```bash\\nRG=WorkloadIdentityRG\\nLOC=eastus\\nCLUSTER_NAME=wilab\\nUNIQUE_ID=$CLUSTER_NAME$RANDOM\\nACR_NAME=$UNIQUE_ID\\nKEY_VAULT_NAME=$UNIQUE_ID\\n\\n# Create the resource group\\naz group create -g $RG -l $LOC\\n\\n# Create the cluster with the OIDC Issuer and Workload Identity enabled\\naz aks create -g $RG -n $CLUSTER_NAME \\\\\\n--node-count 1 \\\\\\n--enable-oidc-issuer \\\\\\n--enable-workload-identity \\\\\\n--generate-ssh-keys\\n\\n# Get the cluster credentials\\naz aks get-credentials -g $RG -n $CLUSTER_NAME\\n```\\n\\n### Set up the identity \\n\\nIn order to federate a managed identity with a Kubernetes Service Account we need to get the AKS OIDC Issure URL, create the Managed Identity and Service Account and then create the federation.\\n\\n```bash\\n# Get the OIDC Issuer URL\\nexport AKS_OIDC_ISSUER=\\"$(az aks show -n $CLUSTER_NAME -g $RG --query \\"oidcIssuerProfile.issuerUrl\\" -otsv)\\"\\n\\n# Create the managed identity\\naz identity create --name wi-demo-identity --resource-group $RG --location $LOC\\n\\n# Get identity client ID\\nexport USER_ASSIGNED_CLIENT_ID=$(az identity show --resource-group $RG --name wi-demo-identity --query \'clientId\' -o tsv)\\n\\n# Create a service account to federate with the managed identity\\ncat <<EOF | kubectl apply -f -\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  annotations:\\n    azure.workload.identity/client-id: ${USER_ASSIGNED_CLIENT_ID}\\n  labels:\\n    azure.workload.identity/use: \\"true\\"\\n  name: wi-demo-sa\\n  namespace: default\\nEOF\\n\\n# Federate the identity\\naz identity federated-credential create \\\\\\n--name wi-demo-federated-id \\\\\\n--identity-name wi-demo-identity \\\\\\n--resource-group $RG \\\\\\n--issuer ${AKS_OIDC_ISSUER} \\\\\\n--subject system:serviceaccount:default:wi-demo-sa\\n```\\n\\n### Create the Key Vault and Secret\\n\\n```bash\\n# Create a key vault\\naz keyvault create --name $KEY_VAULT_NAME --resource-group $RG --location $LOC --enable-rbac-authorization false\\n\\n# Create a secret\\naz keyvault secret set --vault-name $KEY_VAULT_NAME --name \\"Secret\\" --value \\"Hello\\"\\n\\n# Grant access to the secret for the managed identity\\naz keyvault set-policy --name $KEY_VAULT_NAME --secret-permissions get --spn \\"${USER_ASSIGNED_CLIENT_ID}\\"\\n\\n# Get the version ID\\naz keyvault secret show --vault-name $KEY_VAULT_NAME --name \\"Secret\\" -o tsv --query id\\nhttps://wi-demo-keyvault.vault.azure.net/secrets/Secret/ded8e5e3b3e040e9bfa5c47d0e28848a\\n\\n# The version ID is the last part of the resource id above\\n# We\'ll use this later\\nVERSION_ID=ded8e5e3b3e040e9bfa5c47d0e28848a\\n```\\n\\n## Create the sample app\\n\\n```bash\\n# Create and test a new console app\\ndotnet new console -n keyvault-console-app\\ncd keyvault-console-app\\ndotnet run\\n\\n# Add the Key Vault and Azure Identity Packages\\ndotnet add package Azure.Security.KeyVault.Secrets\\ndotnet add package Azure.Identity\\n```\\n\\nEdit the app as follows:\\n\\n```csharp\\nusing System;\\nusing System.IO;\\nusing Azure.Core;\\nusing Azure.Identity;\\nusing Azure.Security.KeyVault.Secrets;\\n\\nclass Program\\n    {\\n        static void Main(string[] args)\\n        {\\n            //Get env variables\\n            string? secretName = Environment.GetEnvironmentVariable(\\"SECRET_NAME\\");;\\n            string? keyVaultName = Environment.GetEnvironmentVariable(\\"KEY_VAULT_NAME\\");;\\n            string? versionID = Environment.GetEnvironmentVariable(\\"VERSION_ID\\");;\\n            \\n            //Create Key Vault Client\\n            var kvUri = String.Format(\\"https://{0}.vault.azure.net\\", keyVaultName);\\n            SecretClientOptions options = new SecretClientOptions()\\n            {\\n                Retry =\\n                {\\n                    Delay= TimeSpan.FromSeconds(2),\\n                    MaxDelay = TimeSpan.FromSeconds(16),\\n                    MaxRetries = 5,\\n                    Mode = RetryMode.Exponential\\n                 }\\n            };\\n\\n            var client = new SecretClient(new Uri(kvUri), new DefaultAzureCredential(),options);\\n\\n            // Get the secret value in a loop\\n            while(true){\\n            Console.WriteLine(\\"Retrieving your secret from \\" + keyVaultName + \\".\\");\\n            KeyVaultSecret secret = client.GetSecret(secretName, versionID);\\n            Console.WriteLine(\\"Your secret is \'\\" + secret.Value + \\"\'.\\");\\n            System.Threading.Thread.Sleep(5000);\\n            }\\n\\n        }\\n    }\\n```\\n\\nCreate a new Dockerfile with the following:\\n\\n```bash\\nFROM mcr.microsoft.com/dotnet/sdk:6.0 AS build-env\\nWORKDIR /App\\n\\n# Copy everything\\nCOPY . ./\\n# Restore as distinct layers\\nRUN dotnet restore\\n# Build and publish a release\\nRUN dotnet publish -c Release -o out\\n\\n# Build runtime image\\nFROM mcr.microsoft.com/dotnet/aspnet:6.0\\nWORKDIR /App\\nCOPY --from=build-env /App/out .\\nENTRYPOINT [\\"dotnet\\", \\"keyvault-console-app.dll\\"]\\n```\\n\\nBuild the image. I\'ll create an Azure Container Registry and build there, and then link that ACR to my AKS cluster.\\n\\n```bash\\n# Create the ACR\\naz acr create -g $RG -n $ACR_NAME --sku Standard\\n\\n# Build the image\\naz acr build -t wi-kv-test -r $ACR_NAME .\\n\\n# Link the ACR to the AKS cluster\\naz aks update -g $RG -n $CLUSTER_NAME --attach-acr $ACR_NAME\\n```\\n\\nNow deploy a pod that gets the value using the service account identity.\\n\\n```bash\\n\\ncat <<EOF | kubectl apply -f -\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: wi-kv-test\\n  namespace: default\\n  labels:\\n    azure.workload.identity/use: \\"true\\"  \\nspec:\\n  serviceAccountName: wi-demo-sa\\n  containers:\\n    - image: ${ACR_NAME}.azurecr.io/wi-kv-test\\n      name: wi-kv-test\\n      env:\\n      - name: KEY_VAULT_NAME\\n        value: ${KEY_VAULT_NAME}\\n      - name: SECRET_NAME\\n        value: Secret\\n      - name: VERSION_ID\\n        value: ${VERSION_ID}       \\n  nodeSelector:\\n    kubernetes.io/os: linux\\nEOF\\n\\n# Check the pod logs\\nkubectl logs -f wi-kv-test\\n\\n# Sample Output\\nRetrieving your secret from wi-demo-keyvault.\\nYour secret is \'Hello\'.\\n```\\n\\n### Conclusion\\n\\nCongrats! You should now have a working pod that uses MSAL along with a Kubernetes Service Account federated to an Azure Managed Identity to access and Azure Key Vault Secret."},{"id":"/2021/09/21/workload-identity-azuresql-example","metadata":{"permalink":"/2021/09/21/workload-identity-azuresql-example","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2021-09-21/workload-identity-azuresql-example/index.md","source":"@site/blog/2021-09-21/workload-identity-azuresql-example/index.md","title":"Accessing Azure SQL DB via Workload Identity and Managed Identity","description":"How to create an AKS cluster enabled with Workload Identity to access Azure SQL DB with Azure Managed Identity from a Kubernetes pod","date":"2021-09-21T00:00:00.000Z","tags":[],"readingTime":5.14,"hasTruncateMarker":true,"authors":[{"name":"Steve Griffith","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/swgriffith","socials":{"x":"https://x.com/SteveGriffith","github":"https://github.com/swgriffith"},"imageURL":"https://github.com/swgriffith.png","key":"steve_griffith","page":null}],"frontMatter":{"authors":["steve_griffith"],"date":"2021-09-21","description":"How to create an AKS cluster enabled with Workload Identity to access Azure SQL DB with Azure Managed Identity from a Kubernetes pod","tags":[],"title":"Accessing Azure SQL DB via Workload Identity and Managed Identity"},"unlisted":false,"prevItem":{"title":"Workload Identity","permalink":"/2023/09/21/workload-identity-example"},"nextItem":{"title":"Using Workload Identity with Self Managed Clusters","permalink":"/2021/09/21/workload-identity-self-managed-setup"}},"content":"\x3c!-- truncate --\x3e\\n\\n## Setup\\n\\n### Cluster Creation\\n\\nLets create the AKS cluster with the OIDC Issure and Workload Identity add-on enabled.\\n\\n```bash\\nRG=WorkloadIdentitySQLRG\\nLOC=eastus\\nCLUSTER_NAME=wisqllab\\nUNIQUE_ID=$CLUSTER_NAME$RANDOM\\nACR_NAME=$UNIQUE_ID\\n\\n# Create the resource group\\naz group create -g $RG -l $LOC\\n\\n# Create the cluster with the OIDC Issuer and Workload Identity enabled\\naz aks create -g $RG -n $CLUSTER_NAME \\\\\\n--node-count 1 \\\\\\n--enable-oidc-issuer \\\\\\n--enable-workload-identity \\\\\\n--generate-ssh-keys\\n\\n# Get the cluster credentials\\naz aks get-credentials -g $RG -n $CLUSTER_NAME\\n```\\n\\n### Set up the identity \\n\\nIn order to federate a managed identity with a Kubernetes Service Account we need to get the AKS OIDC Issure URL, create the Managed Identity and Service Account and then setup the federation.\\n\\n```bash\\n# Get the OIDC Issuer URL\\nexport AKS_OIDC_ISSUER=\\"$(az aks show -n $CLUSTER_NAME -g $RG --query \\"oidcIssuerProfile.issuerUrl\\" -otsv)\\"\\n\\nMANAGED_IDENTITY_NAME=wi-demo-identity\\n\\n# Create the managed identity\\naz identity create --name $MANAGED_IDENTITY_NAME --resource-group $RG --location $LOC\\n\\n# Get identity client ID\\nexport USER_ASSIGNED_CLIENT_ID=$(az identity show --resource-group $RG --name $MANAGED_IDENTITY_NAME --query \'clientId\' -o tsv)\\nexport USER_ASSIGNED_OBJ_ID=$(az identity show --resource-group $RG --name $MANAGED_IDENTITY_NAME --query \'principalId\' -o tsv)\\n\\n# Create a service account to federate with the managed identity\\ncat <<EOF | kubectl apply -f -\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  annotations:\\n    azure.workload.identity/client-id: ${USER_ASSIGNED_CLIENT_ID}\\n  labels:\\n    azure.workload.identity/use: \\"true\\"\\n  name: wi-demo-sa\\n  namespace: default\\nEOF\\n\\n# Federate the identity\\naz identity federated-credential create \\\\\\n--name wi-demo-federated-id \\\\\\n--identity-name $MANAGED_IDENTITY_NAME \\\\\\n--resource-group $RG \\\\\\n--issuer ${AKS_OIDC_ISSUER} \\\\\\n--subject system:serviceaccount:default:wi-demo-sa\\n```\\n\\n### Create the Azure SQL DB Server and Database\\n\\n```bash\\n# Create a single database and configure a firewall rule\\nUNIQUE_ID=$RANDOM\\nSERVER_NAME=\\"widemo-$UNIQUE_ID\\"\\nDATABASE_NAME=\\"widemo$UNIQUE_ID\\"\\nLOGIN=\\"azureuser\\"\\nPASSWD=\\"Pa$$w0rD-$UNIQUE_ID\\"\\n# Specify appropriate IP address values for your environment\\n# to limit access to the SQL Database server\\nMY_IP=$(curl icanhazip.com)\\n\\n# Create the SQL Server Instance\\naz sql server create \\\\\\n--name $SERVER_NAME \\\\\\n--resource-group $RG \\\\\\n--location $LOC \\\\\\n--admin-user $LOGIN \\\\\\n--admin-password $PASSWD\\n\\n# Allow your ip through the server firewall\\naz sql server firewall-rule create \\\\\\n--resource-group $RG \\\\\\n--server $SERVER_NAME \\\\\\n-n AllowYourIp \\\\\\n--start-ip-address $MY_IP \\\\\\n--end-ip-address $MY_IP\\n\\n# Allow azure services through the server firewall\\naz sql server firewall-rule create \\\\\\n--resource-group $RG \\\\\\n--server $SERVER_NAME \\\\\\n-n AllowAzureServices \\\\\\n--start-ip-address 0.0.0.0 \\\\\\n--end-ip-address 0.0.0.0\\n\\n# Create the Database \\naz sql db create --resource-group $RG --server $SERVER_NAME \\\\\\n--name $DATABASE_NAME \\\\\\n--sample-name AdventureWorksLT \\\\\\n--edition GeneralPurpose \\\\\\n--family Gen5 \\\\\\n--capacity 2 \\\\\\n--zone-redundant false \\n\\n# Get user info for adding admin user\\nSIGNED_IN_USER_OBJ_ID=$(az ad signed-in-user show -o tsv --query id)\\nSIGNED_IN_USER_DSP_NAME=$(az ad signed-in-user show -o tsv --query userPrincipalName)\\n\\n# Add yourself as the Admin User\\naz sql server ad-admin create \\\\\\n--resource-group $RG \\\\\\n--server-name $SERVER_NAME \\\\\\n--display-name $SIGNED_IN_USER_DSP_NAME \\\\\\n--object-id $SIGNED_IN_USER_OBJ_ID\\n\\n```\\n\\n### Add a user to the database\\n\\nFor this step we\'ll need to use the [sqlcmd command line tool](https://learn.microsoft.com/en-us/sql/tools/sqlcmd/sqlcmd-utility?view=sql-server-ver16). You can install sqlcmd yourself, or you can use the [Azure Cloud Shell](https://shell.azure.com), which has it pre-installed for you.\\n\\n```bash\\n# Get the server FQDN\\nDB_SERVER_FQDN=$(az sql server show -g $RG -n $SERVER_NAME -o tsv --query fullyQualifiedDomainName)\\n\\n# Generate the user creation command\\n# Copy the output of the following to run against your SQL Server after logged in\\necho \\"CREATE USER [${MANAGED_IDENTITY_NAME}] FROM EXTERNAL PROVIDER WITH OBJECT_ID=\'${USER_ASSIGNED_OBJ_ID}\'\\"\\necho \\"GO\\"\\necho \\"ALTER ROLE db_datareader ADD MEMBER [${MANAGED_IDENTITY_NAME}]\\"\\necho \\"GO\\"\\n\\n# Login to the SQL DB via interactive login\\nsqlcmd -S $DB_SERVER_FQDN -d $DATABASE_NAME -G\\n\\n##################################################\\n# Paste the command generated above to create the \\n# User and grant the user reader access\\n# then type exit to leave the sqlcmd terminal\\n##################################################\\n\\n```\\n## Create the sample app\\n\\n```bash\\n# Create and test a new console app\\ndotnet new console -n sql-console-app\\ncd sql-console-app\\ndotnet run\\n\\n# Add the Key Vault and Azure Identity Packages\\ndotnet add package Microsoft.Data.SqlClient\\n#dotnet add package Azure.Identity\\n```\\n\\nEdit the app Program.cs as follows:\\n\\n```csharp\\nusing Microsoft.Data.SqlClient;\\n\\nnamespace sqltest\\n{\\n    class Program\\n    {\\n        static void Main(string[] args)\\n        {\\n            string? dbServerFQDN = Environment.GetEnvironmentVariable(\\"DB_SERVER_FQDN\\");\\n            string? dbName = Environment.GetEnvironmentVariable(\\"DATABASE_NAME\\");\\n            \\n            while(true){\\n                try \\n                { \\n                    // For system-assigned managed identity\\n                    // Use your own values for Server and Database.\\n                    string ConnectionString = String.Format(\\"Server={0}; Authentication=Active Directory Default; Encrypt=True; Database={1}\\",dbServerFQDN,dbName);\\n\\n                    using (SqlConnection connection = new SqlConnection(ConnectionString)) {\\n\\n                        Console.WriteLine(\\"\\\\nQuery data example:\\");\\n                        Console.WriteLine(\\"=========================================\\\\n\\");\\n                        \\n                        connection.Open();       \\n\\n                        String sql = \\"SELECT TOP 5 FirstName, LastName FROM [SalesLT].[Customer]\\";\\n\\n                        using (SqlCommand command = new SqlCommand(sql, connection))\\n                        {\\n                            using (SqlDataReader reader = command.ExecuteReader())\\n                            {\\n                                while (reader.Read())\\n                                {\\n                                    Console.WriteLine(\\"{0} {1}\\", reader.GetString(0), reader.GetString(1));\\n                                }\\n                            }\\n                        }                    \\n                    }\\n                }\\n                catch (SqlException e)\\n                {\\n                    Console.WriteLine(e.ToString());\\n                }\\n            System.Threading.Thread.Sleep(5000);\\n            }\\n        }\\n    }\\n}\\n```\\n\\nTest the changes\\n\\n```bash\\n# Build and run the console app\\nDB_SERVER_FQDN=$DB_SERVER_FQDN DATABASE_NAME=$DATABASE_NAME dotnet run\\n\\n##################################################################\\n# You should see the console app return sample data at this point\\n##################################################################\\n```\\n\\nCreate a new Dockerfile with the following.\\n\\n> **NOTE:** Don\'t forget to check the dotnet version you used to generate your code by running \'dotnet --version\' and make sure the base container image matches. For example, my dotnet version was 7.0.102 when I wrote this, so I used sdk 7.0.\\n\\n```bash\\nFROM mcr.microsoft.com/dotnet/sdk:7.0 AS build-env\\nWORKDIR /App\\n\\n# Copy everything\\nCOPY . ./\\n# Restore as distinct layers\\nRUN dotnet restore\\n# Build and publish a release\\nRUN dotnet publish -c Release -o out\\n\\n# Build runtime image\\nFROM mcr.microsoft.com/dotnet/aspnet:7.0\\nWORKDIR /App\\nCOPY --from=build-env /App/out .\\nENTRYPOINT [\\"dotnet\\", \\"sql-console-app.dll\\"]\\n```\\n\\nBuild the image. I\'ll create an Azure Container Registry and build there, and then link that ACR to my AKS cluster.\\n\\n```bash\\n# Create the ACR\\naz acr create -g $RG -n $ACR_NAME --sku Standard\\n\\n# Link the ACR to the AKS cluster\\naz aks update -g $RG -n $CLUSTER_NAME --attach-acr $ACR_NAME --no-wait\\n\\n# Build the image\\naz acr build -t wi-sql-test -r $ACR_NAME .\\n```\\n\\nNow deploy a pod that gets the value using the service account identity.\\n\\n```bash\\n\\ncat <<EOF | kubectl apply -f -\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: wi-sql-test\\n  namespace: default\\n  labels:\\n    azure.workload.identity/use: \\"true\\"  \\nspec:\\n  serviceAccountName: wi-demo-sa\\n  containers:\\n    - image: ${ACR_NAME}.azurecr.io/wi-sql-test\\n      name: wi-sql-test\\n      env:\\n      - name: DB_SERVER_FQDN\\n        value: ${DB_SERVER_FQDN}\\n      - name: DATABASE_NAME\\n        value: ${DATABASE_NAME}\\n      imagePullPolicy: Always   \\n  nodeSelector:\\n    kubernetes.io/os: linux\\nEOF\\n\\n# Check the pod logs\\nkubectl logs -f wi-sql-test\\n\\n```"},{"id":"/2021/09/21/workload-identity-self-managed-setup","metadata":{"permalink":"/2021/09/21/workload-identity-self-managed-setup","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2021-09-21/workload-identity-self-managed-setup/index.md","source":"@site/blog/2021-09-21/workload-identity-self-managed-setup/index.md","title":"Using Workload Identity with Self Managed Clusters","description":"How to use Workload Identity with your own bespoke Kubernetes Cluster","date":"2021-09-21T00:00:00.000Z","tags":[],"readingTime":5.71,"hasTruncateMarker":true,"authors":[{"name":"Steve Griffith","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/swgriffith","socials":{"x":"https://x.com/SteveGriffith","github":"https://github.com/swgriffith"},"imageURL":"https://github.com/swgriffith.png","key":"steve_griffith","page":null}],"frontMatter":{"authors":["steve_griffith"],"date":"2021-09-21","description":"How to use Workload Identity with your own bespoke Kubernetes Cluster","tags":[],"title":"Using Workload Identity with Self Managed Clusters"},"unlisted":false,"prevItem":{"title":"Accessing Azure SQL DB via Workload Identity and Managed Identity","permalink":"/2021/09/21/workload-identity-azuresql-example"},"nextItem":{"title":"Workload Identity - Windows Nodepool Walkthrough","permalink":"/2021/09/21/workload-identity-windows-example"}},"content":"\x3c!-- truncate --\x3e\\n\\n### Cluster Creation\\n\\nFor this walk through I created my cluster from scratch using kubeadm, testing on both Azure and Google Cloud. The kubeadm setup directions I followed are linked below:\\n\\n[https://computingforgeeks.com/deploy-kubernetes-cluster-on-ubuntu-with-kubeadm/](https://computingforgeeks.com/deploy-kubernetes-cluster-on-ubuntu-with-kubeadm/)\\n\\nOnce running you need to copy the sa.pub file from your kubernetes master node to the location where you\'ll be running your Azure CLI. This file is located at /etc/kubernetes/pki/sa.pub\\n\\n\\n### Create the Discovery Document in Blob Storage\\n\\nUsing the upstream document [here](https://azure.github.io/azure-workload-identity/docs/installation/self-managed-clusters/oidc-issuer/discovery-document.html).\\n\\n```bash\\nexport RESOURCE_GROUP=\\"oidcissuer\\"\\nexport LOCATION=\\"eastus\\"\\n\\n# Create the resource group\\naz group create --name \\"${RESOURCE_GROUP}\\" --location \\"${LOCATION}\\"\\n\\n# Generate a unique name for the storage account\\nexport AZURE_STORAGE_ACCOUNT=\\"oidcissuer$(openssl rand -hex 4)\\"\\nexport AZURE_STORAGE_CONTAINER=\\"oidc-test\\"\\n\\n# Create the storage account\\naz storage account create --resource-group \\"${RESOURCE_GROUP}\\" --name \\"${AZURE_STORAGE_ACCOUNT}\\"\\naz storage container create --name \\"${AZURE_STORAGE_CONTAINER}\\" --public-access container\\n\\n# Generate the oidc well known configuration document\\ncat <<EOF > openid-configuration.json\\n{\\n  \\"issuer\\": \\"https://${AZURE_STORAGE_ACCOUNT}.blob.core.windows.net/${AZURE_STORAGE_CONTAINER}/\\",\\n  \\"jwks_uri\\": \\"https://${AZURE_STORAGE_ACCOUNT}.blob.core.windows.net/${AZURE_STORAGE_CONTAINER}/openid/v1/jwks\\",\\n  \\"response_types_supported\\": [\\n    \\"id_token\\"\\n  ],\\n  \\"subject_types_supported\\": [\\n    \\"public\\"\\n  ],\\n  \\"id_token_signing_alg_values_supported\\": [\\n    \\"RS256\\"\\n  ]\\n}\\nEOF\\n\\n# Upload the well known configuration document to the blob storage account\\naz storage blob upload \\\\\\n  --container-name \\"${AZURE_STORAGE_CONTAINER}\\" \\\\\\n  --file openid-configuration.json \\\\\\n  --name .well-known/openid-configuration\\n\\n# Test the endpoint\\ncurl -s \\"https://${AZURE_STORAGE_ACCOUNT}.blob.core.windows.net/${AZURE_STORAGE_CONTAINER}/.well-known/openid-configuration\\"\\n```\\n\\n\\n### Create the Json Web Key Sets (jswk) file\\n\\nFollowing the guide [here](https://azure.github.io/azure-workload-identity/docs/installation/self-managed-clusters/oidc-issuer/jwks.html), use the azwi cli to generate the jwks.json file using the sa.pub file created above. You\'ll need to make sure you\'ve installed the azwi cli [here](https://azure.github.io/azure-workload-identity/docs/installation/azwi.html)\\n\\n> *Note:* You\'ll use the sa.pub file you copied from your Kuberenetes master node above.\\n\\n```bash\\n# Generate the jwks file\\nazwi jwks --public-keys sa.pub --output-file jwks.json\\n\\n# Upload the jwks.json file to the blob account\\naz storage blob upload \\\\\\n  --container-name \\"${AZURE_STORAGE_CONTAINER}\\" \\\\\\n  --file jwks.json \\\\\\n  --overwrite \\\\\\n  --name openid/v1/jwks\\n\\n# Test the file\\ncurl -s \\"https://${AZURE_STORAGE_ACCOUNT}.blob.core.windows.net/${AZURE_STORAGE_CONTAINER}/openid/v1/jwks\\"\\n```\\n\\nNext you need to update the kube-apiserver configuration on your Kubernetes master node. First let\'s output the issuer URL.\\n\\n```bash\\n# Get the issuer url\\necho Issuer: \\"https://${AZURE_STORAGE_ACCOUNT}.blob.core.windows.net/${AZURE_STORAGE_CONTAINER}/\\"\\n\\n# Sample Output\\nIssuer: https://oidcissuer4e2fd2e1.blob.core.windows.net/oidc-test/\\n```\\n\\nNow SSH back to the master node to edit the kube-apiserver static manifest.\\n\\n```bash\\n# Edit the kube-apiserver manifest\\nnano /etc/kubernetes/manifests/kube-apiserver.yaml \\n\\n# Set the service-account-issuer value to the first URL\\n# The service-account settings should look like the following given the URLs above:\\n\\n    - --service-account-issuer=https://oidcissuer4e2fd2e1.blob.core.windows.net/oidc-test/\\n    - --service-account-key-file=/etc/kubernetes/pki/sa.pub\\n    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key\\n\\n# Save and exit.\\n```\\n\\nThe above change will cause the API server to restart, so it may take a minute or two before the pods are back online and the API server is accessible.\\n\\n### Install the MutatingWebhook\\n\\nBack at the terminal where you have your Azure CLI and access to the cluster via kubectl, we\'ll install the workload identity components.\\n\\n```bash\\n# Get your Azure Active Directory Tenant ID\\nexport AZURE_TENANT_ID=$(az account show -o tsv --query homeTenantId)\\n\\n# Install the MutatingWebhook\\nhelm repo add azure-workload-identity https://azure.github.io/azure-workload-identity/charts\\nhelm repo update\\nhelm install workload-identity-webhook azure-workload-identity/workload-identity-webhook \\\\\\n   --namespace azure-workload-identity-system \\\\\\n   --create-namespace \\\\\\n   --set azureTenantID=\\"${AZURE_TENANT_ID}\\"\\n\\n# Check the installation\\nkubectl get pods -n azure-workload-identity-system\\n\\n# Sample Output\\nNAME                                                   READY   STATUS    RESTARTS       AGE\\nazure-wi-webhook-controller-manager-747c86695f-9jrk5   1/1     Running   20 (48m ago)   94m\\nazure-wi-webhook-controller-manager-747c86695f-vrpkr   1/1     Running   20 (49m ago)   94m\\n```\\n\\n### Test\\n\\nNow the cluster is configured with all the components needed to enable service account federation and Azure Workload Identity. Lets test it out. We\'ll create a new service account and managed identity and federate them. We\'ll also create a key vault we can use to test the service account federation.\\n\\n> *Note:* We\'ll reuse the resource group variable from above, so you may need to reset it if you have a new terminal session.\\n\\n```bash\\n# Set the managed identity and oidc issuer variables\\nMANAGED_IDENTITY_NAME=testmi\\nOIDC_ISSUER=<Get the oidc issuer url from above>\\n\\n# Create the managed identity\\naz identity create --name $MANAGED_IDENTITY_NAME --resource-group $RESOURCE_GROUP\\n\\n# Get the client ID\\nexport USER_ASSIGNED_CLIENT_ID=$(az identity show --resource-group $RESOURCE_GROUP --name $MANAGED_IDENTITY_NAME --query \'clientId\' -o tsv)\\n\\n# Create the namespace and service account\\nNAMESPACE=wi-test\\n\\nkubectl create ns $NAMESPACE\\n\\ncat <<EOF | kubectl apply -f -\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  annotations:\\n    azure.workload.identity/client-id: ${USER_ASSIGNED_CLIENT_ID}\\n  labels:\\n    azure.workload.identity/use: \\"true\\"\\n  name: ${MANAGED_IDENTITY_NAME}-sa\\n  namespace: ${NAMESPACE}\\nEOF\\n\\n# Federate the service account and managed identity\\naz identity federated-credential create \\\\\\n--name $MANAGED_IDENTITY_NAME-federated-id \\\\\\n--identity-name $MANAGED_IDENTITY_NAME \\\\\\n--resource-group $RESOURCE_GROUP \\\\\\n--issuer ${OIDC_ISSUER} \\\\\\n--subject system:serviceaccount:$NAMESPACE:$MANAGED_IDENTITY_NAME-sa\\n```\\n\\nFor testing purposes we\'ll create a key vault and a secret which is authorized for read access by the managed identity we created above.\\n\\n```bash\\nKEY_VAULT_NAME=vault$(openssl rand -hex 4)\\n\\n# Create a key vault\\naz keyvault create --name $KEY_VAULT_NAME --resource-group $RESOURCE_GROUP\\n\\n# Create a secret\\naz keyvault secret set --vault-name $KEY_VAULT_NAME --name \\"Secret\\" --value \\"Hello from key vault\\"\\n\\n# Grant access to the secret for the managed identity using it\'s AAD client ID\\naz keyvault set-policy --name $KEY_VAULT_NAME --secret-permissions get --spn \\"${USER_ASSIGNED_CLIENT_ID}\\"\\n\\n```\\n\\nFor the test app we\'ll use a Key Vault test container I created previously. You can see the code [here](https://github.com/Azure/reddog-aks-workshop/blob/main/docs/cheatsheets/workload-identity-cheatsheet.md#write-the-code-to-test-your-workload-identity-setup)\\n\\nDeploy the test app:\\n\\n```bash\\n# Deploy the app\\ncat <<EOF | kubectl apply -f -\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: wi-kv-test\\n  namespace: ${NAMESPACE}\\nspec:\\n  serviceAccountName: ${MANAGED_IDENTITY_NAME}-sa\\n  containers:\\n    - image: stevegriffith/wi-kv-test\\n      name: wi-kv-test\\n      env:\\n      - name: KEY_VAULT_NAME\\n        value: ${KEY_VAULT_NAME}\\n      - name: SECRET_NAME\\n        value: Secret    \\n  nodeSelector:\\n    kubernetes.io/os: linux\\nEOF\\n\\n# Check the pod is running\\nkubectl get pods -n $NAMESPACE\\n\\n# Sample Output\\nNAME         READY   STATUS    RESTARTS   AGE\\nwi-kv-test   1/1     Running   0          19s\\n\\n# Check the pod logs to confirm it\'s connecting to key vault with the authorized managed identity\\nkubectl logs -f wi-kv-test -n $NAMESPACE\\n\\n# Sample output\\nRetrieving your secret from vault70abc350.\\nYour secret is \'Hello from key vault\'.\\nRetrieving your secret from vault70abc350.\\nYour secret is \'Hello from key vault\'.\\nRetrieving your secret from vault70abc350.\\nYour secret is \'Hello from key vault\'.\\nRetrieving your secret from vault70abc350.\\nYour secret is \'Hello from key vault\'.\\n```\\n\\n## Conclusion\\n\\nYou should now have a working setup of Azure Workload Identity in your self managed cluster, connecting to keyvault to retrieve a secret via an authorized managed identity which has been federated to a kubernetes service account used by the application pod."},{"id":"/2021/09/21/workload-identity-windows-example","metadata":{"permalink":"/2021/09/21/workload-identity-windows-example","editUrl":"https://github.com/appdevgbb/gbb-blog/tree/main/docusaurus/blog/2021-09-21/workload-identity-windows-example/index.md","source":"@site/blog/2021-09-21/workload-identity-windows-example/index.md","title":"Workload Identity - Windows Nodepool Walkthrough","description":"How you can use Azure Workload Identity with the AKS Workload Identity add-on with MSAL on AKS Windows Nodepool","date":"2021-09-21T00:00:00.000Z","tags":[],"readingTime":4.61,"hasTruncateMarker":true,"authors":[{"name":"Steve Griffith","title":"Principal Cloud Architect, Azure Global Black Belt","url":"https://github.com/swgriffith","socials":{"x":"https://x.com/SteveGriffith","github":"https://github.com/swgriffith"},"imageURL":"https://github.com/swgriffith.png","key":"steve_griffith","page":null}],"frontMatter":{"authors":["steve_griffith"],"date":"2021-09-21","description":"How you can use Azure Workload Identity with the AKS Workload Identity add-on with MSAL on AKS Windows Nodepool","tags":[],"title":"Workload Identity - Windows Nodepool Walkthrough"},"unlisted":false,"prevItem":{"title":"Using Workload Identity with Self Managed Clusters","permalink":"/2021/09/21/workload-identity-self-managed-setup"}},"content":"The following walkthrough shows how you can using [Azure Workload Identity](https://azure.github.io/azure-workload-identity/docs/) with the [AKS Workload Identity](https://learn.microsoft.com/en-us/azure/aks/workload-identity-overview) add-on along with [MSAL](https://learn.microsoft.com/en-us/azure/active-directory/develop/reference-v2-libraries) on an AKS Windows Nodepool.\\n\\n\x3c!-- truncate --\x3e\\n\\n### Register for the preview\\n\\nThe managed add-on for Azure Workload Identity is still in preview, so we must first register for the preview.\\n\\n```bash\\n# Add or update the Azure CLI aks preview extention\\naz extension add --name aks-preview\\naz extension update --name aks-preview\\n\\n# Register for the preview feature\\naz feature register --namespace \\"Microsoft.ContainerService\\" --name \\"EnableWorkloadIdentityPreview\\"\\n\\n# Check registration status\\naz feature list -o table --query \\"[?contains(name, \'Microsoft.ContainerService/EnableWorkloadIdentityPreview\')].{Name:name,State:properties.state}\\"\\n\\n# Refresh the provider\\naz provider register --namespace Microsoft.ContainerService\\n```\\n\\n### Cluster Creation\\n\\nNow lets create the AKS cluster with the OIDC Issure and Workload Identity add-on enabled.\\n\\n```bash\\nRG=WorkloadIdentityRG\\nLOC=eastus\\nCLUSTER_NAME=wilab\\nWINDOWS_ADMIN_NAME=griffith\\nUNIQUE_ID=$CLUSTER_NAME$RANDOM\\nACR_NAME=$UNIQUE_ID\\nKEY_VAULT_NAME=$UNIQUE_ID\\n\\n# Create the resource group\\naz group create -g $RG -l $LOC\\n\\n# Create the cluster with the OIDC Issuer and Workload Identity enabled\\naz aks create -g $RG -n $CLUSTER_NAME \\\\\\n--node-count 1 \\\\\\n--enable-oidc-issuer \\\\\\n--enable-workload-identity \\\\\\n--generate-ssh-keys \\\\\\n--windows-admin-username $WINDOWS_ADMIN_NAME \\\\\\n--network-plugin azure\\n\\n# Add a windows pool\\naz aks nodepool add \\\\\\n--resource-group $RG \\\\\\n--cluster-name $CLUSTER_NAME \\\\\\n--os-type Windows \\\\\\n--name npwin \\\\\\n--node-count 1\\n\\n# Get the cluster credentials\\naz aks get-credentials -g $RG -n $CLUSTER_NAME --admin\\n```\\n\\n### Set up the identity \\n\\nIn order to federate a managed identity with a Kubernetes Service Account we need to get the AKS OIDC Issure URL, create the Managed Identity and Service Account and then create the federation.\\n\\n```bash\\n# Get the OIDC Issuer URL\\nexport AKS_OIDC_ISSUER=\\"$(az aks show -n $CLUSTER_NAME -g $RG --query \\"oidcIssuerProfile.issuerUrl\\" -otsv)\\"\\n\\n# Create the managed identity\\naz identity create --name wi-demo-identity --resource-group $RG --location $LOC\\n\\n# Get identity client ID\\nexport USER_ASSIGNED_CLIENT_ID=$(az identity show --resource-group $RG --name wi-demo-identity --query \'clientId\' -o tsv)\\n\\n# Create a service account to federate with the managed identity\\ncat <<EOF | kubectl apply -f -\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  annotations:\\n    azure.workload.identity/client-id: ${USER_ASSIGNED_CLIENT_ID}\\n  labels:\\n    azure.workload.identity/use: \\"true\\"\\n  name: wi-demo-sa\\n  namespace: default\\nEOF\\n\\n# Federate the identity\\naz identity federated-credential create \\\\\\n--name wi-demo-federated-id \\\\\\n--identity-name wi-demo-identity \\\\\\n--resource-group $RG \\\\\\n--issuer ${AKS_OIDC_ISSUER} \\\\\\n--subject system:serviceaccount:default:wi-demo-sa\\n```\\n\\n### Create the Key Vault and Secret\\n\\n```bash\\n# Create a key vault\\naz keyvault create --name $KEY_VAULT_NAME --resource-group $RG --location $LOC --enable-rbac-authorization false\\n\\n# Create a secret\\naz keyvault secret set --vault-name $KEY_VAULT_NAME --name \\"Secret\\" --value \\"Hello\\"\\n\\n# Grant access to the secret for the managed identity\\naz keyvault set-policy --name $KEY_VAULT_NAME --secret-permissions get --spn \\"${USER_ASSIGNED_CLIENT_ID}\\"\\n\\n# Get the version ID\\naz keyvault secret show --vault-name $KEY_VAULT_NAME --name \\"Secret\\" -o tsv --query id\\nhttps://wi-demo-keyvault.vault.azure.net/secrets/Secret/ded8e5e3b3e040e9bfa5c47d0e28848a\\n\\n# The version ID is the last part of the resource id above\\n# We\'ll use this later\\nVERSION_ID=ded8e5e3b3e040e9bfa5c47d0e28848a\\n```\\n\\n## Create the sample app\\n\\n```bash\\n# Create and test a new console app\\ndotnet new console -n keyvault-console-app\\ncd keyvault-console-app\\ndotnet run\\n\\n# Add the Key Vault and Azure Identity Packages\\ndotnet add package Azure.Security.KeyVault.Secrets\\ndotnet add package Azure.Identity\\n```\\n\\nEdit the app as follows:\\n\\n```csharp\\nusing System;\\nusing System.IO;\\nusing Azure.Core;\\nusing Azure.Identity;\\nusing Azure.Security.KeyVault.Secrets;\\n\\nclass Program\\n    {\\n        static void Main(string[] args)\\n        {\\n            //Get env variables\\n            string? secretName = Environment.GetEnvironmentVariable(\\"SECRET_NAME\\");;\\n            string? keyVaultName = Environment.GetEnvironmentVariable(\\"KEY_VAULT_NAME\\");;\\n            string? versionID = Environment.GetEnvironmentVariable(\\"VERSION_ID\\");;\\n            \\n            //Create Key Vault Client\\n            var kvUri = String.Format(\\"https://{0}.vault.azure.net\\", keyVaultName);\\n            SecretClientOptions options = new SecretClientOptions()\\n            {\\n                Retry =\\n                {\\n                    Delay= TimeSpan.FromSeconds(2),\\n                    MaxDelay = TimeSpan.FromSeconds(16),\\n                    MaxRetries = 5,\\n                    Mode = RetryMode.Exponential\\n                 }\\n            };\\n\\n            var client = new SecretClient(new Uri(kvUri), new DefaultAzureCredential(),options);\\n\\n            // Get the secret value in a loop\\n            while(true){\\n            Console.WriteLine(\\"Retrieving your secret from \\" + keyVaultName + \\".\\");\\n            KeyVaultSecret secret = client.GetSecret(secretName, versionID);\\n            Console.WriteLine(\\"Your secret is \'\\" + secret.Value + \\"\'.\\");\\n            System.Threading.Thread.Sleep(5000);\\n            }\\n\\n        }\\n    }\\n```\\n\\nCreate a new Dockerfile with the following:\\n\\n>*NOTE:* Make sure the dotnet version matches the version you used to generate the code (i.e. 6.0 or 7.0) in both FROM lines below.\\n\\n```bash\\nFROM mcr.microsoft.com/dotnet/sdk:7.0-windowsservercore-ltsc2019 AS build-env\\nWORKDIR /App\\n\\n# Copy everything\\nCOPY . ./\\n# Restore as distinct layers\\nRUN dotnet restore\\n# Build and publish a release\\nRUN dotnet publish -c Release -o out\\n\\n# Build runtime image\\nFROM mcr.microsoft.com/dotnet/aspnet:7.0-windowsservercore-ltsc2019\\nWORKDIR /App\\nCOPY --from=build-env /App/out .\\nENTRYPOINT [\\"dotnet\\", \\"keyvault-console-app.dll\\"]\\n```\\n\\nBuild the image. I\'ll create an Azure Container Registry and build there, and then link that ACR to my AKS cluster.\\n\\n```bash\\n# Create the ACR\\naz acr create -g $RG -n $ACR_NAME --sku Standard\\n\\n# Build the image\\naz acr build -g $RG -t kvtest --platform windows -r $ACR_NAME .\\n\\n# Link the ACR to the AKS cluster\\naz aks update -g $RG -n $CLUSTER_NAME --attach-acr $ACR_NAME\\n```\\n\\nNow deploy a pod that gets the value using the service account identity.\\n\\n```bash\\n\\ncat <<EOF | kubectl apply -f -\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: wi-kv-test\\n  namespace: default\\n  labels:\\n    azure.workload.identity/use: \\"true\\"\\nspec:\\n  nodeSelector:\\n    \\"kubernetes.io/os\\": \\"windows\\"\\n  serviceAccountName: wi-demo-sa\\n  containers:\\n    - image: $ACR_NAME.azurecr.io/kvtest\\n      name: wi-kv-test\\n      env:\\n      - name: KEY_VAULT_NAME\\n        value: $KEY_VAULT_NAME\\n      - name: SECRET_NAME\\n        value: Secret\\n      - name: VERSION_ID\\n        value: ${VERSION_ID}\\nEOF\\n\\n# Check the pod logs\\nkubectl logs -f wi-kv-test\\n\\n# Sample Output\\nRetrieving your secret from wi-demo-keyvault.\\nYour secret is \'Hello\'.\\n```\\n\\n### Conclusion\\n\\nCongrats! You should now have a working pod that uses MSAL along with a Kubernetes Service Account federated to an Azure Managed Identity to access and Azure Key Vault Secret."}]}}')}}]);